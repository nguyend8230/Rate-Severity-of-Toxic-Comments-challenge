{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12b09e08-729d-4df0-814f-d49e7f084f61",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <h3>NOTE</h3>\n",
    "    <p>Before you submit this assignment, <strong>make sure everything runs as expected</strong>:</p>\n",
    "    <ol>\n",
    "        <li><strong>restart the kernel</strong> (in the menubar, select <strong>Kernel → Restart</strong>)\n",
    "        <li><strong>run all cells</strong> (in the menubar, select <strong>Cell → Run All</strong>)</li>\n",
    "    </ol>\n",
    "    <p>Make sure to complete every cell that states \"<strong><TT>YOUR CODE IN THIS CELL</TT></strong>\".</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3a6163-b141-402e-8ab1-b330d7f0832a",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e22d54-a7e5-4612-9ed1-da05c1a74432",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h2>CHANGELOG</h2>\n",
    "    <h3>A3-V5.ipynb</h3>\n",
    "    <p>Nov 29, 2021</br>\n",
    "    The following revisions have been added to <strong>A3-V4.ipynb</strong>.</p>\n",
    "    <ul>\n",
    "        <li>added code for <strong>Voting Ensemble</strong> for regression</li>\n",
    "        <li>added example code to <strong>Plotting & Visualizations</strong> subsection</li>\n",
    "        <li>reduced weight of the <strong>Final Report</strong> section</li>\n",
    "        <li>added <strong>tfds-nightly</strong> package to list of packages to import</li>\n",
    "        <li>explained the evaluation process in <strong>Task: Evaluation</strong></li>\n",
    "        <li>added code to <strong>Task: Evaluation</strong> for evaluating a model</li>\n",
    "        <li>added <strong>pandas</strong> as a package to import</li>\n",
    "        <li>added <strong>pandas DataFrames</strong> tutorial examples under <strong>Task: New Toxic Comments Dataset</strong> for tips on accessing the datasets more easily</li>\n",
    "        <li>added code for SVM for regression to <strong>Task: SVM Classification</strong></li>\n",
    "        <li>removed \"Fit the model\" comments from each model's code cell since it was redundant (already had \"Train the model\")</li>\n",
    "        <li>finished adding code to <strong>Neural Network</strong> task</li>\n",
    "        <li>updated number of marks <strong>Neural Network</strong> task from 10 marks to 5 marks</li>\n",
    "        <!-- <li></li> -->\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069c7ed8-51e8-4640-a6a5-53fffeedbaa9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h2>CHANGELOG</h2>\n",
    "    <h3>A3-V4.ipynb</h3>\n",
    "    <p>Nov 24, 2021</br>\n",
    "    The following revisions have been added to <strong>A3-V3.ipynb</strong>.</p>\n",
    "    <ul>\n",
    "        <li>added code cell to <em>Evaluation</em> & <em>Discussion</em> sections of the <strong>Project Report</strong> for demonstrating visualization capabilities</li>\n",
    "        <li>added additional comment steps to <strong>Language Model</strong> task</li>\n",
    "        <li>updated marks for <strong>Language Model</strong> task</li>\n",
    "        <li>added <strong>PRO TIPS</strong> throughout document</li>\n",
    "        <li>updated pseudocode for <strong>Extract Features From Dataset</strong></li>\n",
    "        <li>added code to <strong>Neural Network</strong> task</li>\n",
    "        <li>added comments to <strong>Decision Tree</strong> task</li>\n",
    "        <li>added explanation (from <strong>Discord</strong>) of <strong>Extract Features From Dataset</strong> task</li>\n",
    "        <li>added regression and classification implementations to each model</li>\n",
    "        <li>added <strong>Plotting & Visualizations</strong> subsection</li>\n",
    "        <li>added explanation (from <strong>Discord</strong>) of <strong>What You Are Being Asked To Do</strong> subsection</li>\n",
    "        <!-- <li></li> -->\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e496991c-edd6-43de-bc5e-9ca302b8b5be",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>A3-V3.ipynb</h3>\n",
    "    <p>Nov 19, 2021</br>\n",
    "    The following revisions have been added to <strong>A3-V2.ipynb</strong>.</p>\n",
    "    <p>Extract Features From Dataset:\n",
    "    <ul>\n",
    "        <li>updated description of <strong>Extract Features From Dataset</strong></li>\n",
    "        <li>updated how many marks <strong>Extract Features From Dataset</strong> is out of</li>\n",
    "        <li>provided a breakdown of the marks in <strong>Extract Features From Dataset</strong></li>\n",
    "        <li>added code examples to <strong>Extract Features From Dataset</strong></li>\n",
    "    </ul>\n",
    "    <p>Miscellaneous:\n",
    "    <ul>\n",
    "        <li>marking rubric updated for some <strong>Tasks</strong> (but not yet finalized)</li>\n",
    "        <li>added comments for all EXAMPLE CODE to be commented out</li>\n",
    "        <li>added comments for removing <tt>raise NotImplementedError()</tt> once the code cell has begun being implemented</li>\n",
    "        <li>new <strong>TODOs</strong> added (these are for the instructor to complete)</li>\n",
    "    </ul>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bf3597-866c-4551-bbe6-29f3cf84ad6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>A3-V2.ipynb</h3>\n",
    "    <p>Nov 18, 2021</br>\n",
    "    The following revisions have been added to <strong>A3-V1.ipynb</strong>:</p>\n",
    "    <ul>\n",
    "        <li>added <strong>Changelog</strong></li>\n",
    "        <li>added code and a description to the <strong>Decision Tree</strong></li>\n",
    "        <li>added code to the <strong>Voting Ensemble</strong></li>\n",
    "        <li>added code for <strong>SVM Classifier</strong></li>\n",
    "        <li>added code for <strong>Decision Tree Classifier</strong></li>\n",
    "        <li>added to the <strong>Overview</strong> section to compare the task to other tasks</li>\n",
    "        <li>updated code description for <strong>n-gram Language Model</strong></li>\n",
    "        <li>added new dataset from just announced <strong>Kaggle</strong> competition on identifying toxic comments  <strong>→</strong>  <strong>$50,000</strong></li>\n",
    "        <li>transitioned to new evaluation metric taken from the just announced <strong>Kaggle</strong> competition on identifying toxic comments</li>\n",
    "        <li>changed font to light gray color for the <strong>Toxic Comments Dataset (OLD COMPETITION)</strong> since this dataset isn't the one provided by the new competition</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52702c8-7675-4abb-aa81-1e0f18a834f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <h4><strong>TODO</strong></h4>\n",
    "    <p>The following items need to be included by the instructor in this assignment:</p>\n",
    "    <ul>\n",
    "        <li>need to add code to <strong>Neural Network</strong></li>\n",
    "        <li>add example code to <strong>Plotting & Visualizations</strong> subsection</li>\n",
    "        <li>marking rubric not finalized</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccb7fe1-25eb-42df-ac84-6828ca20606a",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Assignment #3: Version 5\n",
    "**CMPT-310: Fall 2021**\n",
    "\n",
    "\n",
    "**NOTE:** Complete **Quiz #4** prior to beginning this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4499febe-b6ed-4a36-b4dc-a643e9b81224",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Objectives\n",
    "\n",
    "* access a *real-world* dataset (e.g., used in **kaggle.com** competitions, etc.) rather than a *toy* dataset\n",
    "* implement a variety of machine learning models\n",
    "* download and apply *pre-trained machine learning models*\n",
    "* develop and evaluate ensembles of models\n",
    "* evaluate different machine learning models on *complex* real-world tasks\n",
    "* work with a variety of industry-standard machine learning libraries (*Tensorflow*, *PyTorch*, *sklearn*, *NLTK*, etc.)\n",
    "* visually communicate data and experimental results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37678043-d48b-4205-b71f-72f3fb41ab71",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Instructions\n",
    "\n",
    "Write your **code** in the *code cells* located directly below each red *Write Code* block.\\\n",
    "Write your **text** in the *Markdown cells* that follow every **Task** description below. Also complete this Notebook's **Final Report** section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3983737",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h4>PRO TIP</h4>\n",
    "    <p>The best approach to this assignment is to work on <strong>one task at a time</strong>. Treat each task as a step toward a destination.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4f2825-6b22-4378-a31a-d02e923559bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "------\n",
    "## Preliminaries & Dependencies\n",
    "\n",
    "You will require the following **Python** packages to complete this assignment (it is likely many of these libraries are already installed via **Anaconda**, **Quiz #4**, etc.):\n",
    "* matplotlib\n",
    "* numpy\n",
    "* sklearn\n",
    "* tensorflow\n",
    "* tensorflow-hub\n",
    "* tensorflow-datasets\n",
    "* tfds-nightly\n",
    "* seaborn\n",
    "* nltk\n",
    "* pandas\n",
    "\n",
    "\n",
    "### Imports\n",
    "\n",
    "Import the following libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e084519-9419-4f64-8287-38c245f19138",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import IPython.display as display\n",
    "import seaborn\n",
    "import sklearn\n",
    "import nltk\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e51feb5-8e31-4bf6-b468-608f17ea7717",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Cross Validation Of A Dataset\n",
    "\n",
    "The following example code demonstrates using the builtin cross validation module from the [**Scikit-Learn** library](https://scikit-learn.org/stable/modules/cross_validation.html).\\\n",
    "You will likely use a variation of the following code for all of the models you will be evaluating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af18121c-0c1a-4cf0-a0c9-d627981376aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE CODE: COMMENT OUT THIS CODE CELL AFTER IMPLEMENTING YOUR CODE\n",
    "\n",
    "# Code from:  https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "\n",
    "# X = inputs or features of the data\n",
    "# y = output values from the data that we are trying to predict\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "X.shape, y.shape # shape displays the dimensions of the matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff3faad-8a83-4d2b-b921-8f44bef5c0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE CODE: COMMENT OUT THIS CODE CELL AFTER IMPLEMENTING YOUR CODE\n",
    "\n",
    "# Train the model using 80% of the dataset then test (evaluate) the model on the other 20% of the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# shape displays the dimensions of the matrices\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"The performance of one run of the SVM model:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21809e3-a739-48dd-81ab-f21682a21cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE CODE: COMMENT OUT THIS CODE CELL AFTER IMPLEMENTING YOUR CODE\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1, random_state=42)\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "print(\"Scores for the 5 runs of the SVM model:\", scores)\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeb0e06-5f2b-49d4-bb94-9f8e6ffde2fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "## Task: New Toxic Comments Dataset   (5 Marks)\n",
    "\n",
    "See https://www.kaggle.com/c/jigsaw-toxic-severity-rating.\n",
    "\n",
    "From [**Kaggle**](https://www.kaggle.com/c/jigsaw-toxic-severity-rating/data):\n",
    "> In this competition you will be ranking comments in order of severity of toxicity. You are given a list of comments, and each comment should be scored according to their relative toxicity. Comments with a higher degree of toxicity should receive a higher numerical value compared to comments with a lower degree of toxicity.\n",
    "> \n",
    "> **Disclaimer:** The dataset for this competition contains text that may be considered *profane, vulgar, or offensive*.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Data Description\n",
    "\n",
    "\"*Your task is to predict a score that represents the relative toxic severity of the comment. Comments with a higher degree of toxicity should receive a higher numerical value compared to comments with a lower degree of toxicity; scores are relative, and not constrained to a certain range of values.*\"\n",
    "\n",
    "> Note, there is no training data for this competition. You can refer to previous Jigsaw competitions for data that might be useful to train models. But note that the task of previous competitions has been to predict the probability that a comment was toxic, rather than the degree or severity of a comment's toxicity.\n",
    "> \n",
    "> **Toxic Comment Classification Challenge**\\\n",
    "> **Jigsaw Unintended Bias in Toxicity Classification**\\\n",
    "> **Jigsaw Multilingual Toxic Comment Classification**\n",
    "> \n",
    "> While we don't include training data, we do provide a set of paired toxicity rankings that can be used to validate models.\n",
    "> \n",
    "> #### Files\n",
    "> \n",
    "> **comments_to_score.csv** - for each comment text in this file, your task is to predict a score that represents the relative toxic severity of the comment. Comments with a higher degree of toxicity should receive a higher numerical value compared to comments with a lower degree of toxicity; scores are relative, and not constrained to a certain range of values. NOTE: the rerun version of this file has ~14k comments that will be scored by your submitted model.\\\n",
    "> **sample_submission.csv** - a sample submission file in the correct format\\\n",
    "> **validation_data.csv** - pair rankings that can be used to validate models; this data includes the annotator worker id, and how that annotator ranked a given pair of comments; note, this data contains comments that are not found in comments_to_score.\n",
    "\n",
    "\n",
    "### What You Are Being Asked To Do\n",
    "\n",
    "(A version of this is posted on **Discord**)\n",
    "\n",
    "The file `validation_data.csv` has examples of comment pairs for you to test your model against (*I don't recommend reading the toxic comments though!*).\n",
    "\n",
    "![Contents of validation_data.csv file.](./images/Toxic-comment-database-examples.png)\n",
    "\n",
    "The above image shows the first few rows of the `validation_data.csv` file. Each row contains a pair of comments. The column on the right contains comments that are more toxic than the comments in the left column. The comment pairs in the `validation_data.csv` file was ranked by human annotators.\n",
    "\n",
    "Our job is to provide every comment with a **toxicity score**. To check how accurate our scoring was, two random comments are selected and we compare which comment was identifed to be more toxic.\n",
    "\n",
    "An example of what you are being asked to do:\n",
    "* give your model the comment `This article sucks woo woo wooooooo`, which the model assigns the comment a **toxicity score** (say a score of 20)\n",
    "* give your model another comment  such as `WHAT!!!!!!!!?!?!!?!?!!?!?!?!?!!!!!!!!!!!!!!!!!!!!!!!!???????????????????????????????????????????????...`, which the model assigns the comment a **toxicity score** (say a score of 50)\n",
    "* continue scoring *all* of the comments in `comments_to_score.csv`\n",
    "* in the competition, **Kaggle** will take the scores and compare them to two of the comments that humans determined which was more toxic\n",
    "* if your scoring matches the human rankings (i.e., the \"*more toxic*\" comment gets a higher score than the \"*less toxic*\" comment) then the model got that comparison correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ca4a3a-842b-40e1-873f-404fbf7d8957",
   "metadata": {},
   "source": [
    "<font color=lightgray>\n",
    "    \n",
    "## Resource: Civil Comments Dataset   (2 YEAR OLD COMPETITION)\n",
    "\n",
    "Note this a **large** dataset: 758 Mb. You are not required to use this dataset (although it appears very helpful).\\\n",
    "\"*This dataset is a replica of the data released for the Jigsaw Toxic Comment Classification Challenge and Jigsaw Multilingual Toxic Comment Classification competition on Kaggle, with the test dataset merged with the test_labels released after the end of the competitions*\".\n",
    "\n",
    "**Kaggle** had a competition **Jigsaw Unintended Bias in Toxicity Classification** where they \"detect toxic comments while minimizing unintended model bias\".\\\n",
    "See https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge.\n",
    "\n",
    "From **Kaggle**'s website:\n",
    "> When the Conversation AI team first built toxicity models, they found that the models incorrectly learned to associate the names of frequently attacked identities with toxicity. Models predicted a high likelihood of toxicity for comments containing those identities (e.g. \"gay\"), even when those comments were not actually toxic (such as \"I am a gay woman\"). This happens because training data was pulled from available sources where unfortunately, certain identities are overwhelmingly referred to in offensive ways. Training a model from data with these imbalances risks simply mirroring those biases back to users.\n",
    "> \n",
    "> In this competition, you're challenged to build a model that recognizes toxicity and minimizes this type of unintended bias with respect to mentions of identities. You'll be using a dataset labeled for identity mentions and optimizing a metric designed to measure unintended bias. Develop strategies to reduce unintended bias in machine learning models, and you'll help the Conversation AI team, and the entire industry, build models that work well for a wide range of conversations.\n",
    "> \n",
    "> **Disclaimer:** the dataset for this competition contains text that may be considered *profane*, *vulgar*, or *offensive*.\n",
    "\n",
    "### Data Description\n",
    "\n",
    "From **Kaggle**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc0a411-8b50-4943-a6cc-e6b7e94c16f9",
   "metadata": {},
   "source": [
    "<font color=lightgray>\n",
    "    \n",
    "## Resource: Toxic Comments Dataset   (4 YEAR OLD COMPETITION)\n",
    "\n",
    "**Kaggle** has a competition identifying *toxic comments*.\\\n",
    "See https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge.\n",
    "\n",
    "From **Kaggle**'s website:\n",
    "> ...tools to help improve online conversation. One area of focus is the study of negative online behaviors, like toxic comments (i.e. comments that are rude, disrespectful or otherwise likely to make someone leave a discussion). So far they’ve built a range of publicly available models served through the Perspective API, including toxicity. But the current models still make errors, and they don’t allow users to select which types of toxicity they’re interested in finding (e.g. some platforms may be fine with profanity, but not with other types of toxic content).\n",
    "> \n",
    "> In this competition, you’re challenged to build a multi-headed model that’s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate better than Perspective’s current models. You’ll be using a dataset of comments from Wikipedia’s talk page edits. Improvements to the current model will hopefully help online discussion become more productive and respectful.\n",
    "> \n",
    "> **Disclaimer:** the dataset for this competition contains text that may be considered *profane*, *vulgar*, or *offensive*.\n",
    "\n",
    "### Data Description\n",
    "\n",
    "From **Kaggle**:\n",
    "> You are provided with a large number of Wikipedia comments which have been labeled by human raters for toxic behavior. The types of toxicity are:\n",
    "> * toxic\n",
    "> * severe_toxic\n",
    "> * obscene\n",
    "> * threat\n",
    "> * insult\n",
    "> * identity_hate\n",
    ">\n",
    "> You must create a model which predicts a probability of each type of toxicity for each comment.\n",
    "> \n",
    "> File Descriptions:\n",
    "> * **train.csv** - the training set, contains comments with their binary labels\n",
    "> * **test.csv** - the test set, you must predict the toxicity probabilities for these comments. To deter hand labeling, the test set contains some comments which are not included in scoring.\n",
    "> * **sample_submission.csv** - a sample submission file in the correct format\n",
    "> * **test_labels.csv** - labels for the test data; value of -1 indicates it was not used for scoring; (Note: file added after competition close!)\n",
    "\n",
    "Our **goal** is to predict for each comment the *probability* of each type of toxicity.\n",
    "This is a multi-class classification task (i.e., *more than two categories, more than two labels, more than two classes*, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ece326-df7e-418d-b366-4184595bd4f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download The Datasets\n",
    "\n",
    "Download the datasets from **Kaggle** (under the **Data** tab):\\\n",
    "[old dataset - \"Toxic Comment Classification Challenge\"](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data)\\\n",
    "[old dataset - \"Jigsaw Unintended Bias in Toxicity Classification\"](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data)\\\n",
    "[new dataset - \"Jigsaw Rate Severity of Toxic Comments\"](https://www.kaggle.com/c/jigsaw-toxic-severity-rating/data)\n",
    "\n",
    "Or download the dataset from **Canvas** under **Files** > **Data**:\\\n",
    "[Kaggle-Toxic-Comments-Dataset (OLD)](https://canvas.sfu.ca/files/17538324/download?download_frd=1)\\\n",
    "[Kaggle-Toxic-Comments-Dataset-With-Bias (OLD)](https://canvas.sfu.ca/files/17742270/download?download_frd=1) (note this is a **756 Mb** file)\\\n",
    "[Kaggle-Toxic-Comments-Dataset (NEW)](https://canvas.sfu.ca/files/17622704/download?download_frd=1) is the dataset we will be using to evaluate our model.\n",
    "\n",
    "Or download the datasets from **Tensorflow** (you can then convert the dataset to a **Dataframes** object if you are more comfortable with **Dataframes**).\\\n",
    "[Overview of Dataframes](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html).\\\n",
    "The code to download the datasets via **Tensorflow**'s data repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaac489-97a5-47cc-a2d9-38be6df8b43f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One way to download the data is through Tensorflow's Datasets repository\n",
    "# Data is stored at:\n",
    "#    ~/tensorflow_datasets/wikipedia_toxicity_subtypes/\n",
    "#    ~/tensorflow_datasets/civil_comments/CivilComments/\n",
    "# Dataset sizes are:\n",
    "#    2 Gb - wikipedia_toxicity_subtypes\n",
    "#    1 Gb - civil_comments\n",
    "# This code cell takes about 5-10 minutes to execute\n",
    "# You only need to run this code cell once. If you run it again it doesn't\n",
    "#     do anything since the datasets have already been downloaded\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Construct a tf.data.Dataset\n",
    "ds_wikipedia_comments = tfds.load('wikipedia_toxicity_subtypes')\n",
    "ds_comment_bias = tfds.load('civil_comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1029f4f1-ff35-4321-9fec-c077b7be0a13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code from: https://www.tensorflow.org/datasets/overview#tfdsas_dataframe\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas\n",
    "\n",
    "# working with DataFrames\n",
    "dset, info = tfds.load('wikipedia_toxicity_subtypes', with_info=True)\n",
    "dframe = tfds.as_dataframe(dset[\"train\"].take(1000), info) # only takes the first 1000 data samples\n",
    "#dframe = tfds.as_dataframe(dset[\"train\"], info) # takes all the data into memory (takes a while)\n",
    "\n",
    "# display first few rows & last few rows\n",
    "dframe.head()\n",
    "dframe.tail()\n",
    "\n",
    "# display column names\n",
    "dframe.columns\n",
    "\n",
    "# statistic summary of the data\n",
    "dframe.describe()\n",
    "\n",
    "# selecting the \"text\" column (these do the same thing)\n",
    "dframe[\"text\"]\n",
    "dframe.text\n",
    "\n",
    "# selecting the \"insult\" column (these do the same thing)\n",
    "dframe[\"insult\"]\n",
    "dframe.insult\n",
    "\n",
    "# getting the comment text from the 10th comment in the dataset\n",
    "dframe[\"text\"][9]\n",
    "\n",
    "# selecting rows 10 to 15\n",
    "dframe[10:16]\n",
    "\n",
    "# selecting all the comments that are labelled as an insult\n",
    "dframe[dframe[\"insult\"] > 0]\n",
    "\n",
    "# fancy advanced:\n",
    "#    selects the comments where column \"E\" has either the value of \"two\" or \"four\"\n",
    "dframe = tfds.as_dataframe(dset[\"train\"].take(6), info) # only takes the first 6 data samples\n",
    "dframe2 = dframe.copy()\n",
    "dframe2[\"E\"] = [\"one\", \"one\", \"two\", \"three\", \"four\", \"three\"]\n",
    "dframe2[dframe2[\"E\"].isin([\"two\", \"four\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d1b645-5626-4955-b00e-c14154a9d565",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Other Datasets To Use</h3>\n",
    "    <p>Using additional datasets is optional (though likely to be helpful). Other datasets you can use to supplement the above datasets:</br>\n",
    "    <a href=\"https://www.kaggle.com/carlaperezalmendros/dont-patronize-me\">Don't Patronize Me!</a>  (you will need to sign a privacy form to access the dataset)</br>\n",
    "    Others?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0101614c",
   "metadata": {},
   "source": [
    "----\n",
    "<div class=\"alert alert-info\">\n",
    "    <h4>PRO TIP</h4>\n",
    "    <p>Extract at least one or two features before implementing any of the models.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574fe49c-0488-46f7-8073-36b71989aac2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task: Extract Features From Dataset   (40 Marks)\n",
    "\n",
    "Use **NLTK** to extract **features** from the **Toxic Comments** dataset.\\\n",
    "The **features** will be used to train various Machine Learning models to identify toxic comments.\n",
    "\n",
    "Possible **features** to extract:\n",
    "* check if a word is in a list of swear words, or a list of words that are hate speech\n",
    "* check if punctuation is used in the *body* of a word rather than at the end of the sentence\n",
    "* check if words such as `a$$` are used where letters are replaced by visually similar symbols\n",
    "* sentence length i.e., how many words a sentence has\n",
    "* average word length in a sentence\n",
    "* unusually high number of exclamation marks, which could represent the author being frustrated or angry\n",
    "* etc.\n",
    "\n",
    "\n",
    "### Transforming Data Into Features\n",
    "\n",
    "We represent the data as matrices/vectors. So we are transforming the dataset into a matrix representation. None of the models accept text input! Example:\\\n",
    "`X = [[0, 0], [1, 3], [2, 0], [3, 1]]`\\\n",
    "`Y = [0, 1, 2, 3]`\n",
    "\n",
    "`X` is a features matrix.\n",
    "`Y` is a matrix of the target classes/output the model is trying to predict. In this case we have four classes `[0, 1, 2, 3]`. The actual classes could be non-toxic, toxic, very toxic, and extremely toxic, but we have to replace them by a number.\n",
    "\n",
    "In `X` we have four samples/datapoints/examples, the first being `[0, 0]`. Each column in the sample corresponds to a feature.\\\n",
    "For example, the first column could be number of swear words in comment and the 2nd column could be percentage of capitalized letters in a comment. In matrix entry form:\\\n",
    "`[number of swear words in comment, percentage of capitalized letters in a comment]`\\\n",
    "Thus `[0, 0]` would correspond to a comment with zero swear words and zero capitalized letters.\n",
    "\n",
    "\n",
    "### Rubric\n",
    "\n",
    "We will be evaluating this section in part by how clever your choice of features were (and that you were able to extract them).\n",
    "Simple sets of features may not be as informative as complex setds of features, but simple features are easier to extract from a dataset compared to complex features.\n",
    "\n",
    "A breakdown of the marking for this task:\n",
    "* [**10 marks**] basic features gathered (trivial)\n",
    "* [**25 marks**] quality features gathered (advanced), corresponding to unusual features or clever features most would not have considered\n",
    "* [**5 marks**] formatting the features and output correctly so it can be used immediately downstream for the machine learning classifiers without any further preprocessing needing to be done at that stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbeb03d-2d9c-462e-946a-6739a051fd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EXAMPLE CODE: check if a word is in a list of words\n",
    "# list_of_words = [\"house\", \"hat\", \"war\"]\n",
    "# word = \"hat\"\n",
    "# print(\"Is 'hat' in the wordlist?\", word in list_of_words)\n",
    "\n",
    "# word = \"Hat\"\n",
    "# print(\"Is 'Hat' in the wordlist?\", word in list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbad27fd-292e-46c7-839f-354ac366919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EXAMPLE CODE: check if a character is in a list of characters\n",
    "# character_string = '?.\",!@$%^&*()\\n' # using a String as if it were a list\n",
    "# character = \"?\"\n",
    "# print(\"Is '?' in the characterlist?\", character in character_string)\n",
    "\n",
    "# character = \"\\n\"\n",
    "# print(\"Is '\\\\n' (newline) in the characterlist?\", character in character_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35dae9c-6929-4568-8e9e-ce3e1dd750b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE CODE: for more code fragments that may be useful\n",
    "#               check the Discord server (I will not be adding new snippets to this Task here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5becb7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <h4>WRITE CODE</h4>\n",
    "    In the cell below, write the code that extracts features from the dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b496bf53-1b24-4b5f-a9da-cc23b8f74a00",
   "metadata": {},
   "source": [
    "----\n",
    "<div class=\"alert alert-info\">\n",
    "    <h4>PRO TIP</h4>\n",
    "    Classifiers take two arrays as input: <strong>array X</strong> and <strong>array y</strong>.</br>\n",
    "    <strong>array X</strong> has shape <tt>(number_of_samples, number_of_features)</tt> containing the training samples feature data</br>\n",
    "    <strong>array y</strong> of class labels/outputs (strings or integers) has shape <tt>(number_of_samples)</tt></p>\n",
    "    <p></p>\n",
    "    <p style=\"text-indent:0px\"><tt>print(photos.shape, labels.shape)</br>\n",
    "    num_samples = labels.shape[0]<br>\n",
    "    x = np.reshape(photos, (num_samples, -1))<tt></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ea231bf-3fee-486d-a0ac-5414717fd61d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE IN THIS CELL\n",
    "\n",
    "# read dataset from file\n",
    "import csv\n",
    "import string\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "trainData = pd.read_csv(\"train.csv\")\n",
    "swearWords = pd.read_csv(\"swear_words.csv\")\n",
    "nationalities = pd.read_csv(\"nationalities.csv\")\n",
    "countries = pd.read_csv(\"countries.csv\")\n",
    "\n",
    "def remove_spams(text):\n",
    "    returnedText = ''\n",
    "    for i in range(len(text)-1):\n",
    "        if text[i] != text[i+1]:\n",
    "            returnedText += text[i]\n",
    "    returnedText+=text[len(text)-1]\n",
    "    return returnedText\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    returnedText = text\n",
    "    for s in returnedText:\n",
    "        if s in string.punctuation:\n",
    "            s = ''\n",
    "    return returnedText\n",
    "\n",
    "def decrypt(text):\n",
    "    returnedText = text\n",
    "    returnedText = returnedText.replace('@', 'a')\n",
    "    returnedText = returnedText.replace('3', 'e')\n",
    "    returnedText = returnedText.replace('1', 'i')\n",
    "    returnedText = returnedText.replace('4', 'a')\n",
    "    returnedText = returnedText.replace('0', 'o')\n",
    "    returnedText = returnedText.replace('$', 's')\n",
    "    returnedText = returnedText.replace('9', 'g')\n",
    "    return returnedText\n",
    "\n",
    "def convert_to_arr(text):\n",
    "    return nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "def extract_features(text):\n",
    "    textDecrypted = decrypt(remove_punctuations(remove_spams(text.lower())))\n",
    "    \n",
    "    features = []\n",
    "    # check swear words and swear words percentage\n",
    "    count = 0\n",
    "    for row in swearWords.itertuples():\n",
    "        count+=len(re.findall(row.swear_words, textDecrypted))\n",
    "    features.append(count)\n",
    "    features.append(float(count) / len(convert_to_arr(textDecrypted)))\n",
    "    \n",
    "    # check threats\n",
    "    count = 0\n",
    "    for s in convert_to_arr(textDecrypted):\n",
    "        if s == 'kill' or s == 'cut' or s == 'shoot' or s == 'die' or s == 'rape' or s == 'hang' or s == 'murder':\n",
    "            count+=1\n",
    "    features.append(count)\n",
    "\n",
    "    # check family members\n",
    "    count = 0\n",
    "    count+=len(re.findall('mama', textDecrypted))\n",
    "    count+=len(re.findall('mom', textDecrypted))\n",
    "    count+=len(re.findall('dad', textDecrypted))\n",
    "    count+=len(re.findall('mother', textDecrypted))\n",
    "    count+=len(re.findall('father', textDecrypted))\n",
    "    count+=len(re.findall('brother', textDecrypted))\n",
    "    count+=len(re.findall('sister', textDecrypted))\n",
    "    features.append(count)\n",
    "    \n",
    "    # check gender slurs\n",
    "    count = 0\n",
    "    count+=len(re.findall('gay', textDecrypted))\n",
    "    count+=len(re.findall('fag', textDecrypted))\n",
    "    count+=len(re.findall('queer', textDecrypted))\n",
    "    count+=len(re.findall('homo', textDecrypted))\n",
    "    features.append(count)\n",
    "\n",
    "    #check n-word\n",
    "    count = 0\n",
    "    count+=len(re.findall('n...er', textDecrypted))\n",
    "    features.append(count)\n",
    "\n",
    "    #check f-word\n",
    "    count = 0\n",
    "    count+=len(re.findall('f.*k.*', textDecrypted))\n",
    "    features.append(count)\n",
    "\n",
    "    #check nationality\n",
    "    count = 0\n",
    "    for row in nationalities.itertuples():\n",
    "        if row.nationalities.lower() in textDecrypted:\n",
    "            count+= 1\n",
    "    features.append(count)\n",
    "\n",
    "    #check countries\n",
    "    count = 0\n",
    "    for row in countries.itertuples():\n",
    "        if row.countries.lower() in textDecrypted:\n",
    "            count+= 1\n",
    "    features.append(count)\n",
    "    \n",
    "    # check capital words\n",
    "    count = 0\n",
    "    for s in convert_to_arr(remove_punctuations(text)):\n",
    "        if s == s.upper() and s != 'I':\n",
    "            count += 1\n",
    "    features.append(float(count)/ len(convert_to_arr(remove_punctuations(text))))\n",
    "    \n",
    "    # check word 'you'\n",
    "    count = 0\n",
    "    count+=len(re.findall('you', textDecrypted))\n",
    "    features.append(float(count) / len(convert_to_arr(remove_punctuations(text))))\n",
    "\n",
    "    #check consecutive characters\n",
    "    count = 0\n",
    "    for i in range(len(text)-2):\n",
    "        if text[i] == text[i+1] and text[i] == text[i+2]:\n",
    "            count += 1    \n",
    "    features.append(count)  \n",
    "               \n",
    "    #check longest word\n",
    "    count = 0\n",
    "    max = 0\n",
    "    for s in convert_to_arr(text):\n",
    "        if len(s) > max:\n",
    "            max = len(s)\n",
    "    features.append(max)\n",
    "    \n",
    "    return features\n",
    "\n",
    "for i in range(80000):\n",
    "    text = trainData.iloc[i].comment_text\n",
    "    extractedFeatures = extract_features(text)\n",
    "    with open('features.txt', 'a') as out:\n",
    "        for i in range(len(extractedFeatures)):\n",
    "            out.write(str(extractedFeatures[i]) + ' ')\n",
    "        out.write('\\n')\n",
    "\n",
    "\n",
    "# iterate over each data sample in the dataset\n",
    "#     extract features from each data sample\n",
    "\n",
    "\n",
    "\n",
    "# prepare the features in the correct format\n",
    "\n",
    "\n",
    "# normalize/scale the features and outputs in the range [0-1]\n",
    "#     note that each model may have a different set of requirements as to how it expects its input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6362c59e-1ddc-496a-9024-ba06b2bb177b",
   "metadata": {},
   "source": [
    "## Task: Decision Tree   (5 Marks)\n",
    "\n",
    "Build a *decision tree* model to identify toxic comments.\n",
    "\n",
    "From Norvig & Russel's \"*AI: A Modern Approach*\" (pg. 707, 3rd ed.):\n",
    "> In many areas of industry and commerce, decision trees are usually the first method tried when a classification method is to be extracted from a data set. One important property of decision trees is that it is possible for a human to understand the reason for the output of the learning algorithm. (Indeed, this is a legal requirement for financial decisions that are subject to anti-discrimination laws.) This is a property not shared by some other representations, such as neural networks.\n",
    "\n",
    "From https://scikit-learn.org/stable/modules/tree.html:\n",
    "> `DecisionTreeClassifier` is capable of both:\n",
    "> * **binary classification** where the labels are `[-1, 1]`\n",
    "> * **multiclass classification** where the labels are `[0, ..., K-1]`\n",
    "\n",
    "\n",
    "### Type Of Task We Are Working With\n",
    "\n",
    "Note our task isn't a **classification** task (i.e., predicting a *discrete* value) but a **regression** task (i.e., predicting a *continuous* value). We could pursue one of two approaches:\n",
    "* keep the task as a regression task and use regression versions for each model (e.g., *decision tree regression*, *SVM for regression*, *neural network for regression*, etc.)\n",
    "* convert the regression task into a classification task where we are predicting a range of values rather than a specific value (i.e., discretizing the output space)\n",
    "\n",
    "To discretize the output we are predicting (i.e., convert a continuous value into a set of points/classes/categories), instead of predicting the temperature from 0 degrees Fahrenheit to 100 degrees Fahrenheit, we predict a temperature range:\n",
    "* **freezing** (0 degrees to 20 degrees Fahrenheit)\n",
    "* **cold** (20 degrees to 40 degrees Fahrenheit)\n",
    "* **moderate** (40 degrees to 60 degrees Fahrenheit)\n",
    "* **warm** (60 degrees to 80 degrees Fahrenheit)\n",
    "* **hot** (80 degrees to 100 degrees Fahrenheit)\n",
    "\n",
    "which transforms the regression problem into a 5-class classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95089c7a-f618-4671-8e57-9a477f56a315",
   "metadata": {},
   "source": [
    "### Decision Trees For Classification\n",
    "\n",
    "Example code for a **Decision Tree** performing classification. The **Decision Tree** model is predicting one category from a set of categories, such as which genre a film belongs to (`Horror`, `Comedy`, `Action`, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8560355a-3cd8-4137-92b1-21f02198c805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EXAMPLE CODE: COMMENT OUT THIS CODE CELL AFTER IMPLEMENTING YOUR CODE BELOW\n",
    "# from sklearn import tree\n",
    "\n",
    "# X = [[0, 0, -1], [1, 1, 1], [1, 10, 9], [-3, 0, 33]]\n",
    "# Y = [0, 1, 4, 1]\n",
    "\n",
    "# # DecisionTreeClassifier takes as input two arrays: X & Y\n",
    "# #    an array X, sparse or dense, of shape (number_of_samples, number_of_features) holding the training samples\n",
    "# #    and an array Y of integer values, of shape (number_of_samples) holding the class labels for the training samples\n",
    "# clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "# # train the decision tree classifier model\n",
    "# clf = clf.fit(X, Y)\n",
    "\n",
    "# # after being fitted, predict from a new set of samples\n",
    "# clf.predict([[2., 2., 10.]])\n",
    "\n",
    "\n",
    "# # plot a visualization of the decision tree\n",
    "# #tree.plot_tree(clf)\n",
    "\n",
    "\n",
    "# # a text visualization of the decision tree\n",
    "# #from sklearn.tree import export_text\n",
    "# #text_tree = export_text(clf, feature_names=[\"First Feature\", \"Height Feature\", \"Salary Feature\"])\n",
    "# #print(\"Text visualization of decision tree:\\n\", text_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df590be3-a107-4399-b79e-ebfca3b6cf15",
   "metadata": {},
   "source": [
    "### Decision Trees For Regression\n",
    "\n",
    "Regression using **Decision Trees** is [found here](https://scikit-learn.org/stable/modules/tree.html#regression).\n",
    "\n",
    "Example code of a **Decision Tree** for regression (where the **Decision Tree** model is predicting a continuous value):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8db8bf-3ab7-4612-8fd1-4aa1bd791e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EXAMPLE CODE: COMMENT OUT THIS CODE CELL AFTER IMPLEMENTING YOUR CODE BELOW\n",
    "# # code from https://scikit-learn.org/stable/modules/tree.html#regression\n",
    "\n",
    "# from sklearn import tree\n",
    "\n",
    "# X = [[0, 0], [2, 2]]\n",
    "# y = [0.5, 2.5]\n",
    "\n",
    "\n",
    "# clf = tree.DecisionTreeRegressor()\n",
    "\n",
    "# # train the decision tree regression model\n",
    "# clf = clf.fit(X, y)\n",
    "\n",
    "# # after being fitted, predict from a new set of samples\n",
    "# clf.predict([[1, 1]])\n",
    "\n",
    "# # plot a visualization of the decision tree\n",
    "# tree.plot_tree(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1689ef-d3d7-443f-844e-abb627da153f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <h4>WRITE CODE</h4>\n",
    "    In the cell below, write the code that implements a Decision Tree classifier.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b54a755e-41ef-4a0d-b0fc-85666a37febc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(255.9300330808107, 214.91162790697675, 'X[1] <= 0.252\\nmse = 1640.709\\nsamples = 80000\\nvalue = 10.305'),\n",
       " Text(204.74052025946375, 209.85488372093022, 'X[1] <= 0.135\\nmse = 700.696\\nsamples = 77116\\nvalue = 5.642'),\n",
       " Text(149.03042873947567, 204.79813953488372, 'X[1] <= 0.079\\nmse = 370.664\\nsamples = 71850\\nvalue = 3.539'),\n",
       " Text(96.65039665302473, 199.74139534883722, 'X[10] <= 0.056\\nmse = 213.153\\nsamples = 60083\\nvalue = 2.303'),\n",
       " Text(59.89070649108877, 194.6846511627907, 'X[12] <= 2.5\\nmse = 135.636\\nsamples = 49711\\nvalue = 1.535'),\n",
       " Text(40.60592333328446, 189.62790697674419, 'X[6] <= 0.5\\nmse = 3904.0\\nsamples = 5\\nvalue = 134.0'),\n",
       " Text(40.536896185931056, 184.57116279069766, 'X[9] <= 0.429\\nmse = 355.556\\nsamples = 3\\nvalue = 93.333'),\n",
       " Text(40.50238261225436, 179.51441860465115, 'mse = 0.0\\nsamples = 1\\nvalue = 120.0'),\n",
       " Text(40.571409759607754, 179.51441860465115, 'mse = 0.0\\nsamples = 2\\nvalue = 80.0'),\n",
       " Text(40.674950480637854, 184.57116279069766, 'X[9] <= 0.5\\nmse = 3025.0\\nsamples = 2\\nvalue = 195.0'),\n",
       " Text(40.640436906961156, 179.51441860465115, 'mse = 0.0\\nsamples = 1\\nvalue = 250.0'),\n",
       " Text(40.70946405431455, 179.51441860465115, 'mse = 0.0\\nsamples = 1\\nvalue = 140.0'),\n",
       " Text(79.17548964889309, 189.62790697674419, 'X[9] <= 0.893\\nmse = 133.492\\nsamples = 49706\\nvalue = 1.522'),\n",
       " Text(61.96407273339052, 184.57116279069766, 'X[1] <= 0.055\\nmse = 123.636\\nsamples = 49388\\nvalue = 1.433'),\n",
       " Text(40.778491201667954, 179.51441860465115, 'X[2] <= 2.5\\nmse = 86.297\\nsamples = 38225\\nvalue = 1.052'),\n",
       " Text(31.210919645527806, 174.45767441860465, 'X[12] <= 9.5\\nmse = 83.035\\nsamples = 38218\\nvalue = 1.043'),\n",
       " Text(12.179317254277613, 169.40093023255815, 'X[1] <= 0.037\\nmse = 147.647\\nsamples = 9851\\nvalue = 1.809'),\n",
       " Text(3.9901511383024695, 164.34418604651162, 'X[12] <= 4.5\\nmse = 119.946\\nsamples = 8121\\nvalue = 1.437'),\n",
       " Text(0.4422051627327112, 159.28744186046512, 'X[10] <= 0.019\\nmse = 1094.553\\nsamples = 184\\nvalue = 7.391'),\n",
       " Text(0.40769158905601177, 154.2306976744186, 'X[12] <= 3.5\\nmse = 436.643\\nsamples = 183\\nvalue = 5.492'),\n",
       " Text(0.13805429470679764, 149.1739534883721, 'X[9] <= 0.226\\nmse = 2120.0\\nsamples = 10\\nvalue = 25.0'),\n",
       " Text(0.06902714735339882, 144.1172093023256, 'X[11] <= 400.0\\nmse = 100.0\\nsamples = 5\\nvalue = 5.0'),\n",
       " Text(0.03451357367669941, 139.06046511627906, 'mse = 0.0\\nsamples = 4\\nvalue = 0.0'),\n",
       " Text(0.10354072103009823, 139.06046511627906, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(0.20708144206019646, 144.1172093023256, 'X[9] <= 0.33\\nmse = 3340.0\\nsamples = 5\\nvalue = 45.0'),\n",
       " Text(0.17256786838349705, 139.06046511627906, 'mse = 756.25\\nsamples = 2\\nvalue = 112.5'),\n",
       " Text(0.24159501573689587, 139.06046511627906, 'mse = 0.0\\nsamples = 3\\nvalue = 0.0'),\n",
       " Text(0.6773288834052259, 149.1739534883721, 'X[9] <= 0.129\\nmse = 316.07\\nsamples = 173\\nvalue = 4.364'),\n",
       " Text(0.3451357367669941, 144.1172093023256, 'X[3] <= 0.5\\nmse = 780.726\\nsamples = 41\\nvalue = 10.61'),\n",
       " Text(0.31062216309029467, 139.06046511627906, 'X[9] <= 0.08\\nmse = 814.694\\nsamples = 39\\nvalue = 11.154'),\n",
       " Text(0.24159501573689587, 134.00372093023256, 'X[9] <= 0.038\\nmse = 1009.76\\nsamples = 25\\nvalue = 13.2'),\n",
       " Text(0.20708144206019646, 128.94697674418603, 'mse = 1045.79\\nsamples = 24\\nvalue = 12.708'),\n",
       " Text(0.2761085894135953, 128.94697674418603, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(0.37964931044369354, 134.00372093023256, 'X[9] <= 0.095\\nmse = 445.536\\nsamples = 14\\nvalue = 7.5'),\n",
       " Text(0.3451357367669941, 128.94697674418603, 'mse = 0.0\\nsamples = 5\\nvalue = 0.0'),\n",
       " Text(0.4141628841203929, 128.94697674418603, 'X[9] <= 0.106\\nmse = 644.444\\nsamples = 9\\nvalue = 11.667'),\n",
       " Text(0.37964931044369354, 123.89023255813953, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(0.4486764577970923, 123.89023255813953, 'X[9] <= 0.118\\nmse = 700.0\\nsamples = 8\\nvalue = 10.0'),\n",
       " Text(0.4141628841203929, 118.83348837209302, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(0.48319003147379175, 118.83348837209302, 'X[5] <= 0.5\\nmse = 783.673\\nsamples = 7\\nvalue = 11.429'),\n",
       " Text(0.4486764577970923, 113.77674418604651, 'mse = 888.889\\nsamples = 6\\nvalue = 13.333'),\n",
       " Text(0.5177036051504912, 113.77674418604651, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(0.37964931044369354, 139.06046511627906, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(1.0095220300434578, 144.1172093023256, 'X[11] <= 2.5\\nmse = 155.865\\nsamples = 132\\nvalue = 2.424'),\n",
       " Text(0.8455825550791356, 139.06046511627906, 'X[9] <= 0.477\\nmse = 115.023\\nsamples = 126\\nvalue = 1.905'),\n",
       " Text(0.6212443261805893, 134.00372093023256, 'X[9] <= 0.186\\nmse = 23.605\\nsamples = 88\\nvalue = 0.909'),\n",
       " Text(0.58673075250389, 128.94697674418603, 'mse = 0.0\\nsamples = 27\\nvalue = 0.0'),\n",
       " Text(0.6557578998572888, 128.94697674418603, 'X[9] <= 0.194\\nmse = 33.526\\nsamples = 61\\nvalue = 1.311'),\n",
       " Text(0.6212443261805893, 123.89023255813953, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(0.6902714735339882, 123.89023255813953, 'X[9] <= 0.211\\nmse = 24.576\\nsamples = 60\\nvalue = 0.917'),\n",
       " Text(0.6212443261805893, 118.83348837209302, 'X[11] <= 0.5\\nmse = 110.204\\nsamples = 7\\nvalue = 4.286'),\n",
       " Text(0.58673075250389, 113.77674418604651, 'mse = 125.0\\nsamples = 6\\nvalue = 5.0'),\n",
       " Text(0.6557578998572888, 113.77674418604651, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(0.7592986208873871, 118.83348837209302, 'X[9] <= 0.414\\nmse = 11.57\\nsamples = 53\\nvalue = 0.472'),\n",
       " Text(0.7247850472106876, 113.77674418604651, 'mse = 0.0\\nsamples = 42\\nvalue = 0.0'),\n",
       " Text(0.7938121945640865, 113.77674418604651, 'X[9] <= 0.433\\nmse = 51.653\\nsamples = 11\\nvalue = 2.273'),\n",
       " Text(0.7592986208873871, 108.72, 'X[11] <= 0.5\\nmse = 76.531\\nsamples = 7\\nvalue = 3.571'),\n",
       " Text(0.7247850472106876, 103.66325581395348, 'mse = 86.806\\nsamples = 6\\nvalue = 4.167'),\n",
       " Text(0.7938121945640865, 103.66325581395348, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(0.8283257682407859, 108.72, 'mse = 0.0\\nsamples = 4\\nvalue = 0.0'),\n",
       " Text(1.0699207839776816, 134.00372093023256, 'X[9] <= 0.631\\nmse = 319.114\\nsamples = 38\\nvalue = 4.211'),\n",
       " Text(1.0354072103009824, 128.94697674418603, 'X[9] <= 0.62\\nmse = 576.0\\nsamples = 20\\nvalue = 8.0'),\n",
       " Text(1.0008936366242829, 123.89023255813953, 'X[9] <= 0.519\\nmse = 375.0\\nsamples = 16\\nvalue = 5.0'),\n",
       " Text(0.9663800629475835, 118.83348837209302, 'X[11] <= 0.5\\nmse = 783.673\\nsamples = 7\\nvalue = 11.429'),\n",
       " Text(0.9318664892708841, 113.77674418604651, 'X[6] <= 0.5\\nmse = 1024.0\\nsamples = 5\\nvalue = 16.0'),\n",
       " Text(0.8973529155941846, 108.72, 'mse = 1200.0\\nsamples = 4\\nvalue = 20.0'),\n",
       " Text(0.9663800629475835, 108.72, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(1.0008936366242829, 113.77674418604651, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(1.0354072103009824, 118.83348837209302, 'mse = 0.0\\nsamples = 9\\nvalue = 0.0'),\n",
       " Text(1.0699207839776816, 123.89023255813953, 'mse = 1200.0\\nsamples = 4\\nvalue = 20.0'),\n",
       " Text(1.1044343576543811, 128.94697674418603, 'mse = 0.0\\nsamples = 18\\nvalue = 0.0'),\n",
       " Text(1.17346150500778, 139.06046511627906, 'X[9] <= 0.352\\nmse = 888.889\\nsamples = 6\\nvalue = 13.333'),\n",
       " Text(1.1389479313310806, 134.00372093023256, 'mse = 0.0\\nsamples = 1\\nvalue = 80.0'),\n",
       " Text(1.2079750786844794, 134.00372093023256, 'mse = 0.0\\nsamples = 5\\nvalue = 0.0'),\n",
       " Text(0.4767187364094106, 154.2306976744186, 'mse = 0.0\\nsamples = 1\\nvalue = 355.0'),\n",
       " Text(7.538097113872228, 159.28744186046512, 'X[9] <= 0.05\\nmse = 96.511\\nsamples = 7937\\nvalue = 1.299'),\n",
       " Text(1.66918967066881, 154.2306976744186, 'X[10] <= 0.049\\nmse = 266.162\\nsamples = 589\\nvalue = 3.336'),\n",
       " Text(1.5269559978995997, 149.1739534883721, 'X[12] <= 5.5\\nmse = 235.565\\nsamples = 586\\nvalue = 3.114'),\n",
       " Text(1.346029373391277, 144.1172093023256, 'X[6] <= 0.5\\nmse = 771.637\\nsamples = 49\\nvalue = 11.531'),\n",
       " Text(1.3115157997145777, 139.06046511627906, 'X[11] <= 15.0\\nmse = 842.497\\nsamples = 44\\nvalue = 12.841'),\n",
       " Text(1.2770022260378782, 134.00372093023256, 'X[11] <= 0.5\\nmse = 858.572\\nsamples = 43\\nvalue = 12.558'),\n",
       " Text(1.2424886523611787, 128.94697674418603, 'mse = 875.17\\nsamples = 42\\nvalue = 12.857'),\n",
       " Text(1.3115157997145777, 128.94697674418603, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(1.346029373391277, 134.00372093023256, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(1.3805429470679764, 139.06046511627906, 'mse = 0.0\\nsamples = 5\\nvalue = 0.0'),\n",
       " Text(1.7078826224079224, 144.1172093023256, 'X[1] <= 0.034\\nmse = 179.597\\nsamples = 537\\nvalue = 2.346'),\n",
       " Text(1.5757603481768074, 139.06046511627906, 'X[12] <= 6.5\\nmse = 170.515\\nsamples = 530\\nvalue = 2.226'),\n",
       " Text(1.415056520744676, 134.00372093023256, 'X[8] <= 0.5\\nmse = 334.438\\nsamples = 65\\nvalue = 3.846'),\n",
       " Text(1.3805429470679764, 128.94697674418603, 'X[11] <= 0.5\\nmse = 344.57\\nsamples = 63\\nvalue = 3.968'),\n",
       " Text(1.346029373391277, 123.89023255813953, 'X[3] <= 0.5\\nmse = 355.335\\nsamples = 61\\nvalue = 4.098'),\n",
       " Text(1.3115157997145777, 118.83348837209302, 'X[7] <= 0.5\\nmse = 360.972\\nsamples = 60\\nvalue = 4.167'),\n",
       " Text(1.2770022260378782, 113.77674418604651, 'mse = 366.791\\nsamples = 59\\nvalue = 4.237'),\n",
       " Text(1.346029373391277, 113.77674418604651, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(1.3805429470679764, 118.83348837209302, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(1.415056520744676, 123.89023255813953, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(1.4495700944213752, 128.94697674418603, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(1.7364641756089392, 134.00372093023256, 'X[12] <= 7.5\\nmse = 147.183\\nsamples = 465\\nvalue = 2.0'),\n",
       " Text(1.5531108154514734, 128.94697674418603, 'X[11] <= 1.0\\nmse = 22.559\\nsamples = 160\\nvalue = 0.938'),\n",
       " Text(1.4840836680980747, 123.89023255813953, 'X[6] <= 0.5\\nmse = 19.271\\nsamples = 157\\nvalue = 0.796'),\n",
       " Text(1.4495700944213752, 118.83348837209302, 'X[9] <= 0.011\\nmse = 21.089\\nsamples = 143\\nvalue = 0.874'),\n",
       " Text(1.415056520744676, 113.77674418604651, 'X[5] <= 0.5\\nmse = 22.291\\nsamples = 135\\nvalue = 0.926'),\n",
       " Text(1.3805429470679764, 108.72, 'X[7] <= 0.5\\nmse = 23.46\\nsamples = 128\\nvalue = 0.977'),\n",
       " Text(1.346029373391277, 103.66325581395348, 'X[3] <= 0.5\\nmse = 24.0\\nsamples = 125\\nvalue = 1.0'),\n",
       " Text(1.3115157997145777, 98.60651162790697, 'mse = 24.185\\nsamples = 124\\nvalue = 1.008'),\n",
       " Text(1.3805429470679764, 98.60651162790697, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(1.415056520744676, 103.66325581395348, 'mse = 0.0\\nsamples = 3\\nvalue = 0.0'),\n",
       " Text(1.4495700944213752, 108.72, 'mse = 0.0\\nsamples = 7\\nvalue = 0.0'),\n",
       " Text(1.4840836680980747, 113.77674418604651, 'mse = 0.0\\nsamples = 8\\nvalue = 0.0'),\n",
       " Text(1.5185972417747742, 118.83348837209302, 'mse = 0.0\\nsamples = 14\\nvalue = 0.0'),\n",
       " Text(1.6221379628048722, 123.89023255813953, 'X[10] <= 0.013\\nmse = 138.889\\nsamples = 3\\nvalue = 8.333'),\n",
       " Text(1.587624389128173, 118.83348837209302, 'mse = 156.25\\nsamples = 2\\nvalue = 12.5'),\n",
       " Text(1.6566515364815717, 118.83348837209302, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(1.9198175357664047, 128.94697674418603, 'X[9] <= 0.021\\nmse = 211.657\\nsamples = 305\\nvalue = 2.557'),\n",
       " Text(1.8205910114458939, 123.89023255813953, 'X[9] <= 0.001\\nmse = 247.883\\nsamples = 255\\nvalue = 2.863'),\n",
       " Text(1.7256786838349705, 118.83348837209302, 'X[6] <= 0.5\\nmse = 175.875\\nsamples = 249\\nvalue = 2.269'),\n",
       " Text(1.639394749643222, 113.77674418604651, 'X[3] <= 0.5\\nmse = 105.569\\nsamples = 234\\nvalue = 1.816'),\n",
       " Text(1.6048811759665227, 108.72, 'X[12] <= 8.5\\nmse = 103.705\\nsamples = 233\\nvalue = 1.717'),\n",
       " Text(1.5358540286131237, 103.66325581395348, 'X[8] <= 0.5\\nmse = 146.899\\nsamples = 118\\nvalue = 2.331'),\n",
       " Text(1.5013404549364244, 98.60651162790697, 'X[5] <= 0.5\\nmse = 150.586\\nsamples = 115\\nvalue = 2.391'),\n",
       " Text(1.466826881259725, 93.54976744186047, 'X[10] <= 0.018\\nmse = 154.462\\nsamples = 112\\nvalue = 2.455'),\n",
       " Text(1.4323133075830254, 88.49302325581394, 'X[1] <= 0.017\\nmse = 155.799\\nsamples = 111\\nvalue = 2.477'),\n",
       " Text(1.3977997339063262, 83.43627906976744, 'X[11] <= 0.5\\nmse = 157.159\\nsamples = 110\\nvalue = 2.5'),\n",
       " Text(1.3632861602296267, 78.37953488372094, 'mse = 158.543\\nsamples = 109\\nvalue = 2.523'),\n",
       " Text(1.4323133075830254, 78.37953488372094, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(1.466826881259725, 83.43627906976744, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(1.5013404549364244, 88.49302325581394, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(1.5358540286131237, 93.54976744186047, 'mse = 0.0\\nsamples = 3\\nvalue = 0.0'),\n",
       " Text(1.5703676022898232, 98.60651162790697, 'mse = 0.0\\nsamples = 3\\nvalue = 0.0'),\n",
       " Text(1.6739083233199215, 103.66325581395348, 'X[8] <= 0.5\\nmse = 58.601\\nsamples = 115\\nvalue = 1.087'),\n",
       " Text(1.639394749643222, 98.60651162790697, 'X[5] <= 0.5\\nmse = 59.617\\nsamples = 113\\nvalue = 1.106'),\n",
       " Text(1.6048811759665227, 93.54976744186047, 'X[10] <= 0.022\\nmse = 60.669\\nsamples = 111\\nvalue = 1.126'),\n",
       " Text(1.5703676022898232, 88.49302325581394, 'X[7] <= 0.5\\nmse = 61.209\\nsamples = 110\\nvalue = 1.136'),\n",
       " Text(1.5358540286131237, 83.43627906976744, 'mse = 61.758\\nsamples = 109\\nvalue = 1.147'),\n",
       " Text(1.6048811759665227, 83.43627906976744, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(1.639394749643222, 88.49302325581394, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(1.6739083233199215, 93.54976744186047, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(1.7084218969966207, 98.60651162790697, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(1.6739083233199215, 108.72, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(1.811962618026719, 113.77674418604651, 'X[12] <= 8.5\\nmse = 1219.556\\nsamples = 15\\nvalue = 9.333'),\n",
       " Text(1.7774490443500197, 108.72, 'X[10] <= 0.018\\nmse = 1935.802\\nsamples = 9\\nvalue = 15.556'),\n",
       " Text(1.7429354706733202, 103.66325581395348, 'mse = 2400.0\\nsamples = 7\\nvalue = 20.0'),\n",
       " Text(1.811962618026719, 103.66325581395348, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(1.8464761917034185, 108.72, 'mse = 0.0\\nsamples = 6\\nvalue = 0.0'),\n",
       " Text(1.9155033390568172, 118.83348837209302, 'X[9] <= 0.009\\nmse = 2614.583\\nsamples = 6\\nvalue = 27.5'),\n",
       " Text(1.880989765380118, 113.77674418604651, 'mse = 0.0\\nsamples = 1\\nvalue = 140.0'),\n",
       " Text(1.9500169127335167, 113.77674418604651, 'X[1] <= 0.019\\nmse = 100.0\\nsamples = 5\\nvalue = 5.0'),\n",
       " Text(1.9155033390568172, 108.72, 'mse = 0.0\\nsamples = 4\\nvalue = 0.0'),\n",
       " Text(1.984530486410216, 108.72, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(2.0190440600869155, 123.89023255813953, 'X[10] <= 0.024\\nmse = 24.0\\nsamples = 50\\nvalue = 1.0'),\n",
       " Text(1.984530486410216, 118.83348837209302, 'mse = 0.0\\nsamples = 32\\nvalue = 0.0'),\n",
       " Text(2.053557633763615, 118.83348837209302, 'X[10] <= 0.027\\nmse = 61.728\\nsamples = 18\\nvalue = 2.778'),\n",
       " Text(2.0190440600869155, 113.77674418604651, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(2.0880712074403145, 113.77674418604651, 'X[12] <= 8.5\\nmse = 34.602\\nsamples = 17\\nvalue = 1.471'),\n",
       " Text(2.053557633763615, 108.72, 'X[9] <= 0.04\\nmse = 117.188\\nsamples = 4\\nvalue = 6.25'),\n",
       " Text(2.0190440600869155, 103.66325581395348, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(2.0880712074403145, 103.66325581395348, 'X[9] <= 0.047\\nmse = 156.25\\nsamples = 2\\nvalue = 12.5'),\n",
       " Text(2.053557633763615, 98.60651162790697, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(2.1225847811170135, 98.60651162790697, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(2.1225847811170135, 108.72, 'mse = 0.0\\nsamples = 13\\nvalue = 0.0'),\n",
       " Text(1.8400048966390374, 139.06046511627906, 'X[12] <= 8.5\\nmse = 783.673\\nsamples = 7\\nvalue = 11.429'),\n",
       " Text(1.805491322962338, 134.00372093023256, 'mse = 0.0\\nsamples = 1\\nvalue = 80.0'),\n",
       " Text(1.8745184703157367, 134.00372093023256, 'mse = 0.0\\nsamples = 6\\nvalue = 0.0'),\n",
       " Text(1.8114233434380207, 149.1739534883721, 'X[12] <= 7.5\\nmse = 4355.556\\nsamples = 3\\nvalue = 46.667'),\n",
       " Text(1.7769097697613212, 144.1172093023256, 'mse = 0.0\\nsamples = 1\\nvalue = 140.0'),\n",
       " Text(1.84593691711472, 144.1172093023256, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(13.407004557075645, 154.2306976744186, 'X[3] <= 0.5\\nmse = 82.553\\nsamples = 7348\\nvalue = 1.136'),\n",
       " Text(11.08876210770512, 149.1739534883721, 'X[1] <= 0.022\\nmse = 73.308\\nsamples = 7286\\nvalue = 1.084'),\n",
       " Text(7.655617896651924, 144.1172093023256, 'X[12] <= 7.5\\nmse = 68.836\\nsamples = 6191\\nvalue = 0.96'),\n",
       " Text(4.3229936263155695, 139.06046511627906, 'X[9] <= 0.155\\nmse = 100.588\\nsamples = 2283\\nvalue = 1.321'),\n",
       " Text(2.7330436155236346, 134.00372093023256, 'X[9] <= 0.117\\nmse = 157.926\\nsamples = 517\\nvalue = 2.099'),\n",
       " Text(2.390064977111434, 128.94697674418603, 'X[6] <= 0.5\\nmse = 17.386\\nsamples = 260\\nvalue = 0.635'),\n",
       " Text(2.3555514034347347, 123.89023255813953, 'X[9] <= 0.09\\nmse = 21.114\\nsamples = 213\\nvalue = 0.775'),\n",
       " Text(2.2606390758238115, 118.83348837209302, 'X[9] <= 0.064\\nmse = 7.811\\nsamples = 79\\nvalue = 0.316'),\n",
       " Text(2.226125502147112, 113.77674418604651, 'X[12] <= 6.5\\nmse = 29.688\\nsamples = 20\\nvalue = 1.25'),\n",
       " Text(2.1916119284704125, 108.72, 'X[9] <= 0.059\\nmse = 100.0\\nsamples = 5\\nvalue = 5.0'),\n",
       " Text(2.157098354793713, 103.66325581395348, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(2.226125502147112, 103.66325581395348, 'X[11] <= 1.0\\nmse = 138.889\\nsamples = 3\\nvalue = 8.333'),\n",
       " Text(2.1916119284704125, 98.60651162790697, 'mse = 156.25\\nsamples = 2\\nvalue = 12.5'),\n",
       " Text(2.2606390758238115, 98.60651162790697, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(2.2606390758238115, 108.72, 'mse = 0.0\\nsamples = 15\\nvalue = 0.0'),\n",
       " Text(2.295152649500511, 113.77674418604651, 'mse = 0.0\\nsamples = 59\\nvalue = 0.0'),\n",
       " Text(2.4504637310456583, 118.83348837209302, 'X[9] <= 0.094\\nmse = 28.759\\nsamples = 134\\nvalue = 1.045'),\n",
       " Text(2.3641797968539096, 113.77674418604651, 'X[9] <= 0.092\\nmse = 60.586\\nsamples = 23\\nvalue = 2.391'),\n",
       " Text(2.32966622317721, 108.72, 'X[12] <= 6.5\\nmse = 27.118\\nsamples = 22\\nvalue = 1.136'),\n",
       " Text(2.295152649500511, 103.66325581395348, 'mse = 0.0\\nsamples = 6\\nvalue = 0.0'),\n",
       " Text(2.3641797968539096, 103.66325581395348, 'X[8] <= 0.5\\nmse = 36.621\\nsamples = 16\\nvalue = 1.562'),\n",
       " Text(2.32966622317721, 98.60651162790697, 'X[5] <= 0.5\\nmse = 41.454\\nsamples = 14\\nvalue = 1.786'),\n",
       " Text(2.295152649500511, 93.54976744186047, 'X[11] <= 0.5\\nmse = 47.743\\nsamples = 12\\nvalue = 2.083'),\n",
       " Text(2.2606390758238115, 88.49302325581394, 'X[7] <= 0.5\\nmse = 51.653\\nsamples = 11\\nvalue = 2.273'),\n",
       " Text(2.226125502147112, 83.43627906976744, 'mse = 56.25\\nsamples = 10\\nvalue = 2.5'),\n",
       " Text(2.295152649500511, 83.43627906976744, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(2.32966622317721, 88.49302325581394, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(2.3641797968539096, 93.54976744186047, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(2.398693370530609, 98.60651162790697, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(2.398693370530609, 108.72, 'mse = 0.0\\nsamples = 1\\nvalue = 30.0'),\n",
       " Text(2.5367476652374066, 113.77674418604651, 'X[10] <= 0.017\\nmse = 21.711\\nsamples = 111\\nvalue = 0.766'),\n",
       " Text(2.502234091560707, 108.72, 'X[12] <= 5.5\\nmse = 26.049\\nsamples = 92\\nvalue = 0.924'),\n",
       " Text(2.467720517884008, 103.66325581395348, 'mse = 0.0\\nsamples = 12\\nvalue = 0.0'),\n",
       " Text(2.5367476652374066, 103.66325581395348, 'X[12] <= 6.5\\nmse = 29.809\\nsamples = 80\\nvalue = 1.062'),\n",
       " Text(2.467720517884008, 98.60651162790697, 'X[9] <= 0.102\\nmse = 55.556\\nsamples = 21\\nvalue = 1.667'),\n",
       " Text(2.4332069442073085, 93.54976744186047, 'mse = 133.984\\nsamples = 8\\nvalue = 4.375'),\n",
       " Text(2.502234091560707, 93.54976744186047, 'mse = 0.0\\nsamples = 13\\nvalue = 0.0'),\n",
       " Text(2.6057748125908056, 98.60651162790697, 'X[9] <= 0.11\\nmse = 20.468\\nsamples = 59\\nvalue = 0.847'),\n",
       " Text(2.571261238914106, 93.54976744186047, 'mse = 0.0\\nsamples = 26\\nvalue = 0.0'),\n",
       " Text(2.640288386267505, 93.54976744186047, 'X[9] <= 0.113\\nmse = 35.583\\nsamples = 33\\nvalue = 1.515'),\n",
       " Text(2.6057748125908056, 88.49302325581394, 'X[5] <= 0.5\\nmse = 40.131\\nsamples = 29\\nvalue = 1.724'),\n",
       " Text(2.571261238914106, 83.43627906976744, 'mse = 41.454\\nsamples = 28\\nvalue = 1.786'),\n",
       " Text(2.640288386267505, 83.43627906976744, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(2.674801959944204, 88.49302325581394, 'mse = 0.0\\nsamples = 4\\nvalue = 0.0'),\n",
       " Text(2.571261238914106, 108.72, 'mse = 0.0\\nsamples = 19\\nvalue = 0.0'),\n",
       " Text(2.4245785507881337, 123.89023255813953, 'mse = 0.0\\nsamples = 47\\nvalue = 0.0'),\n",
       " Text(3.076022253935835, 128.94697674418603, 'X[10] <= 0.009\\nmse = 295.746\\nsamples = 257\\nvalue = 3.58'),\n",
       " Text(3.0415086802591356, 123.89023255813953, 'X[11] <= 0.5\\nmse = 318.251\\nsamples = 238\\nvalue = 3.866'),\n",
       " Text(3.006995106582436, 118.83348837209302, 'X[12] <= 5.5\\nmse = 337.149\\nsamples = 224\\nvalue = 4.107'),\n",
       " Text(2.8473698283277016, 113.77674418604651, 'X[9] <= 0.148\\nmse = 38.889\\nsamples = 30\\nvalue = 1.667'),\n",
       " Text(2.812856254651002, 108.72, 'X[6] <= 0.5\\nmse = 46.0\\nsamples = 25\\nvalue = 2.0'),\n",
       " Text(2.7783426809743026, 103.66325581395348, 'X[5] <= 0.5\\nmse = 53.855\\nsamples = 21\\nvalue = 2.381'),\n",
       " Text(2.743829107297603, 98.60651162790697, 'X[9] <= 0.129\\nmse = 56.25\\nsamples = 20\\nvalue = 2.5'),\n",
       " Text(2.7093155336209036, 93.54976744186047, 'mse = 61.728\\nsamples = 9\\nvalue = 2.778'),\n",
       " Text(2.7783426809743026, 93.54976744186047, 'X[9] <= 0.138\\nmse = 51.653\\nsamples = 11\\nvalue = 2.273'),\n",
       " Text(2.743829107297603, 88.49302325581394, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(2.812856254651002, 88.49302325581394, 'mse = 61.728\\nsamples = 9\\nvalue = 2.778'),\n",
       " Text(2.812856254651002, 98.60651162790697, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(2.8473698283277016, 103.66325581395348, 'mse = 0.0\\nsamples = 4\\nvalue = 0.0'),\n",
       " Text(2.8818834020044006, 108.72, 'mse = 0.0\\nsamples = 5\\nvalue = 0.0'),\n",
       " Text(3.166620384837171, 113.77674418604651, 'X[8] <= 0.5\\nmse = 382.209\\nsamples = 194\\nvalue = 4.485'),\n",
       " Text(3.1321068111604715, 108.72, 'X[2] <= 0.5\\nmse = 389.823\\nsamples = 190\\nvalue = 4.579'),\n",
       " Text(3.097593237483772, 103.66325581395348, 'X[5] <= 0.5\\nmse = 393.744\\nsamples = 188\\nvalue = 4.628'),\n",
       " Text(3.0630796638070725, 98.60651162790697, 'X[0] <= 0.5\\nmse = 397.745\\nsamples = 186\\nvalue = 4.677'),\n",
       " Text(3.0285660901303735, 93.54976744186047, 'X[12] <= 6.5\\nmse = 401.828\\nsamples = 184\\nvalue = 4.728'),\n",
       " Text(2.8818834020044006, 88.49302325581394, 'X[6] <= 0.5\\nmse = 246.165\\nsamples = 59\\nvalue = 4.068'),\n",
       " Text(2.7955994678126523, 83.43627906976744, 'X[9] <= 0.138\\nmse = 154.337\\nsamples = 56\\nvalue = 2.857'),\n",
       " Text(2.7265723204592534, 78.37953488372094, 'X[9] <= 0.121\\nmse = 38.889\\nsamples = 30\\nvalue = 1.667'),\n",
       " Text(2.692058746782554, 73.3227906976744, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(2.761085894135953, 73.3227906976744, 'X[9] <= 0.128\\nmse = 41.454\\nsamples = 28\\nvalue = 1.786'),\n",
       " Text(2.7265723204592534, 68.2660465116279, 'mse = 44.379\\nsamples = 26\\nvalue = 1.923'),\n",
       " Text(2.7955994678126523, 68.2660465116279, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(2.864626615166051, 78.37953488372094, 'X[9] <= 0.146\\nmse = 284.024\\nsamples = 26\\nvalue = 4.231'),\n",
       " Text(2.830113041489352, 73.3227906976744, 'mse = 362.25\\nsamples = 20\\nvalue = 5.5'),\n",
       " Text(2.8991401888427504, 73.3227906976744, 'mse = 0.0\\nsamples = 6\\nvalue = 0.0'),\n",
       " Text(2.9681673361961494, 83.43627906976744, 'X[9] <= 0.153\\nmse = 1422.222\\nsamples = 3\\nvalue = 26.667'),\n",
       " Text(2.93365376251945, 78.37953488372094, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(3.002680909872849, 78.37953488372094, 'mse = 0.0\\nsamples = 1\\nvalue = 80.0'),\n",
       " Text(3.175248778256346, 88.49302325581394, 'X[6] <= 0.5\\nmse = 474.998\\nsamples = 125\\nvalue = 5.04'),\n",
       " Text(3.106221630902947, 83.43627906976744, 'X[9] <= 0.146\\nmse = 561.592\\nsamples = 104\\nvalue = 5.817'),\n",
       " Text(3.0717080572262474, 78.37953488372094, 'X[9] <= 0.127\\nmse = 629.853\\nsamples = 92\\nvalue = 6.576'),\n",
       " Text(3.002680909872849, 73.3227906976744, 'X[9] <= 0.118\\nmse = 796.351\\nsamples = 39\\nvalue = 8.462'),\n",
       " Text(2.9681673361961494, 68.2660465116279, 'mse = 790.234\\nsamples = 8\\nvalue = 10.625'),\n",
       " Text(3.0371944835495484, 68.2660465116279, 'X[9] <= 0.122\\nmse = 796.41\\nsamples = 31\\nvalue = 7.903'),\n",
       " Text(3.002680909872849, 63.209302325581405, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(3.0717080572262474, 63.209302325581405, 'mse = 846.73\\nsamples = 29\\nvalue = 8.448'),\n",
       " Text(3.1407352045796464, 73.3227906976744, 'X[9] <= 0.143\\nmse = 502.795\\nsamples = 53\\nvalue = 5.189'),\n",
       " Text(3.106221630902947, 68.2660465116279, 'mse = 0.0\\nsamples = 5\\nvalue = 0.0'),\n",
       " Text(3.175248778256346, 68.2660465116279, 'mse = 552.072\\nsamples = 48\\nvalue = 5.729'),\n",
       " Text(3.1407352045796464, 78.37953488372094, 'mse = 0.0\\nsamples = 12\\nvalue = 0.0'),\n",
       " Text(3.2442759256097444, 83.43627906976744, 'X[9] <= 0.152\\nmse = 28.345\\nsamples = 21\\nvalue = 1.19'),\n",
       " Text(3.2097623519330454, 78.37953488372094, 'mse = 0.0\\nsamples = 16\\nvalue = 0.0'),\n",
       " Text(3.278789499286444, 78.37953488372094, 'mse = 100.0\\nsamples = 5\\nvalue = 5.0'),\n",
       " Text(3.097593237483772, 93.54976744186047, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(3.1321068111604715, 98.60651162790697, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(3.166620384837171, 103.66325581395348, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(3.2011339585138705, 108.72, 'mse = 0.0\\nsamples = 4\\nvalue = 0.0'),\n",
       " Text(3.076022253935835, 118.83348837209302, 'mse = 0.0\\nsamples = 14\\nvalue = 0.0'),\n",
       " Text(3.1105358276125346, 123.89023255813953, 'mse = 0.0\\nsamples = 19\\nvalue = 0.0'),\n",
       " Text(5.912943637107504, 134.00372093023256, 'X[6] <= 0.5\\nmse = 83.573\\nsamples = 1766\\nvalue = 1.093'),\n",
       " Text(5.4408761440256175, 128.94697674418603, 'X[9] <= 0.809\\nmse = 67.205\\nsamples = 1567\\nvalue = 0.967'),\n",
       " Text(5.406362570348918, 123.89023255813953, 'X[9] <= 0.789\\nmse = 71.622\\nsamples = 1469\\nvalue = 1.031'),\n",
       " Text(4.790106534113789, 118.83348837209302, 'X[12] <= 6.5\\nmse = 67.729\\nsamples = 1449\\nvalue = 0.987'),\n",
       " Text(4.017595685803291, 113.77674418604651, 'X[9] <= 0.46\\nmse = 82.249\\nsamples = 690\\nvalue = 1.203'),\n",
       " Text(3.617453940989057, 108.72, 'X[9] <= 0.17\\nmse = 67.685\\nsamples = 505\\nvalue = 0.911'),\n",
       " Text(3.3133030729631434, 103.66325581395348, 'X[12] <= 5.5\\nmse = 326.573\\nsamples = 59\\nvalue = 2.373'),\n",
       " Text(3.278789499286444, 98.60651162790697, 'mse = 0.0\\nsamples = 22\\nvalue = 0.0'),\n",
       " Text(3.347816646639843, 98.60651162790697, 'X[9] <= 0.162\\nmse = 515.413\\nsamples = 37\\nvalue = 3.784'),\n",
       " Text(3.3133030729631434, 93.54976744186047, 'mse = 0.0\\nsamples = 4\\nvalue = 0.0'),\n",
       " Text(3.3823302203165424, 93.54976744186047, 'X[11] <= 0.5\\nmse = 575.941\\nsamples = 33\\nvalue = 4.242'),\n",
       " Text(3.347816646639843, 88.49302325581394, 'X[10] <= 0.028\\nmse = 631.556\\nsamples = 30\\nvalue = 4.667'),\n",
       " Text(3.3133030729631434, 83.43627906976744, 'mse = 652.556\\nsamples = 29\\nvalue = 4.828'),\n",
       " Text(3.3823302203165424, 83.43627906976744, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(3.4168437939932415, 88.49302325581394, 'mse = 0.0\\nsamples = 3\\nvalue = 0.0'),\n",
       " Text(3.9216048090149704, 103.66325581395348, 'X[9] <= 0.405\\nmse = 33.117\\nsamples = 446\\nvalue = 0.717'),\n",
       " Text(3.887091235338271, 98.60651162790697, 'X[9] <= 0.387\\nmse = 37.318\\nsamples = 395\\nvalue = 0.81'),\n",
       " Text(3.701580776826012, 93.54976744186047, 'X[9] <= 0.275\\nmse = 35.223\\nsamples = 368\\nvalue = 0.666'),\n",
       " Text(3.5376413018616897, 88.49302325581394, 'X[9] <= 0.212\\nmse = 54.41\\nsamples = 215\\nvalue = 0.907'),\n",
       " Text(3.451357367669941, 83.43627906976744, 'X[9] <= 0.184\\nmse = 6.72\\nsamples = 92\\nvalue = 0.272'),\n",
       " Text(3.4168437939932415, 78.37953488372094, 'X[12] <= 5.5\\nmse = 28.345\\nsamples = 21\\nvalue = 1.19'),\n",
       " Text(3.3823302203165424, 73.3227906976744, 'mse = 0.0\\nsamples = 6\\nvalue = 0.0'),\n",
       " Text(3.451357367669941, 73.3227906976744, 'X[9] <= 0.179\\nmse = 38.889\\nsamples = 15\\nvalue = 1.667'),\n",
       " Text(3.4168437939932415, 68.2660465116279, 'mse = 0.0\\nsamples = 3\\nvalue = 0.0'),\n",
       " Text(3.4858709413466404, 68.2660465116279, 'X[5] <= 0.5\\nmse = 47.743\\nsamples = 12\\nvalue = 2.083'),\n",
       " Text(3.451357367669941, 63.209302325581405, 'mse = 51.653\\nsamples = 11\\nvalue = 2.273'),\n",
       " Text(3.52038451502334, 63.209302325581405, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(3.4858709413466404, 78.37953488372094, 'mse = 0.0\\nsamples = 71\\nvalue = 0.0'),\n",
       " Text(3.623925236053438, 83.43627906976744, 'X[9] <= 0.216\\nmse = 89.553\\nsamples = 123\\nvalue = 1.382'),\n",
       " Text(3.5548980887000394, 78.37953488372094, 'X[12] <= 5.5\\nmse = 586.806\\nsamples = 6\\nvalue = 10.833'),\n",
       " Text(3.52038451502334, 73.3227906976744, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(3.5894116623767385, 73.3227906976744, 'X[11] <= 0.5\\nmse = 792.188\\nsamples = 4\\nvalue = 16.25'),\n",
       " Text(3.5548980887000394, 68.2660465116279, 'mse = 938.889\\nsamples = 3\\nvalue = 21.667'),\n",
       " Text(3.623925236053438, 68.2660465116279, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(3.692952383406837, 78.37953488372094, 'X[9] <= 0.245\\nmse = 59.237\\nsamples = 117\\nvalue = 0.897'),\n",
       " Text(3.6584388097301375, 73.3227906976744, 'mse = 0.0\\nsamples = 38\\nvalue = 0.0'),\n",
       " Text(3.7274659570835365, 73.3227906976744, 'X[12] <= 5.5\\nmse = 87.158\\nsamples = 79\\nvalue = 1.329'),\n",
       " Text(3.692952383406837, 68.2660465116279, 'mse = 0.0\\nsamples = 23\\nvalue = 0.0'),\n",
       " Text(3.761979530760236, 68.2660465116279, 'X[11] <= 0.5\\nmse = 121.931\\nsamples = 56\\nvalue = 1.875'),\n",
       " Text(3.7274659570835365, 63.209302325581405, 'X[10] <= 0.021\\nmse = 147.507\\nsamples = 46\\nvalue = 2.283'),\n",
       " Text(3.692952383406837, 58.152558139534875, 'X[5] <= 0.5\\nmse = 157.409\\nsamples = 43\\nvalue = 2.442'),\n",
       " Text(3.6584388097301375, 53.095813953488374, 'X[8] <= 0.5\\nmse = 161.012\\nsamples = 42\\nvalue = 2.5'),\n",
       " Text(3.623925236053438, 48.039069767441845, 'X[9] <= 0.258\\nmse = 164.783\\nsamples = 41\\nvalue = 2.561'),\n",
       " Text(3.5894116623767385, 42.982325581395344, 'mse = 206.222\\nsamples = 30\\nvalue = 2.667'),\n",
       " Text(3.6584388097301375, 42.982325581395344, 'X[9] <= 0.27\\nmse = 51.653\\nsamples = 11\\nvalue = 2.273'),\n",
       " Text(3.623925236053438, 37.92558139534884, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(3.692952383406837, 37.92558139534884, 'mse = 56.25\\nsamples = 10\\nvalue = 2.5'),\n",
       " Text(3.692952383406837, 48.039069767441845, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(3.7274659570835365, 53.095813953488374, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(3.761979530760236, 58.152558139534875, 'mse = 0.0\\nsamples = 3\\nvalue = 0.0'),\n",
       " Text(3.796493104436935, 63.209302325581405, 'mse = 0.0\\nsamples = 10\\nvalue = 0.0'),\n",
       " Text(3.865520251790334, 88.49302325581394, 'X[9] <= 0.307\\nmse = 8.063\\nsamples = 153\\nvalue = 0.327'),\n",
       " Text(3.8310066781136345, 83.43627906976744, 'mse = 0.0\\nsamples = 58\\nvalue = 0.0'),\n",
       " Text(3.9000338254670335, 83.43627906976744, 'X[9] <= 0.309\\nmse = 12.881\\nsamples = 95\\nvalue = 0.526'),\n",
       " Text(3.8310066781136345, 78.37953488372094, 'X[12] <= 5.5\\nmse = 100.0\\nsamples = 5\\nvalue = 5.0'),\n",
       " Text(3.796493104436935, 73.3227906976744, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(3.865520251790334, 73.3227906976744, 'mse = 0.0\\nsamples = 4\\nvalue = 0.0'),\n",
       " Text(3.969060972820432, 78.37953488372094, 'X[9] <= 0.343\\nmse = 6.867\\nsamples = 90\\nvalue = 0.278'),\n",
       " Text(3.934547399143733, 73.3227906976744, 'X[12] <= 5.5\\nmse = 11.157\\nsamples = 55\\nvalue = 0.455'),\n",
       " Text(3.9000338254670335, 68.2660465116279, 'mse = 0.0\\nsamples = 16\\nvalue = 0.0'),\n",
       " Text(3.969060972820432, 68.2660465116279, 'X[11] <= 0.5\\nmse = 15.615\\nsamples = 39\\nvalue = 0.641'),\n",
       " Text(3.934547399143733, 63.209302325581405, 'X[8] <= 0.5\\nmse = 17.347\\nsamples = 35\\nvalue = 0.714'),\n",
       " Text(3.9000338254670335, 58.152558139534875, 'X[9] <= 0.325\\nmse = 17.842\\nsamples = 34\\nvalue = 0.735'),\n",
       " Text(3.865520251790334, 53.095813953488374, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(3.934547399143733, 53.095813953488374, 'mse = 18.365\\nsamples = 33\\nvalue = 0.758'),\n",
       " Text(3.969060972820432, 58.152558139534875, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(4.0035745464971315, 63.209302325581405, 'mse = 0.0\\nsamples = 4\\nvalue = 0.0'),\n",
       " Text(4.0035745464971315, 73.3227906976744, 'mse = 0.0\\nsamples = 35\\nvalue = 0.0'),\n",
       " Text(4.0726016938505305, 93.54976744186047, 'X[9] <= 0.394\\nmse = 61.728\\nsamples = 27\\nvalue = 2.778'),\n",
       " Text(4.038088120173831, 88.49302325581394, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(4.10711526752723, 88.49302325581394, 'X[11] <= 0.5\\nmse = 44.379\\nsamples = 26\\nvalue = 1.923'),\n",
       " Text(4.0726016938505305, 83.43627906976744, 'X[12] <= 5.5\\nmse = 47.743\\nsamples = 24\\nvalue = 2.083'),\n",
       " Text(4.038088120173831, 78.37953488372094, 'mse = 68.359\\nsamples = 8\\nvalue = 3.125'),\n",
       " Text(4.10711526752723, 78.37953488372094, 'X[5] <= 1.0\\nmse = 36.621\\nsamples = 16\\nvalue = 1.562'),\n",
       " Text(4.0726016938505305, 73.3227906976744, 'mse = 38.889\\nsamples = 15\\nvalue = 1.667'),\n",
       " Text(4.1416288412039295, 73.3227906976744, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(4.1416288412039295, 83.43627906976744, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(3.95611838269167, 98.60651162790697, 'mse = 0.0\\nsamples = 51\\nvalue = 0.0'),\n",
       " Text(4.4177374306175246, 108.72, 'X[9] <= 0.464\\nmse = 121.135\\nsamples = 185\\nvalue = 2.0'),\n",
       " Text(4.331453496425776, 103.66325581395348, 'X[10] <= 0.019\\nmse = 1067.188\\nsamples = 4\\nvalue = 26.25'),\n",
       " Text(4.296939922749076, 98.60651162790697, 'X[12] <= 5.5\\nmse = 1116.667\\nsamples = 3\\nvalue = 35.0'),\n",
       " Text(4.262426349072377, 93.54976744186047, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(4.331453496425776, 93.54976744186047, 'mse = 1600.0\\nsamples = 2\\nvalue = 40.0'),\n",
       " Text(4.365967070102475, 98.60651162790697, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(4.504021364809273, 103.66325581395348, 'X[7] <= 0.5\\nmse = 86.945\\nsamples = 181\\nvalue = 1.464'),\n",
       " Text(4.434994217455874, 98.60651162790697, 'X[11] <= 0.5\\nmse = 84.794\\nsamples = 179\\nvalue = 1.341'),\n",
       " Text(4.400480643779175, 93.54976744186047, 'X[9] <= 0.575\\nmse = 121.254\\nsamples = 124\\nvalue = 1.935'),\n",
       " Text(4.279683135910727, 88.49302325581394, 'X[9] <= 0.567\\nmse = 230.656\\nsamples = 57\\nvalue = 2.895'),\n",
       " Text(4.2106559885573285, 83.43627906976744, 'X[12] <= 5.5\\nmse = 144.44\\nsamples = 49\\nvalue = 1.735'),\n",
       " Text(4.176142414880629, 78.37953488372094, 'mse = 0.0\\nsamples = 15\\nvalue = 0.0'),\n",
       " Text(4.245169562234027, 78.37953488372094, 'X[9] <= 0.513\\nmse = 206.25\\nsamples = 34\\nvalue = 2.5'),\n",
       " Text(4.2106559885573285, 73.3227906976744, 'X[9] <= 0.492\\nmse = 300.473\\nsamples = 23\\nvalue = 3.696'),\n",
       " Text(4.176142414880629, 68.2660465116279, 'mse = 0.0\\nsamples = 4\\nvalue = 0.0'),\n",
       " Text(4.245169562234027, 68.2660465116279, 'mse = 360.249\\nsamples = 19\\nvalue = 4.474'),\n",
       " Text(4.279683135910727, 73.3227906976744, 'mse = 0.0\\nsamples = 11\\nvalue = 0.0'),\n",
       " Text(4.348710283264126, 83.43627906976744, 'X[12] <= 5.5\\nmse = 700.0\\nsamples = 8\\nvalue = 10.0'),\n",
       " Text(4.314196709587426, 78.37953488372094, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(4.383223856940825, 78.37953488372094, 'mse = 888.889\\nsamples = 6\\nvalue = 13.333'),\n",
       " Text(4.521278151647623, 88.49302325581394, 'X[12] <= 5.5\\nmse = 26.732\\nsamples = 67\\nvalue = 1.119'),\n",
       " Text(4.4867645779709235, 83.43627906976744, 'X[9] <= 0.757\\nmse = 47.743\\nsamples = 36\\nvalue = 2.083'),\n",
       " Text(4.452251004294224, 78.37953488372094, 'X[9] <= 0.739\\nmse = 57.967\\nsamples = 29\\nvalue = 2.586'),\n",
       " Text(4.4177374306175246, 73.3227906976744, 'X[9] <= 0.679\\nmse = 42.867\\nsamples = 27\\nvalue = 1.852'),\n",
       " Text(4.383223856940825, 68.2660465116279, 'X[9] <= 0.613\\nmse = 58.864\\nsamples = 19\\nvalue = 2.632'),\n",
       " Text(4.348710283264126, 63.209302325581405, 'mse = 0.0\\nsamples = 4\\nvalue = 0.0'),\n",
       " Text(4.4177374306175246, 63.209302325581405, 'X[9] <= 0.646\\nmse = 72.222\\nsamples = 15\\nvalue = 3.333'),\n",
       " Text(4.383223856940825, 58.152558139534875, 'mse = 100.0\\nsamples = 5\\nvalue = 5.0'),\n",
       " Text(4.452251004294224, 58.152558139534875, 'mse = 56.25\\nsamples = 10\\nvalue = 2.5'),\n",
       " Text(4.452251004294224, 68.2660465116279, 'mse = 0.0\\nsamples = 8\\nvalue = 0.0'),\n",
       " Text(4.4867645779709235, 73.3227906976744, 'mse = 156.25\\nsamples = 2\\nvalue = 12.5'),\n",
       " Text(4.521278151647623, 78.37953488372094, 'mse = 0.0\\nsamples = 7\\nvalue = 0.0'),\n",
       " Text(4.5557917253243225, 83.43627906976744, 'mse = 0.0\\nsamples = 31\\nvalue = 0.0'),\n",
       " Text(4.469507791132574, 93.54976744186047, 'mse = 0.0\\nsamples = 55\\nvalue = 0.0'),\n",
       " Text(4.573048512162672, 98.60651162790697, 'X[9] <= 0.607\\nmse = 156.25\\nsamples = 2\\nvalue = 12.5'),\n",
       " Text(4.538534938485973, 93.54976744186047, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(4.607562085839371, 93.54976744186047, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(5.562617382424288, 113.77674418604651, 'X[9] <= 0.314\\nmse = 54.448\\nsamples = 759\\nvalue = 0.791'),\n",
       " Text(5.275184026648025, 108.72, 'X[9] <= 0.297\\nmse = 94.634\\nsamples = 406\\nvalue = 1.219'),\n",
       " Text(5.062709838700845, 103.66325581395348, 'X[9] <= 0.282\\nmse = 29.736\\nsamples = 384\\nvalue = 0.638'),\n",
       " Text(4.8966132653817285, 98.60651162790697, 'X[10] <= 0.049\\nmse = 12.323\\nsamples = 347\\nvalue = 0.461'),\n",
       " Text(4.8060151344803925, 93.54976744186047, 'X[9] <= 0.205\\nmse = 10.925\\nsamples = 336\\nvalue = 0.402'),\n",
       " Text(4.771501560803694, 88.49302325581394, 'X[9] <= 0.179\\nmse = 19.706\\nsamples = 184\\nvalue = 0.734'),\n",
       " Text(4.624818872677721, 83.43627906976744, 'X[9] <= 0.169\\nmse = 7.101\\nsamples = 87\\nvalue = 0.287'),\n",
       " Text(4.590305299001022, 78.37953488372094, 'X[9] <= 0.165\\nmse = 8.332\\nsamples = 74\\nvalue = 0.338'),\n",
       " Text(4.5557917253243225, 73.3227906976744, 'mse = 0.0\\nsamples = 12\\nvalue = 0.0'),\n",
       " Text(4.624818872677721, 73.3227906976744, 'X[11] <= 0.5\\nmse = 9.918\\nsamples = 62\\nvalue = 0.403'),\n",
       " Text(4.590305299001022, 68.2660465116279, 'X[5] <= 0.5\\nmse = 10.414\\nsamples = 59\\nvalue = 0.424'),\n",
       " Text(4.5557917253243225, 63.209302325581405, 'X[10] <= 0.021\\nmse = 10.773\\nsamples = 57\\nvalue = 0.439'),\n",
       " Text(4.521278151647623, 58.152558139534875, 'X[8] <= 0.5\\nmse = 10.961\\nsamples = 56\\nvalue = 0.446'),\n",
       " Text(4.4867645779709235, 53.095813953488374, 'mse = 11.157\\nsamples = 55\\nvalue = 0.455'),\n",
       " Text(4.5557917253243225, 53.095813953488374, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(4.590305299001022, 58.152558139534875, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(4.624818872677721, 63.209302325581405, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(4.65933244635442, 68.2660465116279, 'mse = 0.0\\nsamples = 3\\nvalue = 0.0'),\n",
       " Text(4.65933244635442, 78.37953488372094, 'mse = 0.0\\nsamples = 13\\nvalue = 0.0'),\n",
       " Text(4.918184248929666, 83.43627906976744, 'X[9] <= 0.189\\nmse = 30.673\\nsamples = 97\\nvalue = 1.134'),\n",
       " Text(4.831900314737918, 78.37953488372094, 'X[9] <= 0.185\\nmse = 51.298\\nsamples = 34\\nvalue = 1.765'),\n",
       " Text(4.762873167384519, 73.3227906976744, 'X[10] <= 0.023\\nmse = 22.291\\nsamples = 27\\nvalue = 0.926'),\n",
       " Text(4.728359593707819, 68.2660465116279, 'X[11] <= 1.5\\nmse = 24.957\\nsamples = 24\\nvalue = 1.042'),\n",
       " Text(4.69384602003112, 63.209302325581405, 'X[5] <= 0.5\\nmse = 25.992\\nsamples = 23\\nvalue = 1.087'),\n",
       " Text(4.65933244635442, 58.152558139534875, 'mse = 27.118\\nsamples = 22\\nvalue = 1.136'),\n",
       " Text(4.728359593707819, 58.152558139534875, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(4.762873167384519, 63.209302325581405, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(4.797386741061218, 68.2660465116279, 'mse = 0.0\\nsamples = 3\\nvalue = 0.0'),\n",
       " Text(4.900927462091317, 73.3227906976744, 'X[5] <= 0.5\\nmse = 150.0\\nsamples = 7\\nvalue = 5.0'),\n",
       " Text(4.866413888414617, 68.2660465116279, 'mse = 170.139\\nsamples = 6\\nvalue = 5.833'),\n",
       " Text(4.935441035768016, 68.2660465116279, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(5.004468183121414, 78.37953488372094, 'X[9] <= 0.197\\nmse = 19.211\\nsamples = 63\\nvalue = 0.794'),\n",
       " Text(4.969954609444715, 73.3227906976744, 'mse = 0.0\\nsamples = 6\\nvalue = 0.0'),\n",
       " Text(5.038981756798114, 73.3227906976744, 'X[11] <= 0.5\\nmse = 21.16\\nsamples = 57\\nvalue = 0.877'),\n",
       " Text(5.004468183121414, 68.2660465116279, 'X[5] <= 0.5\\nmse = 23.114\\nsamples = 52\\nvalue = 0.962'),\n",
       " Text(4.969954609444715, 63.209302325581405, 'X[7] <= 0.5\\nmse = 24.957\\nsamples = 48\\nvalue = 1.042'),\n",
       " Text(4.935441035768016, 58.152558139534875, 'X[8] <= 1.0\\nmse = 25.992\\nsamples = 46\\nvalue = 1.087'),\n",
       " Text(4.900927462091317, 53.095813953488374, 'mse = 26.543\\nsamples = 45\\nvalue = 1.111'),\n",
       " Text(4.969954609444715, 53.095813953488374, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(5.004468183121414, 58.152558139534875, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(5.038981756798114, 63.209302325581405, 'mse = 0.0\\nsamples = 4\\nvalue = 0.0'),\n",
       " Text(5.073495330474813, 68.2660465116279, 'mse = 0.0\\nsamples = 5\\nvalue = 0.0'),\n",
       " Text(4.840528708157092, 88.49302325581394, 'mse = 0.0\\nsamples = 152\\nvalue = 0.0'),\n",
       " Text(4.987211396283064, 93.54976744186047, 'X[9] <= 0.236\\nmse = 51.653\\nsamples = 11\\nvalue = 2.273'),\n",
       " Text(4.952697822606366, 88.49302325581394, 'mse = 0.0\\nsamples = 8\\nvalue = 0.0'),\n",
       " Text(5.021724969959764, 88.49302325581394, 'X[10] <= 0.051\\nmse = 138.889\\nsamples = 3\\nvalue = 8.333'),\n",
       " Text(4.987211396283064, 83.43627906976744, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(5.056238543636463, 83.43627906976744, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(5.228806412019961, 98.60651162790697, 'X[9] <= 0.29\\nmse = 189.993\\nsamples = 37\\nvalue = 2.297'),\n",
       " Text(5.194292838343261, 93.54976744186047, 'X[5] <= 0.5\\nmse = 206.25\\nsamples = 34\\nvalue = 2.5'),\n",
       " Text(5.159779264666562, 88.49302325581394, 'X[11] <= 0.5\\nmse = 212.305\\nsamples = 33\\nvalue = 2.576'),\n",
       " Text(5.125265690989862, 83.43627906976744, 'mse = 218.726\\nsamples = 32\\nvalue = 2.656'),\n",
       " Text(5.194292838343261, 83.43627906976744, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(5.228806412019961, 88.49302325581394, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(5.26331998569666, 93.54976744186047, 'mse = 0.0\\nsamples = 3\\nvalue = 0.0'),\n",
       " Text(5.487658214595206, 103.66325581395348, 'X[9] <= 0.309\\nmse = 1118.595\\nsamples = 22\\nvalue = 11.364'),\n",
       " Text(5.4013742804034575, 98.60651162790697, 'X[9] <= 0.306\\nmse = 1763.905\\nsamples = 13\\nvalue = 17.308'),\n",
       " Text(5.332347133050059, 93.54976744186047, 'X[10] <= 0.022\\nmse = 1619.835\\nsamples = 11\\nvalue = 12.727'),\n",
       " Text(5.29783355937336, 88.49302325581394, 'mse = 1935.802\\nsamples = 9\\nvalue = 15.556'),\n",
       " Text(5.366860706726758, 88.49302325581394, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(5.4704014277568564, 93.54976744186047, 'X[11] <= 1.5\\nmse = 1806.25\\nsamples = 2\\nvalue = 42.5'),\n",
       " Text(5.435887854080157, 88.49302325581394, 'mse = 0.0\\nsamples = 1\\nvalue = 85.0'),\n",
       " Text(5.504915001433556, 88.49302325581394, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(5.573942148786955, 98.60651162790697, 'X[11] <= 1.0\\nmse = 61.728\\nsamples = 9\\nvalue = 2.778'),\n",
       " Text(5.539428575110255, 93.54976744186047, 'mse = 76.531\\nsamples = 7\\nvalue = 3.571'),\n",
       " Text(5.608455722463654, 93.54976744186047, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(5.85005073820055, 108.72, 'X[9] <= 0.742\\nmse = 7.773\\nsamples = 353\\nvalue = 0.297'),\n",
       " Text(5.781023590847151, 103.66325581395348, 'X[9] <= 0.41\\nmse = 6.196\\nsamples = 344\\nvalue = 0.233'),\n",
       " Text(5.7465100171704515, 98.60651162790697, 'X[9] <= 0.408\\nmse = 13.779\\nsamples = 153\\nvalue = 0.523'),\n",
       " Text(5.677482869817053, 93.54976744186047, 'X[9] <= 0.373\\nmse = 10.166\\nsamples = 148\\nvalue = 0.372'),\n",
       " Text(5.642969296140354, 88.49302325581394, 'mse = 0.0\\nsamples = 97\\nvalue = 0.0'),\n",
       " Text(5.711996443493752, 88.49302325581394, 'X[9] <= 0.386\\nmse = 28.739\\nsamples = 51\\nvalue = 1.078'),\n",
       " Text(5.677482869817053, 83.43627906976744, 'X[9] <= 0.384\\nmse = 65.76\\nsamples = 21\\nvalue = 2.619'),\n",
       " Text(5.642969296140354, 78.37953488372094, 'X[9] <= 0.378\\nmse = 47.222\\nsamples = 18\\nvalue = 1.667'),\n",
       " Text(5.608455722463654, 73.3227906976744, 'X[11] <= 0.5\\nmse = 56.0\\nsamples = 15\\nvalue = 2.0'),\n",
       " Text(5.573942148786955, 68.2660465116279, 'X[10] <= 0.021\\nmse = 63.905\\nsamples = 13\\nvalue = 2.308'),\n",
       " Text(5.539428575110255, 63.209302325581405, 'X[5] <= 0.5\\nmse = 68.75\\nsamples = 12\\nvalue = 2.5'),\n",
       " Text(5.504915001433556, 58.152558139534875, 'mse = 74.38\\nsamples = 11\\nvalue = 2.727'),\n",
       " Text(5.573942148786955, 58.152558139534875, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(5.608455722463654, 63.209302325581405, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(5.642969296140354, 68.2660465116279, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(5.677482869817053, 73.3227906976744, 'mse = 0.0\\nsamples = 3\\nvalue = 0.0'),\n",
       " Text(5.711996443493752, 78.37953488372094, 'mse = 138.889\\nsamples = 3\\nvalue = 8.333'),\n",
       " Text(5.7465100171704515, 83.43627906976744, 'mse = 0.0\\nsamples = 30\\nvalue = 0.0'),\n",
       " Text(5.8155371645238505, 93.54976744186047, 'X[10] <= 0.023\\nmse = 100.0\\nsamples = 5\\nvalue = 5.0'),\n",
       " Text(5.781023590847151, 88.49302325581394, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(5.85005073820055, 88.49302325581394, 'X[11] <= 1.0\\nmse = 138.889\\nsamples = 3\\nvalue = 8.333'),\n",
       " Text(5.8155371645238505, 83.43627906976744, 'mse = 156.25\\nsamples = 2\\nvalue = 12.5'),\n",
       " Text(5.8845643118772495, 83.43627906976744, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(5.8155371645238505, 98.60651162790697, 'mse = 0.0\\nsamples = 191\\nvalue = 0.0'),\n",
       " Text(5.919077885553949, 103.66325581395348, 'X[11] <= 1.0\\nmse = 61.728\\nsamples = 9\\nvalue = 2.778'),\n",
       " Text(5.8845643118772495, 98.60651162790697, 'mse = 0.0\\nsamples = 8\\nvalue = 0.0'),\n",
       " Text(5.9535914592306485, 98.60651162790697, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(6.0226186065840475, 118.83348837209302, 'X[12] <= 5.5\\nmse = 343.188\\nsamples = 20\\nvalue = 4.25'),\n",
       " Text(5.988105032907348, 113.77674418604651, 'mse = 0.0\\nsamples = 9\\nvalue = 0.0'),\n",
       " Text(6.057132180260747, 113.77674418604651, 'X[12] <= 6.5\\nmse = 597.107\\nsamples = 11\\nvalue = 7.727'),\n",
       " Text(6.0226186065840475, 108.72, 'X[11] <= 0.5\\nmse = 884.694\\nsamples = 7\\nvalue = 12.143'),\n",
       " Text(5.988105032907348, 103.66325581395348, 'mse = 1003.472\\nsamples = 6\\nvalue = 14.167'),\n",
       " Text(6.057132180260747, 103.66325581395348, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(6.091645753937446, 108.72, 'mse = 0.0\\nsamples = 4\\nvalue = 0.0'),\n",
       " Text(5.475389717702317, 123.89023255813953, 'mse = 0.0\\nsamples = 98\\nvalue = 0.0'),\n",
       " Text(6.385011130189391, 128.94697674418603, 'X[9] <= 0.764\\nmse = 211.355\\nsamples = 199\\nvalue = 2.085'),\n",
       " Text(6.350497556512692, 123.89023255813953, 'X[9] <= 0.532\\nmse = 115.874\\nsamples = 198\\nvalue = 1.389'),\n",
       " Text(6.264213622320943, 118.83348837209302, 'X[1] <= 0.019\\nmse = 19.298\\nsamples = 188\\nvalue = 0.718'),\n",
       " Text(6.195186474967544, 113.77674418604651, 'X[9] <= 0.265\\nmse = 13.58\\nsamples = 180\\nvalue = 0.556'),\n",
       " Text(6.1606729012908445, 108.72, 'mse = 0.0\\nsamples = 90\\nvalue = 0.0'),\n",
       " Text(6.2297000486442435, 108.72, 'X[9] <= 0.268\\nmse = 26.543\\nsamples = 90\\nvalue = 1.111'),\n",
       " Text(6.126159327614145, 103.66325581395348, 'X[12] <= 6.5\\nmse = 150.0\\nsamples = 5\\nvalue = 10.0'),\n",
       " Text(6.091645753937446, 98.60651162790697, 'mse = 0.0\\nsamples = 3\\nvalue = 0.0'),\n",
       " Text(6.1606729012908445, 98.60651162790697, 'mse = 0.0\\nsamples = 2\\nvalue = 25.0'),\n",
       " Text(6.333240769674342, 103.66325581395348, 'X[9] <= 0.487\\nmse = 14.36\\nsamples = 85\\nvalue = 0.588'),\n",
       " Text(6.2297000486442435, 98.60651162790697, 'X[9] <= 0.309\\nmse = 8.011\\nsamples = 77\\nvalue = 0.325'),\n",
       " Text(6.195186474967544, 93.54976744186047, 'X[9] <= 0.305\\nmse = 23.114\\nsamples = 26\\nvalue = 0.962'),\n",
       " Text(6.1606729012908445, 88.49302325581394, 'mse = 0.0\\nsamples = 24\\nvalue = 0.0'),\n",
       " Text(6.2297000486442435, 88.49302325581394, 'X[12] <= 6.5\\nmse = 156.25\\nsamples = 2\\nvalue = 12.5'),\n",
       " Text(6.195186474967544, 83.43627906976744, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(6.264213622320943, 83.43627906976744, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(6.264213622320943, 93.54976744186047, 'mse = 0.0\\nsamples = 51\\nvalue = 0.0'),\n",
       " Text(6.4367814907044405, 98.60651162790697, 'X[12] <= 6.5\\nmse = 68.359\\nsamples = 8\\nvalue = 3.125'),\n",
       " Text(6.402267917027741, 93.54976744186047, 'X[11] <= 0.5\\nmse = 117.188\\nsamples = 4\\nvalue = 6.25'),\n",
       " Text(6.3677543433510415, 88.49302325581394, 'X[12] <= 5.5\\nmse = 156.25\\nsamples = 2\\nvalue = 12.5'),\n",
       " Text(6.333240769674342, 83.43627906976744, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(6.402267917027741, 83.43627906976744, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(6.4367814907044405, 88.49302325581394, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(6.471295064381139, 93.54976744186047, 'mse = 0.0\\nsamples = 4\\nvalue = 0.0'),\n",
       " Text(6.333240769674342, 113.77674418604651, 'X[10] <= 0.032\\nmse = 133.984\\nsamples = 8\\nvalue = 4.375'),\n",
       " Text(6.2987271959976425, 108.72, 'mse = 0.0\\nsamples = 7\\nvalue = 0.0'),\n",
       " Text(6.3677543433510415, 108.72, 'mse = 0.0\\nsamples = 1\\nvalue = 35.0'),\n",
       " Text(6.4367814907044405, 118.83348837209302, 'X[9] <= 0.547\\nmse = 1764.0\\nsamples = 10\\nvalue = 14.0'),\n",
       " Text(6.402267917027741, 113.77674418604651, 'mse = 0.0\\nsamples = 1\\nvalue = 140.0'),\n",
       " Text(6.471295064381139, 113.77674418604651, 'mse = 0.0\\nsamples = 9\\nvalue = 0.0'),\n",
       " Text(6.419524703866091, 123.89023255813953, 'mse = 0.0\\nsamples = 1\\nvalue = 140.0'),\n",
       " Text(10.988242166988279, 139.06046511627906, 'X[11] <= 2.5\\nmse = 50.167\\nsamples = 3908\\nvalue = 0.75'),\n",
       " Text(9.586111384041468, 134.00372093023256, 'X[12] <= 8.5\\nmse = 48.92\\nsamples = 3789\\nvalue = 0.711'),\n",
       " Text(8.506247724834658, 128.94697674418603, 'X[6] <= 0.5\\nmse = 67.855\\nsamples = 1792\\nvalue = 0.924'),\n",
       " Text(7.8405132450864485, 123.89023255813953, 'X[10] <= 0.031\\nmse = 50.353\\nsamples = 1409\\nvalue = 0.731'),\n",
       " Text(7.337370053830815, 118.83348837209302, 'X[9] <= 0.135\\nmse = 38.315\\nsamples = 1282\\nvalue = 0.628'),\n",
       " Text(6.9631134892741064, 113.77674418604651, 'X[9] <= 0.123\\nmse = 66.04\\nsamples = 240\\nvalue = 1.208'),\n",
       " Text(6.764660440633085, 108.72, 'X[9] <= 0.107\\nmse = 13.959\\nsamples = 175\\nvalue = 0.571'),\n",
       " Text(6.643862932764637, 103.66325581395348, 'X[9] <= 0.064\\nmse = 9.84\\nsamples = 125\\nvalue = 0.4'),\n",
       " Text(6.574835785411238, 98.60651162790697, 'X[9] <= 0.061\\nmse = 31.163\\nsamples = 19\\nvalue = 1.316'),\n",
       " Text(6.540322211734538, 93.54976744186047, 'mse = 0.0\\nsamples = 11\\nvalue = 0.0'),\n",
       " Text(6.609349359087937, 93.54976744186047, 'mse = 68.359\\nsamples = 8\\nvalue = 3.125'),\n",
       " Text(6.712890080118036, 98.60651162790697, 'X[9] <= 0.089\\nmse = 5.841\\nsamples = 106\\nvalue = 0.236'),\n",
       " Text(6.678376506441336, 93.54976744186047, 'mse = 0.0\\nsamples = 48\\nvalue = 0.0'),\n",
       " Text(6.747403653794735, 93.54976744186047, 'X[9] <= 0.093\\nmse = 10.59\\nsamples = 58\\nvalue = 0.431'),\n",
       " Text(6.712890080118036, 88.49302325581394, 'X[5] <= 0.5\\nmse = 29.688\\nsamples = 20\\nvalue = 1.25'),\n",
       " Text(6.678376506441336, 83.43627906976744, 'X[8] <= 0.5\\nmse = 32.793\\nsamples = 18\\nvalue = 1.389'),\n",
       " Text(6.643862932764637, 78.37953488372094, 'X[11] <= 0.5\\nmse = 36.621\\nsamples = 16\\nvalue = 1.562'),\n",
       " Text(6.609349359087937, 73.3227906976744, 'mse = 38.889\\nsamples = 15\\nvalue = 1.667'),\n",
       " Text(6.678376506441336, 73.3227906976744, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(6.712890080118036, 78.37953488372094, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(6.747403653794735, 83.43627906976744, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(6.7819172274714346, 88.49302325581394, 'mse = 0.0\\nsamples = 38\\nvalue = 0.0'),\n",
       " Text(6.885457948501532, 103.66325581395348, 'X[9] <= 0.11\\nmse = 24.0\\nsamples = 50\\nvalue = 1.0'),\n",
       " Text(6.850944374824833, 98.60651162790697, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(6.919971522178232, 98.60651162790697, 'X[9] <= 0.113\\nmse = 12.495\\nsamples = 49\\nvalue = 0.51'),\n",
       " Text(6.885457948501532, 93.54976744186047, 'X[5] <= 0.5\\nmse = 20.809\\nsamples = 29\\nvalue = 0.862'),\n",
       " Text(6.850944374824833, 88.49302325581394, 'mse = 21.524\\nsamples = 28\\nvalue = 0.893'),\n",
       " Text(6.919971522178232, 88.49302325581394, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(6.954485095854931, 93.54976744186047, 'mse = 0.0\\nsamples = 20\\nvalue = 0.0'),\n",
       " Text(7.161566537915128, 108.72, 'X[8] <= 0.5\\nmse = 202.225\\nsamples = 65\\nvalue = 2.923'),\n",
       " Text(7.092539390561729, 103.66325581395348, 'X[9] <= 0.127\\nmse = 199.792\\nsamples = 62\\nvalue = 2.581'),\n",
       " Text(7.05802581688503, 98.60651162790697, 'X[11] <= 0.5\\nmse = 304.0\\nsamples = 40\\nvalue = 4.0'),\n",
       " Text(7.02351224320833, 93.54976744186047, 'X[5] <= 0.5\\nmse = 311.374\\nsamples = 39\\nvalue = 4.103'),\n",
       " Text(6.988998669531631, 88.49302325581394, 'mse = 319.114\\nsamples = 38\\nvalue = 4.211'),\n",
       " Text(7.05802581688503, 88.49302325581394, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(7.092539390561729, 93.54976744186047, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(7.127052964238429, 98.60651162790697, 'mse = 0.0\\nsamples = 22\\nvalue = 0.0'),\n",
       " Text(7.230593685268526, 103.66325581395348, 'X[9] <= 0.129\\nmse = 200.0\\nsamples = 3\\nvalue = 10.0'),\n",
       " Text(7.196080111591827, 98.60651162790697, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(7.265107258945226, 98.60651162790697, 'mse = 0.0\\nsamples = 1\\nvalue = 30.0'),\n",
       " Text(7.711626618387524, 113.77674418604651, 'X[9] <= 0.237\\nmse = 31.833\\nsamples = 1042\\nvalue = 0.494'),\n",
       " Text(7.403161553652024, 108.72, 'X[9] <= 0.168\\nmse = 3.655\\nsamples = 340\\nvalue = 0.147'),\n",
       " Text(7.368647979975324, 103.66325581395348, 'X[9] <= 0.153\\nmse = 8.277\\nsamples = 149\\nvalue = 0.336'),\n",
       " Text(7.334134406298625, 98.60651162790697, 'mse = 0.0\\nsamples = 55\\nvalue = 0.0'),\n",
       " Text(7.403161553652024, 98.60651162790697, 'X[9] <= 0.156\\nmse = 13.015\\nsamples = 94\\nvalue = 0.532'),\n",
       " Text(7.334134406298625, 93.54976744186047, 'X[5] <= 0.5\\nmse = 34.602\\nsamples = 17\\nvalue = 1.471'),\n",
       " Text(7.299620832621925, 88.49302325581394, 'X[7] <= 1.0\\nmse = 38.889\\nsamples = 15\\nvalue = 1.667'),\n",
       " Text(7.265107258945226, 83.43627906976744, 'mse = 41.454\\nsamples = 14\\nvalue = 1.786'),\n",
       " Text(7.334134406298625, 83.43627906976744, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(7.368647979975324, 88.49302325581394, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(7.472188701005423, 93.54976744186047, 'X[9] <= 0.164\\nmse = 8.011\\nsamples = 77\\nvalue = 0.325'),\n",
       " Text(7.437675127328723, 88.49302325581394, 'mse = 0.0\\nsamples = 6\\nvalue = 0.0'),\n",
       " Text(7.506702274682122, 88.49302325581394, 'X[11] <= 0.5\\nmse = 8.679\\nsamples = 71\\nvalue = 0.352'),\n",
       " Text(7.472188701005423, 83.43627906976744, 'X[5] <= 0.5\\nmse = 9.189\\nsamples = 67\\nvalue = 0.373'),\n",
       " Text(7.437675127328723, 78.37953488372094, 'X[7] <= 0.5\\nmse = 9.613\\nsamples = 64\\nvalue = 0.391'),\n",
       " Text(7.403161553652024, 73.3227906976744, 'mse = 10.078\\nsamples = 61\\nvalue = 0.41'),\n",
       " Text(7.472188701005423, 73.3227906976744, 'mse = 0.0\\nsamples = 3\\nvalue = 0.0'),\n",
       " Text(7.506702274682122, 78.37953488372094, 'mse = 0.0\\nsamples = 3\\nvalue = 0.0'),\n",
       " Text(7.541215848358822, 83.43627906976744, 'mse = 0.0\\nsamples = 4\\nvalue = 0.0'),\n",
       " Text(7.437675127328723, 103.66325581395348, 'mse = 0.0\\nsamples = 191\\nvalue = 0.0'),\n",
       " Text(8.020091683123026, 108.72, 'X[9] <= 0.239\\nmse = 45.395\\nsamples = 702\\nvalue = 0.662'),\n",
       " Text(7.985578109446326, 103.66325581395348, 'mse = 156.25\\nsamples = 2\\nvalue = 12.5'),\n",
       " Text(8.054605256799725, 103.66325581395348, 'X[9] <= 0.559\\nmse = 44.676\\nsamples = 700\\nvalue = 0.629'),\n",
       " Text(8.020091683123026, 98.60651162790697, 'X[9] <= 0.536\\nmse = 53.456\\nsamples = 584\\nvalue = 0.753'),\n",
       " Text(7.843209618029941, 93.54976744186047, 'X[9] <= 0.36\\nmse = 43.591\\nsamples = 558\\nvalue = 0.6'),\n",
       " Text(7.6965269299039685, 88.49302325581394, 'X[9] <= 0.358\\nmse = 73.765\\nsamples = 312\\nvalue = 0.913'),\n",
       " Text(7.662013356227269, 83.43627906976744, 'X[9] <= 0.253\\nmse = 28.255\\nsamples = 311\\nvalue = 0.531'),\n",
       " Text(7.57572942203552, 78.37953488372094, 'X[9] <= 0.247\\nmse = 95.5\\nsamples = 66\\nvalue = 1.212'),\n",
       " Text(7.541215848358822, 73.3227906976744, 'mse = 0.0\\nsamples = 6\\nvalue = 0.0'),\n",
       " Text(7.61024299571222, 73.3227906976744, 'X[5] <= 0.5\\nmse = 104.889\\nsamples = 60\\nvalue = 1.333'),\n",
       " Text(7.57572942203552, 68.2660465116279, 'X[8] <= 0.5\\nmse = 112.245\\nsamples = 56\\nvalue = 1.429'),\n",
       " Text(7.541215848358822, 63.209302325581405, 'X[11] <= 0.5\\nmse = 116.324\\nsamples = 54\\nvalue = 1.481'),\n",
       " Text(7.506702274682122, 58.152558139534875, 'mse = 118.476\\nsamples = 53\\nvalue = 1.509'),\n",
       " Text(7.57572942203552, 58.152558139534875, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(7.61024299571222, 63.209302325581405, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(7.644756569388919, 68.2660465116279, 'mse = 0.0\\nsamples = 4\\nvalue = 0.0'),\n",
       " Text(7.748297290419018, 78.37953488372094, 'X[9] <= 0.297\\nmse = 9.982\\nsamples = 245\\nvalue = 0.347'),\n",
       " Text(7.713783716742318, 73.3227906976744, 'mse = 0.0\\nsamples = 85\\nvalue = 0.0'),\n",
       " Text(7.782810864095717, 73.3227906976744, 'X[9] <= 0.315\\nmse = 15.187\\nsamples = 160\\nvalue = 0.531'),\n",
       " Text(7.713783716742318, 68.2660465116279, 'X[9] <= 0.313\\nmse = 51.298\\nsamples = 34\\nvalue = 1.765'),\n",
       " Text(7.679270143065619, 63.209302325581405, 'X[9] <= 0.301\\nmse = 18.365\\nsamples = 33\\nvalue = 0.758'),\n",
       " Text(7.644756569388919, 58.152558139534875, 'X[11] <= 0.5\\nmse = 34.602\\nsamples = 17\\nvalue = 1.471'),\n",
       " Text(7.61024299571222, 53.095813953488374, 'X[5] <= 0.5\\nmse = 44.379\\nsamples = 13\\nvalue = 1.923'),\n",
       " Text(7.57572942203552, 48.039069767441845, 'X[1] <= 0.009\\nmse = 47.743\\nsamples = 12\\nvalue = 2.083'),\n",
       " Text(7.541215848358822, 42.982325581395344, 'mse = 51.653\\nsamples = 11\\nvalue = 2.273'),\n",
       " Text(7.61024299571222, 42.982325581395344, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(7.644756569388919, 48.039069767441845, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(7.679270143065619, 53.095813953488374, 'mse = 0.0\\nsamples = 4\\nvalue = 0.0'),\n",
       " Text(7.713783716742318, 58.152558139534875, 'mse = 0.0\\nsamples = 16\\nvalue = 0.0'),\n",
       " Text(7.748297290419018, 63.209302325581405, 'mse = 0.0\\nsamples = 1\\nvalue = 35.0'),\n",
       " Text(7.851838011449116, 68.2660465116279, 'X[11] <= 0.5\\nmse = 4.921\\nsamples = 126\\nvalue = 0.198'),\n",
       " Text(7.817324437772417, 63.209302325581405, 'X[9] <= 0.339\\nmse = 5.733\\nsamples = 108\\nvalue = 0.231'),\n",
       " Text(7.782810864095717, 58.152558139534875, 'X[9] <= 0.329\\nmse = 6.188\\nsamples = 100\\nvalue = 0.25'),\n",
       " Text(7.748297290419018, 53.095813953488374, 'mse = 0.0\\nsamples = 5\\nvalue = 0.0'),\n",
       " Text(7.817324437772417, 53.095813953488374, 'X[5] <= 0.5\\nmse = 6.51\\nsamples = 95\\nvalue = 0.263'),\n",
       " Text(7.782810864095717, 48.039069767441845, 'X[8] <= 0.5\\nmse = 6.793\\nsamples = 91\\nvalue = 0.275'),\n",
       " Text(7.748297290419018, 42.982325581395344, 'mse = 7.022\\nsamples = 88\\nvalue = 0.284'),\n",
       " Text(7.817324437772417, 42.982325581395344, 'mse = 0.0\\nsamples = 3\\nvalue = 0.0'),\n",
       " Text(7.851838011449116, 48.039069767441845, 'mse = 0.0\\nsamples = 4\\nvalue = 0.0'),\n",
       " Text(7.851838011449116, 58.152558139534875, 'mse = 0.0\\nsamples = 8\\nvalue = 0.0'),\n",
       " Text(7.886351585125816, 63.209302325581405, 'mse = 0.0\\nsamples = 18\\nvalue = 0.0'),\n",
       " Text(7.731040503580668, 83.43627906976744, 'mse = 0.0\\nsamples = 1\\nvalue = 120.0'),\n",
       " Text(7.989892306155913, 88.49302325581394, 'X[11] <= 1.5\\nmse = 5.04\\nsamples = 246\\nvalue = 0.203'),\n",
       " Text(7.920865158802514, 83.43627906976744, 'X[9] <= 0.417\\nmse = 2.637\\nsamples = 236\\nvalue = 0.106'),\n",
       " Text(7.886351585125816, 78.37953488372094, 'X[9] <= 0.413\\nmse = 6.578\\nsamples = 94\\nvalue = 0.266'),\n",
       " Text(7.851838011449116, 73.3227906976744, 'mse = 0.0\\nsamples = 93\\nvalue = 0.0'),\n",
       " Text(7.920865158802514, 73.3227906976744, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(7.955378732479214, 78.37953488372094, 'mse = 0.0\\nsamples = 142\\nvalue = 0.0'),\n",
       " Text(8.058919453509313, 83.43627906976744, 'X[9] <= 0.449\\nmse = 56.25\\nsamples = 10\\nvalue = 2.5'),\n",
       " Text(8.024405879832614, 78.37953488372094, 'mse = 0.0\\nsamples = 7\\nvalue = 0.0'),\n",
       " Text(8.093433027186013, 78.37953488372094, 'X[9] <= 0.458\\nmse = 138.889\\nsamples = 3\\nvalue = 8.333'),\n",
       " Text(8.058919453509313, 73.3227906976744, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(8.12794660086271, 73.3227906976744, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(8.19697374821611, 93.54976744186047, 'X[11] <= 0.5\\nmse = 253.883\\nsamples = 26\\nvalue = 4.038'),\n",
       " Text(8.16246017453941, 88.49302325581394, 'X[9] <= 0.54\\nmse = 309.524\\nsamples = 21\\nvalue = 5.0'),\n",
       " Text(8.12794660086271, 83.43627906976744, 'mse = 156.25\\nsamples = 2\\nvalue = 12.5'),\n",
       " Text(8.19697374821611, 83.43627906976744, 'X[9] <= 0.551\\nmse = 319.114\\nsamples = 19\\nvalue = 4.211'),\n",
       " Text(8.16246017453941, 78.37953488372094, 'mse = 0.0\\nsamples = 5\\nvalue = 0.0'),\n",
       " Text(8.231487321892809, 78.37953488372094, 'X[8] <= 0.5\\nmse = 424.49\\nsamples = 14\\nvalue = 5.714'),\n",
       " Text(8.19697374821611, 73.3227906976744, 'mse = 454.438\\nsamples = 13\\nvalue = 6.154'),\n",
       " Text(8.266000895569508, 73.3227906976744, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(8.231487321892809, 88.49302325581394, 'mse = 0.0\\nsamples = 5\\nvalue = 0.0'),\n",
       " Text(8.089118830476425, 98.60651162790697, 'mse = 0.0\\nsamples = 116\\nvalue = 0.0'),\n",
       " Text(8.343656436342082, 118.83348837209302, 'X[10] <= 0.031\\nmse = 170.68\\nsamples = 127\\nvalue = 1.772'),\n",
       " Text(8.19697374821611, 113.77674418604651, 'X[9] <= 0.203\\nmse = 3136.0\\nsamples = 5\\nvalue = 28.0'),\n",
       " Text(8.16246017453941, 108.72, 'mse = 0.0\\nsamples = 3\\nvalue = 0.0'),\n",
       " Text(8.231487321892809, 108.72, 'X[9] <= 0.234\\nmse = 4900.0\\nsamples = 2\\nvalue = 70.0'),\n",
       " Text(8.19697374821611, 103.66325581395348, 'mse = 0.0\\nsamples = 1\\nvalue = 140.0'),\n",
       " Text(8.266000895569508, 103.66325581395348, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(8.490339124468054, 113.77674418604651, 'X[5] <= 0.5\\nmse = 19.801\\nsamples = 122\\nvalue = 0.697'),\n",
       " Text(8.404055190276306, 108.72, 'X[9] <= 0.414\\nmse = 10.501\\nsamples = 117\\nvalue = 0.427'),\n",
       " Text(8.335028042922907, 103.66325581395348, 'X[9] <= 0.122\\nmse = 6.067\\nsamples = 102\\nvalue = 0.245'),\n",
       " Text(8.300514469246208, 98.60651162790697, 'X[9] <= 0.116\\nmse = 29.688\\nsamples = 20\\nvalue = 1.25'),\n",
       " Text(8.266000895569508, 93.54976744186047, 'mse = 0.0\\nsamples = 19\\nvalue = 0.0'),\n",
       " Text(8.335028042922907, 93.54976744186047, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(8.369541616599607, 98.60651162790697, 'mse = 0.0\\nsamples = 82\\nvalue = 0.0'),\n",
       " Text(8.473082337629705, 103.66325581395348, 'X[9] <= 0.437\\nmse = 38.889\\nsamples = 15\\nvalue = 1.667'),\n",
       " Text(8.438568763953006, 98.60651162790697, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(8.507595911306405, 98.60651162790697, 'mse = 0.0\\nsamples = 14\\nvalue = 0.0'),\n",
       " Text(8.576623058659804, 108.72, 'X[10] <= 0.036\\nmse = 196.0\\nsamples = 5\\nvalue = 7.0'),\n",
       " Text(8.542109484983104, 103.66325581395348, 'mse = 0.0\\nsamples = 1\\nvalue = 35.0'),\n",
       " Text(8.611136632336503, 103.66325581395348, 'mse = 0.0\\nsamples = 4\\nvalue = 0.0'),\n",
       " Text(9.171982204582868, 123.89023255813953, 'X[9] <= 0.178\\nmse = 131.606\\nsamples = 383\\nvalue = 1.632'),\n",
       " Text(8.947643975684322, 118.83348837209302, 'X[9] <= 0.176\\nmse = 247.591\\nsamples = 169\\nvalue = 2.781'),\n",
       " Text(8.913130402007623, 113.77674418604651, 'X[1] <= 0.014\\nmse = 137.113\\nsamples = 167\\nvalue = 1.976'),\n",
       " Text(8.80096128755835, 108.72, 'X[9] <= 0.165\\nmse = 64.143\\nsamples = 139\\nvalue = 1.151'),\n",
       " Text(8.680163779689902, 103.66325581395348, 'X[10] <= 0.049\\nmse = 10.078\\nsamples = 122\\nvalue = 0.41'),\n",
       " Text(8.611136632336503, 98.60651162790697, 'X[9] <= 0.096\\nmse = 5.58\\nsamples = 111\\nvalue = 0.225'),\n",
       " Text(8.576623058659804, 93.54976744186047, 'X[9] <= 0.095\\nmse = 16.879\\nsamples = 36\\nvalue = 0.694'),\n",
       " Text(8.542109484983104, 88.49302325581394, 'mse = 0.0\\nsamples = 33\\nvalue = 0.0'),\n",
       " Text(8.611136632336503, 88.49302325581394, 'X[10] <= 0.024\\nmse = 138.889\\nsamples = 3\\nvalue = 8.333'),\n",
       " Text(8.576623058659804, 83.43627906976744, 'mse = 156.25\\nsamples = 2\\nvalue = 12.5'),\n",
       " Text(8.645650206013203, 83.43627906976744, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(8.645650206013203, 93.54976744186047, 'mse = 0.0\\nsamples = 75\\nvalue = 0.0'),\n",
       " Text(8.749190927043301, 98.60651162790697, 'X[10] <= 0.051\\nmse = 51.653\\nsamples = 11\\nvalue = 2.273'),\n",
       " Text(8.714677353366602, 93.54976744186047, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(8.78370450072, 93.54976744186047, 'mse = 0.0\\nsamples = 10\\nvalue = 0.0'),\n",
       " Text(8.921758795426797, 103.66325581395348, 'X[10] <= 0.013\\nmse = 419.896\\nsamples = 17\\nvalue = 6.471'),\n",
       " Text(8.887245221750097, 98.60651162790697, 'X[11] <= 0.5\\nmse = 613.636\\nsamples = 11\\nvalue = 10.0'),\n",
       " Text(8.852731648073398, 93.54976744186047, 'X[9] <= 0.173\\nmse = 792.188\\nsamples = 8\\nvalue = 13.75'),\n",
       " Text(8.8182180743967, 88.49302325581394, 'X[9] <= 0.17\\nmse = 884.694\\nsamples = 7\\nvalue = 12.143'),\n",
       " Text(8.78370450072, 83.43627906976744, 'mse = 1156.0\\nsamples = 5\\nvalue = 17.0'),\n",
       " Text(8.852731648073398, 83.43627906976744, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(8.887245221750097, 88.49302325581394, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(8.921758795426797, 93.54976744186047, 'mse = 0.0\\nsamples = 3\\nvalue = 0.0'),\n",
       " Text(8.956272369103496, 98.60651162790697, 'mse = 0.0\\nsamples = 6\\nvalue = 0.0'),\n",
       " Text(9.025299516456895, 108.72, 'X[1] <= 0.014\\nmse = 479.209\\nsamples = 28\\nvalue = 6.071'),\n",
       " Text(8.990785942780196, 103.66325581395348, 'mse = 0.0\\nsamples = 1\\nvalue = 85.0'),\n",
       " Text(9.059813090133595, 103.66325581395348, 'X[10] <= 0.018\\nmse = 257.682\\nsamples = 27\\nvalue = 3.148'),\n",
       " Text(9.025299516456895, 98.60651162790697, 'X[10] <= 0.017\\nmse = 597.107\\nsamples = 11\\nvalue = 7.727'),\n",
       " Text(8.990785942780196, 93.54976744186047, 'mse = 0.0\\nsamples = 10\\nvalue = 0.0'),\n",
       " Text(9.059813090133595, 93.54976744186047, 'mse = 0.0\\nsamples = 1\\nvalue = 85.0'),\n",
       " Text(9.094326663810294, 98.60651162790697, 'mse = 0.0\\nsamples = 16\\nvalue = 0.0'),\n",
       " Text(8.982157549361022, 113.77674418604651, 'mse = 4900.0\\nsamples = 2\\nvalue = 70.0'),\n",
       " Text(9.396320433481414, 118.83348837209302, 'X[10] <= 0.055\\nmse = 38.144\\nsamples = 214\\nvalue = 0.724'),\n",
       " Text(9.28415131903214, 113.77674418604651, 'X[11] <= 0.5\\nmse = 35.876\\nsamples = 211\\nvalue = 0.616'),\n",
       " Text(9.163353811163693, 108.72, 'X[9] <= 0.262\\nmse = 4.085\\nsamples = 152\\nvalue = 0.164'),\n",
       " Text(9.128840237486994, 103.66325581395348, 'mse = 0.0\\nsamples = 81\\nvalue = 0.0'),\n",
       " Text(9.197867384840393, 103.66325581395348, 'X[9] <= 0.265\\nmse = 8.679\\nsamples = 71\\nvalue = 0.352'),\n",
       " Text(9.163353811163693, 98.60651162790697, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(9.232380958517092, 98.60651162790697, 'mse = 0.0\\nsamples = 70\\nvalue = 0.0'),\n",
       " Text(9.40494882690059, 108.72, 'X[9] <= 0.31\\nmse = 115.901\\nsamples = 59\\nvalue = 1.78'),\n",
       " Text(9.33592167954719, 103.66325581395348, 'X[9] <= 0.204\\nmse = 14.872\\nsamples = 41\\nvalue = 0.61'),\n",
       " Text(9.301408105870491, 98.60651162790697, 'X[10] <= 0.02\\nmse = 61.728\\nsamples = 9\\nvalue = 2.778'),\n",
       " Text(9.266894532193792, 93.54976744186047, 'mse = 0.0\\nsamples = 7\\nvalue = 0.0'),\n",
       " Text(9.33592167954719, 93.54976744186047, 'X[10] <= 0.035\\nmse = 156.25\\nsamples = 2\\nvalue = 12.5'),\n",
       " Text(9.301408105870491, 88.49302325581394, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(9.37043525322389, 88.49302325581394, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(9.37043525322389, 98.60651162790697, 'mse = 0.0\\nsamples = 32\\nvalue = 0.0'),\n",
       " Text(9.473975974253989, 103.66325581395348, 'X[9] <= 0.311\\nmse = 335.802\\nsamples = 18\\nvalue = 4.444'),\n",
       " Text(9.43946240057729, 98.60651162790697, 'mse = 0.0\\nsamples = 1\\nvalue = 80.0'),\n",
       " Text(9.508489547930688, 98.60651162790697, 'mse = 0.0\\nsamples = 17\\nvalue = 0.0'),\n",
       " Text(9.508489547930688, 113.77674418604651, 'X[6] <= 1.5\\nmse = 138.889\\nsamples = 3\\nvalue = 8.333'),\n",
       " Text(9.473975974253989, 108.72, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(9.543003121607388, 108.72, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(10.665975043248277, 128.94697674418603, 'X[9] <= 0.082\\nmse = 31.852\\nsamples = 1997\\nvalue = 0.521'),\n",
       " Text(9.853625284697682, 123.89023255813953, 'X[9] <= 0.081\\nmse = 183.283\\nsamples = 112\\nvalue = 1.696'),\n",
       " Text(9.750084563667583, 118.83348837209302, 'X[9] <= 0.079\\nmse = 11.464\\nsamples = 107\\nvalue = 0.467'),\n",
       " Text(9.646543842637485, 113.77674418604651, 'X[9] <= 0.071\\nmse = 6.377\\nsamples = 97\\nvalue = 0.258'),\n",
       " Text(9.612030268960785, 108.72, 'mse = 0.0\\nsamples = 49\\nvalue = 0.0'),\n",
       " Text(9.681057416314184, 108.72, 'X[9] <= 0.072\\nmse = 12.75\\nsamples = 48\\nvalue = 0.521'),\n",
       " Text(9.646543842637485, 103.66325581395348, 'X[6] <= 0.5\\nmse = 38.889\\nsamples = 15\\nvalue = 1.667'),\n",
       " Text(9.612030268960785, 98.60651162790697, 'X[5] <= 0.5\\nmse = 51.653\\nsamples = 11\\nvalue = 2.273'),\n",
       " Text(9.577516695284086, 93.54976744186047, 'X[8] <= 0.5\\nmse = 56.25\\nsamples = 10\\nvalue = 2.5'),\n",
       " Text(9.543003121607388, 88.49302325581394, 'mse = 61.728\\nsamples = 9\\nvalue = 2.778'),\n",
       " Text(9.612030268960785, 88.49302325581394, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(9.646543842637485, 93.54976744186047, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(9.681057416314184, 98.60651162790697, 'mse = 0.0\\nsamples = 4\\nvalue = 0.0'),\n",
       " Text(9.715570989990884, 103.66325581395348, 'mse = 0.0\\nsamples = 33\\nvalue = 0.0'),\n",
       " Text(9.853625284697682, 113.77674418604651, 'X[6] <= 0.5\\nmse = 56.25\\nsamples = 10\\nvalue = 2.5'),\n",
       " Text(9.819111711020982, 108.72, 'X[11] <= 0.5\\nmse = 86.806\\nsamples = 6\\nvalue = 4.167'),\n",
       " Text(9.784598137344283, 103.66325581395348, 'X[9] <= 0.081\\nmse = 100.0\\nsamples = 5\\nvalue = 5.0'),\n",
       " Text(9.750084563667583, 98.60651162790697, 'mse = 117.188\\nsamples = 4\\nvalue = 6.25'),\n",
       " Text(9.819111711020982, 98.60651162790697, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(9.853625284697682, 103.66325581395348, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(9.888138858374381, 108.72, 'mse = 0.0\\nsamples = 4\\nvalue = 0.0'),\n",
       " Text(9.95716600572778, 118.83348837209302, 'X[6] <= 0.5\\nmse = 3136.0\\nsamples = 5\\nvalue = 28.0'),\n",
       " Text(9.92265243205108, 113.77674418604651, 'mse = 0.0\\nsamples = 1\\nvalue = 140.0'),\n",
       " Text(9.99167957940448, 113.77674418604651, 'mse = 0.0\\nsamples = 4\\nvalue = 0.0'),\n",
       " Text(11.478324801798871, 123.89023255813953, 'X[10] <= 0.05\\nmse = 22.767\\nsamples = 1885\\nvalue = 0.451'),\n",
       " Text(11.135750619328196, 118.83348837209302, 'X[11] <= 1.5\\nmse = 20.219\\nsamples = 1787\\nvalue = 0.406'),\n",
       " Text(10.830251564830535, 113.77674418604651, 'X[5] <= 0.5\\nmse = 19.762\\nsamples = 1736\\nvalue = 0.374'),\n",
       " Text(10.460848471572112, 108.72, 'X[9] <= 0.24\\nmse = 16.764\\nsamples = 1631\\nvalue = 0.334'),\n",
       " Text(10.008936366242828, 103.66325581395348, 'X[9] <= 0.166\\nmse = 9.111\\nsamples = 928\\nvalue = 0.145'),\n",
       " Text(9.888138858374381, 98.60651162790697, 'X[9] <= 0.092\\nmse = 1.177\\nsamples = 530\\nvalue = 0.047'),\n",
       " Text(9.853625284697682, 93.54976744186047, 'X[10] <= 0.029\\nmse = 9.918\\nsamples = 62\\nvalue = 0.403'),\n",
       " Text(9.819111711020982, 88.49302325581394, 'mse = 0.0\\nsamples = 52\\nvalue = 0.0'),\n",
       " Text(9.888138858374381, 88.49302325581394, 'X[10] <= 0.032\\nmse = 56.25\\nsamples = 10\\nvalue = 2.5'),\n",
       " Text(9.853625284697682, 83.43627906976744, 'X[6] <= 0.5\\nmse = 138.889\\nsamples = 3\\nvalue = 8.333'),\n",
       " Text(9.819111711020982, 78.37953488372094, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(9.888138858374381, 78.37953488372094, 'mse = 156.25\\nsamples = 2\\nvalue = 12.5'),\n",
       " Text(9.92265243205108, 83.43627906976744, 'mse = 0.0\\nsamples = 7\\nvalue = 0.0'),\n",
       " Text(9.92265243205108, 93.54976744186047, 'mse = 0.0\\nsamples = 468\\nvalue = 0.0'),\n",
       " Text(10.129733874111277, 98.60651162790697, 'X[9] <= 0.168\\nmse = 19.647\\nsamples = 398\\nvalue = 0.276'),\n",
       " Text(10.060706726757878, 93.54976744186047, 'X[6] <= 0.5\\nmse = 107.811\\nsamples = 66\\nvalue = 1.288'),\n",
       " Text(10.026193153081179, 88.49302325581394, 'X[10] <= 0.021\\nmse = 133.749\\nsamples = 53\\nvalue = 1.604'),\n",
       " Text(9.99167957940448, 83.43627906976744, 'X[8] <= 0.5\\nmse = 141.61\\nsamples = 50\\nvalue = 1.7'),\n",
       " Text(9.95716600572778, 78.37953488372094, 'X[11] <= 0.5\\nmse = 147.385\\nsamples = 48\\nvalue = 1.771'),\n",
       " Text(9.92265243205108, 73.3227906976744, 'mse = 153.651\\nsamples = 46\\nvalue = 1.848'),\n",
       " Text(9.99167957940448, 73.3227906976744, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(10.026193153081179, 78.37953488372094, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(10.060706726757878, 83.43627906976744, 'mse = 0.0\\nsamples = 3\\nvalue = 0.0'),\n",
       " Text(10.095220300434578, 88.49302325581394, 'mse = 0.0\\nsamples = 13\\nvalue = 0.0'),\n",
       " Text(10.198761021464676, 93.54976744186047, 'X[6] <= 0.5\\nmse = 1.877\\nsamples = 332\\nvalue = 0.075'),\n",
       " Text(10.164247447787977, 88.49302325581394, 'mse = 0.0\\nsamples = 199\\nvalue = 0.0'),\n",
       " Text(10.233274595141376, 88.49302325581394, 'X[9] <= 0.201\\nmse = 4.664\\nsamples = 133\\nvalue = 0.188'),\n",
       " Text(10.198761021464676, 83.43627906976744, 'X[9] <= 0.199\\nmse = 9.189\\nsamples = 67\\nvalue = 0.373'),\n",
       " Text(10.164247447787977, 78.37953488372094, 'mse = 0.0\\nsamples = 53\\nvalue = 0.0'),\n",
       " Text(10.233274595141376, 78.37953488372094, 'X[0] <= 0.5\\nmse = 41.454\\nsamples = 14\\nvalue = 1.786'),\n",
       " Text(10.198761021464676, 73.3227906976744, 'X[10] <= 0.008\\nmse = 61.728\\nsamples = 9\\nvalue = 2.778'),\n",
       " Text(10.164247447787977, 68.2660465116279, 'X[11] <= 0.5\\nmse = 100.0\\nsamples = 5\\nvalue = 5.0'),\n",
       " Text(10.129733874111277, 63.209302325581405, 'mse = 117.188\\nsamples = 4\\nvalue = 6.25'),\n",
       " Text(10.198761021464676, 63.209302325581405, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(10.233274595141376, 68.2660465116279, 'mse = 0.0\\nsamples = 4\\nvalue = 0.0'),\n",
       " Text(10.267788168818075, 73.3227906976744, 'mse = 0.0\\nsamples = 5\\nvalue = 0.0'),\n",
       " Text(10.267788168818075, 83.43627906976744, 'mse = 0.0\\nsamples = 66\\nvalue = 0.0'),\n",
       " Text(10.912760576901395, 103.66325581395348, 'X[9] <= 0.316\\nmse = 26.758\\nsamples = 703\\nvalue = 0.583'),\n",
       " Text(10.798434364097329, 98.60651162790697, 'X[9] <= 0.315\\nmse = 61.188\\nsamples = 274\\nvalue = 1.223'),\n",
       " Text(10.673322659519293, 93.54976744186047, 'X[9] <= 0.26\\nmse = 38.875\\nsamples = 271\\nvalue = 0.941'),\n",
       " Text(10.52663997139332, 88.49302325581394, 'X[10] <= 0.039\\nmse = 79.576\\nsamples = 101\\nvalue = 1.535'),\n",
       " Text(10.492126397716621, 83.43627906976744, 'X[9] <= 0.259\\nmse = 74.81\\nsamples = 100\\nvalue = 1.3'),\n",
       " Text(10.405842463524872, 78.37953488372094, 'X[10] <= 0.03\\nmse = 70.536\\nsamples = 98\\nvalue = 1.071'),\n",
       " Text(10.336815316171473, 73.3227906976744, 'X[6] <= 0.5\\nmse = 68.809\\nsamples = 92\\nvalue = 0.87'),\n",
       " Text(10.302301742494775, 68.2660465116279, 'X[9] <= 0.247\\nmse = 94.097\\nsamples = 67\\nvalue = 1.194'),\n",
       " Text(10.267788168818075, 63.209302325581405, 'mse = 0.0\\nsamples = 6\\nvalue = 0.0'),\n",
       " Text(10.336815316171473, 63.209302325581405, 'X[8] <= 0.5\\nmse = 103.198\\nsamples = 61\\nvalue = 1.311'),\n",
       " Text(10.302301742494775, 58.152558139534875, 'X[9] <= 0.254\\nmse = 108.442\\nsamples = 58\\nvalue = 1.379'),\n",
       " Text(10.267788168818075, 53.095813953488374, 'X[11] <= 0.5\\nmse = 114.248\\nsamples = 55\\nvalue = 1.455'),\n",
       " Text(10.233274595141376, 48.039069767441845, 'mse = 118.476\\nsamples = 53\\nvalue = 1.509'),\n",
       " Text(10.302301742494775, 48.039069767441845, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(10.336815316171473, 53.095813953488374, 'mse = 0.0\\nsamples = 3\\nvalue = 0.0'),\n",
       " Text(10.371328889848172, 58.152558139534875, 'mse = 0.0\\nsamples = 3\\nvalue = 0.0'),\n",
       " Text(10.371328889848172, 68.2660465116279, 'mse = 0.0\\nsamples = 25\\nvalue = 0.0'),\n",
       " Text(10.47486961087827, 73.3227906976744, 'X[10] <= 0.033\\nmse = 86.806\\nsamples = 6\\nvalue = 4.167'),\n",
       " Text(10.440356037201571, 68.2660465116279, 'X[6] <= 0.5\\nmse = 156.25\\nsamples = 2\\nvalue = 12.5'),\n",
       " Text(10.405842463524872, 63.209302325581405, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(10.47486961087827, 63.209302325581405, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(10.50938318455497, 68.2660465116279, 'mse = 0.0\\nsamples = 4\\nvalue = 0.0'),\n",
       " Text(10.578410331908369, 78.37953488372094, 'X[11] <= 0.5\\nmse = 156.25\\nsamples = 2\\nvalue = 12.5'),\n",
       " Text(10.54389675823167, 73.3227906976744, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(10.612923905585069, 73.3227906976744, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(10.56115354507002, 83.43627906976744, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(10.820005347645266, 88.49302325581394, 'X[9] <= 0.311\\nmse = 14.36\\nsamples = 170\\nvalue = 0.588'),\n",
       " Text(10.750978200291867, 83.43627906976744, 'X[6] <= 0.5\\nmse = 11.429\\nsamples = 161\\nvalue = 0.466'),\n",
       " Text(10.716464626615167, 78.37953488372094, 'X[11] <= 0.5\\nmse = 16.879\\nsamples = 108\\nvalue = 0.694'),\n",
       " Text(10.681951052938468, 73.3227906976744, 'X[9] <= 0.302\\nmse = 19.925\\nsamples = 91\\nvalue = 0.824'),\n",
       " Text(10.647437479261768, 68.2660465116279, 'X[9] <= 0.298\\nmse = 22.559\\nsamples = 80\\nvalue = 0.938'),\n",
       " Text(10.56115354507002, 63.209302325581405, 'X[9] <= 0.273\\nmse = 18.639\\nsamples = 65\\nvalue = 0.769'),\n",
       " Text(10.492126397716621, 58.152558139534875, 'X[9] <= 0.271\\nmse = 31.163\\nsamples = 19\\nvalue = 1.316'),\n",
       " Text(10.457612824039922, 53.095813953488374, 'mse = 0.0\\nsamples = 8\\nvalue = 0.0'),\n",
       " Text(10.52663997139332, 53.095813953488374, 'mse = 51.653\\nsamples = 11\\nvalue = 2.273'),\n",
       " Text(10.63018069242342, 58.152558139534875, 'X[9] <= 0.283\\nmse = 13.292\\nsamples = 46\\nvalue = 0.543'),\n",
       " Text(10.59566711874672, 53.095813953488374, 'mse = 0.0\\nsamples = 12\\nvalue = 0.0'),\n",
       " Text(10.664694266100119, 53.095813953488374, 'X[9] <= 0.288\\nmse = 17.842\\nsamples = 34\\nvalue = 0.735'),\n",
       " Text(10.63018069242342, 48.039069767441845, 'X[8] <= 0.5\\nmse = 23.114\\nsamples = 26\\nvalue = 0.962'),\n",
       " Text(10.59566711874672, 42.982325581395344, 'mse = 27.118\\nsamples = 22\\nvalue = 1.136'),\n",
       " Text(10.664694266100119, 42.982325581395344, 'mse = 0.0\\nsamples = 4\\nvalue = 0.0'),\n",
       " Text(10.699207839776816, 48.039069767441845, 'mse = 0.0\\nsamples = 8\\nvalue = 0.0'),\n",
       " Text(10.733721413453516, 63.209302325581405, 'X[10] <= 0.017\\nmse = 38.889\\nsamples = 15\\nvalue = 1.667'),\n",
       " Text(10.699207839776816, 58.152558139534875, 'mse = 44.379\\nsamples = 13\\nvalue = 1.923'),\n",
       " Text(10.768234987130215, 58.152558139534875, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(10.716464626615167, 68.2660465116279, 'mse = 0.0\\nsamples = 11\\nvalue = 0.0'),\n",
       " Text(10.750978200291867, 73.3227906976744, 'mse = 0.0\\nsamples = 17\\nvalue = 0.0'),\n",
       " Text(10.785491773968566, 78.37953488372094, 'mse = 0.0\\nsamples = 53\\nvalue = 0.0'),\n",
       " Text(10.889032494998665, 83.43627906976744, 'X[6] <= 0.5\\nmse = 61.728\\nsamples = 9\\nvalue = 2.778'),\n",
       " Text(10.854518921321965, 78.37953488372094, 'mse = 0.0\\nsamples = 5\\nvalue = 0.0'),\n",
       " Text(10.923546068675364, 78.37953488372094, 'X[11] <= 0.5\\nmse = 117.188\\nsamples = 4\\nvalue = 6.25'),\n",
       " Text(10.889032494998665, 73.3227906976744, 'mse = 156.25\\nsamples = 2\\nvalue = 12.5'),\n",
       " Text(10.958059642352064, 73.3227906976744, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(10.923546068675364, 93.54976744186047, 'X[6] <= 0.5\\nmse = 1422.222\\nsamples = 3\\nvalue = 26.667'),\n",
       " Text(10.889032494998665, 88.49302325581394, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(10.958059642352064, 88.49302325581394, 'mse = 0.0\\nsamples = 1\\nvalue = 80.0'),\n",
       " Text(11.027086789705463, 98.60651162790697, 'X[9] <= 0.528\\nmse = 4.34\\nsamples = 429\\nvalue = 0.175'),\n",
       " Text(10.992573216028763, 93.54976744186047, 'mse = 0.0\\nsamples = 326\\nvalue = 0.0'),\n",
       " Text(11.06160036338216, 93.54976744186047, 'X[9] <= 0.531\\nmse = 17.674\\nsamples = 103\\nvalue = 0.728'),\n",
       " Text(11.027086789705463, 88.49302325581394, 'mse = 156.25\\nsamples = 2\\nvalue = 12.5'),\n",
       " Text(11.09611393705886, 88.49302325581394, 'X[6] <= 0.5\\nmse = 12.131\\nsamples = 101\\nvalue = 0.495'),\n",
       " Text(11.027086789705463, 83.43627906976744, 'X[9] <= 0.742\\nmse = 6.72\\nsamples = 92\\nvalue = 0.272'),\n",
       " Text(10.992573216028763, 78.37953488372094, 'mse = 0.0\\nsamples = 69\\nvalue = 0.0'),\n",
       " Text(11.06160036338216, 78.37953488372094, 'X[9] <= 0.76\\nmse = 25.992\\nsamples = 23\\nvalue = 1.087'),\n",
       " Text(11.027086789705463, 73.3227906976744, 'mse = 100.0\\nsamples = 5\\nvalue = 5.0'),\n",
       " Text(11.09611393705886, 73.3227906976744, 'mse = 0.0\\nsamples = 18\\nvalue = 0.0'),\n",
       " Text(11.165141084412259, 83.43627906976744, 'X[9] <= 0.572\\nmse = 61.728\\nsamples = 9\\nvalue = 2.778'),\n",
       " Text(11.13062751073556, 78.37953488372094, 'mse = 0.0\\nsamples = 5\\nvalue = 0.0'),\n",
       " Text(11.199654658088958, 78.37953488372094, 'X[9] <= 0.594\\nmse = 117.188\\nsamples = 4\\nvalue = 6.25'),\n",
       " Text(11.165141084412259, 73.3227906976744, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(11.234168231765658, 73.3227906976744, 'mse = 0.0\\nsamples = 3\\nvalue = 0.0'),\n",
       " Text(11.199654658088958, 108.72, 'X[9] <= 0.118\\nmse = 65.905\\nsamples = 105\\nvalue = 1.0'),\n",
       " Text(11.13062751073556, 103.66325581395348, 'X[9] <= 0.117\\nmse = 255.556\\nsamples = 24\\nvalue = 3.333'),\n",
       " Text(11.09611393705886, 98.60651162790697, 'mse = 0.0\\nsamples = 22\\nvalue = 0.0'),\n",
       " Text(11.165141084412259, 98.60651162790697, 'mse = 1600.0\\nsamples = 2\\nvalue = 40.0'),\n",
       " Text(11.268681805442357, 103.66325581395348, 'X[6] <= 0.5\\nmse = 7.621\\nsamples = 81\\nvalue = 0.309'),\n",
       " Text(11.234168231765658, 98.60651162790697, 'X[9] <= 0.205\\nmse = 16.435\\nsamples = 37\\nvalue = 0.676'),\n",
       " Text(11.199654658088958, 93.54976744186047, 'X[9] <= 0.191\\nmse = 32.793\\nsamples = 18\\nvalue = 1.389'),\n",
       " Text(11.165141084412259, 88.49302325581394, 'mse = 0.0\\nsamples = 15\\nvalue = 0.0'),\n",
       " Text(11.234168231765658, 88.49302325581394, 'mse = 138.889\\nsamples = 3\\nvalue = 8.333'),\n",
       " Text(11.268681805442357, 93.54976744186047, 'mse = 0.0\\nsamples = 19\\nvalue = 0.0'),\n",
       " Text(11.303195379119057, 98.60651162790697, 'mse = 0.0\\nsamples = 44\\nvalue = 0.0'),\n",
       " Text(11.441249673825855, 113.77674418604651, 'X[9] <= 0.131\\nmse = 34.602\\nsamples = 51\\nvalue = 1.471'),\n",
       " Text(11.372222526472456, 108.72, 'X[9] <= 0.119\\nmse = 156.25\\nsamples = 2\\nvalue = 12.5'),\n",
       " Text(11.337708952795756, 103.66325581395348, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(11.406736100149155, 103.66325581395348, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(11.510276821179254, 108.72, 'X[9] <= 0.426\\nmse = 24.469\\nsamples = 49\\nvalue = 1.02'),\n",
       " Text(11.475763247502554, 103.66325581395348, 'mse = 0.0\\nsamples = 39\\nvalue = 0.0'),\n",
       " Text(11.544790394855953, 103.66325581395348, 'X[9] <= 0.45\\nmse = 100.0\\nsamples = 10\\nvalue = 5.0'),\n",
       " Text(11.510276821179254, 98.60651162790697, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(11.579303968532653, 98.60651162790697, 'X[9] <= 0.513\\nmse = 61.728\\nsamples = 9\\nvalue = 2.778'),\n",
       " Text(11.544790394855953, 93.54976744186047, 'mse = 0.0\\nsamples = 4\\nvalue = 0.0'),\n",
       " Text(11.613817542209352, 93.54976744186047, 'X[9] <= 0.531\\nmse = 100.0\\nsamples = 5\\nvalue = 5.0'),\n",
       " Text(11.579303968532653, 88.49302325581394, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(11.648331115886052, 88.49302325581394, 'mse = 0.0\\nsamples = 4\\nvalue = 0.0'),\n",
       " Text(11.820898984269547, 118.83348837209302, 'X[6] <= 0.5\\nmse = 68.526\\nsamples = 98\\nvalue = 1.276'),\n",
       " Text(11.786385410592848, 113.77674418604651, 'X[9] <= 0.154\\nmse = 128.797\\nsamples = 51\\nvalue = 2.451'),\n",
       " Text(11.75187183691615, 108.72, 'mse = 0.0\\nsamples = 17\\nvalue = 0.0'),\n",
       " Text(11.820898984269547, 108.72, 'X[9] <= 0.162\\nmse = 188.689\\nsamples = 34\\nvalue = 3.676'),\n",
       " Text(11.75187183691615, 103.66325581395348, 'X[8] <= 0.5\\nmse = 555.556\\nsamples = 9\\nvalue = 8.333'),\n",
       " Text(11.71735826323945, 98.60651162790697, 'X[5] <= 0.5\\nmse = 615.234\\nsamples = 8\\nvalue = 9.375'),\n",
       " Text(11.682844689562751, 93.54976744186047, 'mse = 688.776\\nsamples = 7\\nvalue = 10.714'),\n",
       " Text(11.75187183691615, 93.54976744186047, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(11.786385410592848, 98.60651162790697, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(11.889926131622946, 103.66325581395348, 'X[9] <= 0.219\\nmse = 46.0\\nsamples = 25\\nvalue = 2.0'),\n",
       " Text(11.855412557946247, 98.60651162790697, 'mse = 0.0\\nsamples = 7\\nvalue = 0.0'),\n",
       " Text(11.924439705299646, 98.60651162790697, 'X[9] <= 0.236\\nmse = 61.728\\nsamples = 18\\nvalue = 2.778'),\n",
       " Text(11.889926131622946, 93.54976744186047, 'mse = 138.889\\nsamples = 3\\nvalue = 8.333'),\n",
       " Text(11.958953278976345, 93.54976744186047, 'X[10] <= 0.051\\nmse = 38.889\\nsamples = 15\\nvalue = 1.667'),\n",
       " Text(11.924439705299646, 88.49302325581394, 'X[11] <= 0.5\\nmse = 100.0\\nsamples = 5\\nvalue = 5.0'),\n",
       " Text(11.889926131622946, 83.43627906976744, 'mse = 0.0\\nsamples = 3\\nvalue = 0.0'),\n",
       " Text(11.958953278976345, 83.43627906976744, 'X[9] <= 0.3\\nmse = 156.25\\nsamples = 2\\nvalue = 12.5'),\n",
       " Text(11.924439705299646, 78.37953488372094, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(11.993466852653045, 78.37953488372094, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(11.993466852653045, 88.49302325581394, 'mse = 0.0\\nsamples = 10\\nvalue = 0.0'),\n",
       " Text(11.855412557946247, 113.77674418604651, 'mse = 0.0\\nsamples = 47\\nvalue = 0.0'),\n",
       " Text(12.390372949935088, 134.00372093023256, 'X[10] <= 0.034\\nmse = 88.327\\nsamples = 119\\nvalue = 1.975'),\n",
       " Text(12.200548294713242, 128.94697674418603, 'X[9] <= 0.581\\nmse = 25.726\\nsamples = 93\\nvalue = 1.075'),\n",
       " Text(12.097007573683143, 123.89023255813953, 'X[10] <= 0.021\\nmse = 15.615\\nsamples = 78\\nvalue = 0.641'),\n",
       " Text(11.993466852653045, 118.83348837209302, 'X[11] <= 4.5\\nmse = 9.326\\nsamples = 66\\nvalue = 0.379'),\n",
       " Text(11.958953278976345, 113.77674418604651, 'mse = 0.0\\nsamples = 43\\nvalue = 0.0'),\n",
       " Text(12.027980426329744, 113.77674418604651, 'X[11] <= 5.5\\nmse = 25.992\\nsamples = 23\\nvalue = 1.087'),\n",
       " Text(11.993466852653045, 108.72, 'X[9] <= 0.343\\nmse = 117.188\\nsamples = 4\\nvalue = 6.25'),\n",
       " Text(11.958953278976345, 103.66325581395348, 'mse = 0.0\\nsamples = 3\\nvalue = 0.0'),\n",
       " Text(12.027980426329744, 103.66325581395348, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(12.062494000006444, 108.72, 'mse = 0.0\\nsamples = 19\\nvalue = 0.0'),\n",
       " Text(12.200548294713242, 118.83348837209302, 'X[10] <= 0.024\\nmse = 47.743\\nsamples = 12\\nvalue = 2.083'),\n",
       " Text(12.166034721036542, 113.77674418604651, 'X[1] <= 0.011\\nmse = 156.25\\nsamples = 2\\nvalue = 12.5'),\n",
       " Text(12.131521147359843, 108.72, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(12.200548294713242, 108.72, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(12.235061868389941, 113.77674418604651, 'mse = 0.0\\nsamples = 10\\nvalue = 0.0'),\n",
       " Text(12.30408901574334, 123.89023255813953, 'X[9] <= 0.602\\nmse = 72.222\\nsamples = 15\\nvalue = 3.333'),\n",
       " Text(12.26957544206664, 118.83348837209302, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(12.33860258942004, 118.83348837209302, 'X[12] <= 8.5\\nmse = 41.454\\nsamples = 14\\nvalue = 1.786'),\n",
       " Text(12.30408901574334, 113.77674418604651, 'mse = 0.0\\nsamples = 10\\nvalue = 0.0'),\n",
       " Text(12.37311616309674, 113.77674418604651, 'X[9] <= 0.776\\nmse = 117.188\\nsamples = 4\\nvalue = 6.25'),\n",
       " Text(12.33860258942004, 108.72, 'mse = 0.0\\nsamples = 3\\nvalue = 0.0'),\n",
       " Text(12.407629736773439, 108.72, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(12.580197605156934, 128.94697674418603, 'X[10] <= 0.04\\nmse = 299.001\\nsamples = 26\\nvalue = 5.192'),\n",
       " Text(12.511170457803537, 123.89023255813953, 'X[10] <= 0.039\\nmse = 1206.25\\nsamples = 4\\nvalue = 27.5'),\n",
       " Text(12.476656884126838, 118.83348837209302, 'X[10] <= 0.034\\nmse = 138.889\\nsamples = 3\\nvalue = 8.333'),\n",
       " Text(12.442143310450138, 113.77674418604651, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(12.511170457803537, 113.77674418604651, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(12.545684031480235, 118.83348837209302, 'mse = 0.0\\nsamples = 1\\nvalue = 85.0'),\n",
       " Text(12.649224752510333, 123.89023255813953, 'X[9] <= 0.406\\nmse = 27.118\\nsamples = 22\\nvalue = 1.136'),\n",
       " Text(12.614711178833634, 118.83348837209302, 'mse = 0.0\\nsamples = 20\\nvalue = 0.0'),\n",
       " Text(12.683738326187033, 118.83348837209302, 'X[10] <= 0.049\\nmse = 156.25\\nsamples = 2\\nvalue = 12.5'),\n",
       " Text(12.649224752510333, 113.77674418604651, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(12.718251899863732, 113.77674418604651, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(14.521906318758317, 144.1172093023256, 'X[10] <= 0.027\\nmse = 98.016\\nsamples = 1095\\nvalue = 1.781'),\n",
       " Text(13.841257526166416, 139.06046511627906, 'X[9] <= 0.59\\nmse = 51.773\\nsamples = 783\\nvalue = 1.137'),\n",
       " Text(13.52156880905363, 134.00372093023256, 'X[1] <= 0.025\\nmse = 50.57\\nsamples = 779\\nvalue = 1.078'),\n",
       " Text(12.985732095858154, 128.94697674418603, 'X[8] <= 2.0\\nmse = 151.899\\nsamples = 150\\nvalue = 2.367'),\n",
       " Text(12.951218522181454, 123.89023255813953, 'X[9] <= 0.207\\nmse = 149.457\\nsamples = 149\\nvalue = 2.215'),\n",
       " Text(12.916704948504755, 118.83348837209302, 'X[9] <= 0.199\\nmse = 230.039\\nsamples = 95\\nvalue = 3.474'),\n",
       " Text(12.787279047217131, 113.77674418604651, 'X[9] <= 0.094\\nmse = 100.806\\nsamples = 90\\nvalue = 1.833'),\n",
       " Text(12.700995113025384, 108.72, 'X[9] <= 0.092\\nmse = 356.916\\nsamples = 21\\nvalue = 5.476'),\n",
       " Text(12.631967965671985, 103.66325581395348, 'X[5] <= 0.5\\nmse = 49.827\\nsamples = 17\\nvalue = 1.765'),\n",
       " Text(12.597454391995285, 98.60651162790697, 'mse = 0.0\\nsamples = 15\\nvalue = 0.0'),\n",
       " Text(12.666481539348684, 98.60651162790697, 'X[9] <= 0.073\\nmse = 225.0\\nsamples = 2\\nvalue = 15.0'),\n",
       " Text(12.631967965671985, 93.54976744186047, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(12.700995113025384, 93.54976744186047, 'mse = 0.0\\nsamples = 1\\nvalue = 30.0'),\n",
       " Text(12.770022260378783, 103.66325581395348, 'X[6] <= 0.5\\nmse = 1354.688\\nsamples = 4\\nvalue = 21.25'),\n",
       " Text(12.735508686702083, 98.60651162790697, 'mse = 0.0\\nsamples = 1\\nvalue = 85.0'),\n",
       " Text(12.804535834055482, 98.60651162790697, 'mse = 0.0\\nsamples = 3\\nvalue = 0.0'),\n",
       " Text(12.873562981408881, 108.72, 'X[11] <= 0.5\\nmse = 17.591\\nsamples = 69\\nvalue = 0.725'),\n",
       " Text(12.839049407732182, 103.66325581395348, 'mse = 0.0\\nsamples = 54\\nvalue = 0.0'),\n",
       " Text(12.908076555085579, 103.66325581395348, 'X[9] <= 0.134\\nmse = 72.222\\nsamples = 15\\nvalue = 3.333'),\n",
       " Text(12.873562981408881, 98.60651162790697, 'X[9] <= 0.122\\nmse = 156.25\\nsamples = 4\\nvalue = 12.5'),\n",
       " Text(12.839049407732182, 93.54976744186047, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(12.908076555085579, 93.54976744186047, 'mse = 0.0\\nsamples = 2\\nvalue = 25.0'),\n",
       " Text(12.942590128762278, 98.60651162790697, 'mse = 0.0\\nsamples = 11\\nvalue = 0.0'),\n",
       " Text(13.046130849792377, 113.77674418604651, 'X[6] <= 0.5\\nmse = 1636.0\\nsamples = 5\\nvalue = 33.0'),\n",
       " Text(13.011617276115677, 108.72, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(13.080644423469076, 108.72, 'X[10] <= 0.011\\nmse = 1516.667\\nsamples = 3\\nvalue = 55.0'),\n",
       " Text(13.046130849792377, 103.66325581395348, 'X[11] <= 0.5\\nmse = 6.25\\nsamples = 2\\nvalue = 82.5'),\n",
       " Text(13.011617276115677, 98.60651162790697, 'mse = 0.0\\nsamples = 1\\nvalue = 80.0'),\n",
       " Text(13.080644423469076, 98.60651162790697, 'mse = 0.0\\nsamples = 1\\nvalue = 85.0'),\n",
       " Text(13.115157997145776, 103.66325581395348, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(12.985732095858154, 118.83348837209302, 'mse = 0.0\\nsamples = 54\\nvalue = 0.0'),\n",
       " Text(13.020245669534853, 123.89023255813953, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(14.057405522249105, 128.94697674418603, 'X[5] <= 2.5\\nmse = 25.916\\nsamples = 629\\nvalue = 0.771'),\n",
       " Text(13.912475476536402, 123.89023255813953, 'X[1] <= 0.036\\nmse = 25.06\\nsamples = 627\\nvalue = 0.734'),\n",
       " Text(13.726156106141095, 118.83348837209302, 'X[1] <= 0.03\\nmse = 13.978\\nsamples = 544\\nvalue = 0.561'),\n",
       " Text(13.612369167925728, 113.77674418604651, 'X[9] <= 0.493\\nmse = 25.702\\nsamples = 256\\nvalue = 1.074'),\n",
       " Text(13.488336012525089, 108.72, 'X[2] <= 0.5\\nmse = 23.638\\nsamples = 254\\nvalue = 0.984'),\n",
       " Text(13.34381042275391, 103.66325581395348, 'X[8] <= 0.5\\nmse = 21.607\\nsamples = 251\\nvalue = 0.896'),\n",
       " Text(13.15829996424165, 98.60651162790697, 'X[10] <= 0.013\\nmse = 18.137\\nsamples = 234\\nvalue = 0.748'),\n",
       " Text(12.977103702438978, 93.54976744186047, 'X[9] <= 0.174\\nmse = 10.356\\nsamples = 178\\nvalue = 0.421'),\n",
       " Text(12.942590128762278, 88.49302325581394, 'X[9] <= 0.17\\nmse = 22.029\\nsamples = 82\\nvalue = 0.915'),\n",
       " Text(12.873562981408881, 83.43627906976744, 'X[1] <= 0.027\\nmse = 15.234\\nsamples = 80\\nvalue = 0.625'),\n",
       " Text(12.839049407732182, 78.37953488372094, 'X[9] <= 0.126\\nmse = 29.001\\nsamples = 41\\nvalue = 1.22'),\n",
       " Text(12.804535834055482, 73.3227906976744, 'X[9] <= 0.12\\nmse = 58.864\\nsamples = 19\\nvalue = 2.632'),\n",
       " Text(12.770022260378783, 68.2660465116279, 'X[9] <= 0.107\\nmse = 32.793\\nsamples = 18\\nvalue = 1.389'),\n",
       " Text(12.735508686702083, 63.209302325581405, 'mse = 0.0\\nsamples = 15\\nvalue = 0.0'),\n",
       " Text(12.804535834055482, 63.209302325581405, 'X[9] <= 0.111\\nmse = 138.889\\nsamples = 3\\nvalue = 8.333'),\n",
       " Text(12.770022260378783, 58.152558139534875, 'mse = 156.25\\nsamples = 2\\nvalue = 12.5'),\n",
       " Text(12.839049407732182, 58.152558139534875, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(12.839049407732182, 68.2660465116279, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(12.873562981408881, 73.3227906976744, 'mse = 0.0\\nsamples = 22\\nvalue = 0.0'),\n",
       " Text(12.908076555085579, 78.37953488372094, 'mse = 0.0\\nsamples = 39\\nvalue = 0.0'),\n",
       " Text(13.011617276115677, 83.43627906976744, 'X[6] <= 0.5\\nmse = 156.25\\nsamples = 2\\nvalue = 12.5'),\n",
       " Text(12.977103702438978, 78.37953488372094, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(13.046130849792377, 78.37953488372094, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(13.011617276115677, 88.49302325581394, 'mse = 0.0\\nsamples = 96\\nvalue = 0.0'),\n",
       " Text(13.339496226044322, 93.54976744186047, 'X[1] <= 0.028\\nmse = 41.454\\nsamples = 56\\nvalue = 1.786'),\n",
       " Text(13.218698718175874, 88.49302325581394, 'X[10] <= 0.014\\nmse = 24.0\\nsamples = 50\\nvalue = 1.0'),\n",
       " Text(13.149671570822475, 83.43627906976744, 'X[9] <= 0.101\\nmse = 156.25\\nsamples = 2\\nvalue = 12.5'),\n",
       " Text(13.115157997145776, 78.37953488372094, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(13.184185144499175, 78.37953488372094, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(13.287725865529273, 83.43627906976744, 'X[9] <= 0.091\\nmse = 12.75\\nsamples = 48\\nvalue = 0.521'),\n",
       " Text(13.253212291852574, 78.37953488372094, 'X[12] <= 8.5\\nmse = 117.188\\nsamples = 4\\nvalue = 6.25'),\n",
       " Text(13.218698718175874, 73.3227906976744, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(13.287725865529273, 73.3227906976744, 'X[10] <= 0.026\\nmse = 156.25\\nsamples = 2\\nvalue = 12.5'),\n",
       " Text(13.253212291852574, 68.2660465116279, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(13.322239439205973, 68.2660465116279, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(13.322239439205973, 78.37953488372094, 'mse = 0.0\\nsamples = 44\\nvalue = 0.0'),\n",
       " Text(13.46029373391277, 88.49302325581394, 'X[12] <= 8.5\\nmse = 138.889\\nsamples = 6\\nvalue = 8.333'),\n",
       " Text(13.425780160236071, 83.43627906976744, 'X[10] <= 0.027\\nmse = 138.889\\nsamples = 3\\nvalue = 16.667'),\n",
       " Text(13.391266586559372, 78.37953488372094, 'mse = 0.0\\nsamples = 2\\nvalue = 25.0'),\n",
       " Text(13.46029373391277, 78.37953488372094, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " Text(13.49480730758947, 83.43627906976744, 'mse = 0.0\\nsamples = 3\\nvalue = 0.0'),\n",
       " Text(13.52932088126617, 98.60651162790697, 'X[9] <= 0.276\\nmse = 64.879\\nsamples = 17\\nvalue = 2.941'),\n",
       " Text(13.49480730758947, 93.54976744186047, 'mse = 0.0\\nsamples = 13\\nvalue = 0.0'),\n",
       " Text(13.563834454942869, 93.54976744186047, 'X[1] <= 0.028\\nmse = 156.25\\nsamples = 4\\nvalue = 12.5'),\n",
       " Text(13.52932088126617, 88.49302325581394, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(13.598348028619569, 88.49302325581394, 'mse = 0.0\\nsamples = 2\\nvalue = 25.0'),\n",
       " Text(13.632861602296266, 103.66325581395348, 'X[6] <= 0.5\\nmse = 138.889\\nsamples = 3\\nvalue = 8.333'),\n",
       " Text(13.598348028619569, 98.60651162790697, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(13.667375175972966, 98.60651162790697, 'mse = 0.0\\nsamples = 2\\nvalue = 0.0'),\n",
       " Text(13.736402323326365, 108.72, 'X[9] <= 0.514\\nmse = 156.25\\nsamples = 2\\nvalue = 12.5'),\n",
       " Text(13.701888749649665, 103.66325581395348, 'mse = 0.0\\nsamples = 1\\nvalue = 25.0'),\n",
       " Text(13.770915897003064, 103.66325581395348, 'mse = 0.0\\nsamples = 1\\nvalue = 0.0'),\n",
       " ...]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "trainData = pd.read_csv(\"train.csv\")\n",
    "testExamples = pd.read_csv(\"validation_data.csv\")\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "with open('features.txt') as f:\n",
    "    for i in range(80000):\n",
    "        x.append(f.readline().split())\n",
    "\n",
    "for i in range(len(x)):\n",
    "    toxic_point = trainData.iloc[i].toxic * 25  \n",
    "    obscene_point = trainData.iloc[i].obscene * (30 + toxic_point)\n",
    "    insult_point =  trainData.iloc[i].insult * (35 + toxic_point)\n",
    "    severe_toxic_point = trainData.iloc[i].severe_toxic * (45 + insult_point)\n",
    "    threat_point = trainData.iloc[i].threat * (50 + insult_point)\n",
    "    identity_hate_point = trainData.iloc[i].identity_hate * (50 + insult_point)\n",
    "    y.append(toxic_point + obscene_point + insult_point + severe_toxic_point + threat_point + identity_hate_point)\n",
    "\n",
    "dt = tree.DecisionTreeRegressor()\n",
    "dt = dt.fit(x, y)\n",
    "tree.plot_tree(dt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3977e62-f633-4058-bdca-2911a1fe61cf",
   "metadata": {},
   "source": [
    "----\n",
    "## Task: SVM Classifier   (5 Marks)\n",
    "\n",
    "Build an [SVM classifier](https://scikit-learn.org/stable/modules/svm.html#classification) that identifies toxic comments.\\\n",
    "Use the **SVM**'s default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae63000-79a2-49ec-9de4-1c231cc3fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EXAMPLE CODE: COMMENT OUT THIS CODE CELL AFTER IMPLEMENTING YOUR CODE BELOW\n",
    "# # Code from: https://scikit-learn.org/stable/modules/svm.html#classification\n",
    "\n",
    "# from sklearn import svm\n",
    "\n",
    "# # the dataset  X:features, y:output values we are trying to predict\n",
    "# X = [[0.0, 0], [1.2, 1]]\n",
    "# y = [0, 1]\n",
    "\n",
    "# clf = svm.SVC()\n",
    "\n",
    "# # train the model\n",
    "# clf.fit(X, y)\n",
    "\n",
    "# # after being fitted, predict new values\n",
    "# clf.predict([[2., 2.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c6eaf6-02da-470a-b435-91e6918ea685",
   "metadata": {},
   "source": [
    "**SVM** for regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7965fc32-0c4e-45ac-b29b-7148463a6858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EXAMPLE CODE: COMMENT OUT THIS CODE CELL AFTER IMPLEMENTING YOUR CODE BELOW\n",
    "# # Code from: https://scikit-learn.org/stable/modules/svm.html#regression\n",
    "\n",
    "# from sklearn import svm\n",
    "\n",
    "# # the dataset  X:features, y:output values we are trying to predict\n",
    "# X = [[0, 0], [1, 1]]\n",
    "# y = [0, 1.2]\n",
    "\n",
    "# clf = svm.SVR()\n",
    "\n",
    "# # train the model\n",
    "# clf.fit(X, y)\n",
    "\n",
    "# # after being fitted, predict new values\n",
    "# clf.predict([[2., 2.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c13330c-177a-4700-9fe8-b9b623ccfa45",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <h4>WRITE CODE</h4>\n",
    "    In the cell below, write the code that implements an SVM classifier.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c3c5a384-6e2b-4724-9f8c-e9fc623773a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR()"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVR()\n",
    "clf.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e825695a-70ba-4c2c-b7f8-f832edbfdcce",
   "metadata": {},
   "source": [
    "----\n",
    "## Task: Language Model   (20 Marks)\n",
    "\n",
    "Build a *language model* to identify toxic comments.\\\n",
    "Recall that language models can be built using n-grams.\\\n",
    "Also refer to **Chapter 3** in \"[*Speech and Language Processing*](https://web.stanford.edu/~jurafsky/slp3)\" (freely available textbook!) by Jurafsky & Martin for explanations of **N-gram Language Models**.\n",
    "\n",
    "Construct a **Tri-gram** language model with **backoff smoothing**.\\\n",
    "Refer to **Quiz #4** for insights into **n-gram** language models.\n",
    "\n",
    "A breakdown of the marking for this task:\n",
    "* [**15 marks**] trigram language model implemented\n",
    "* [**5 marks**] back-off smoothing implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6430d2-def8-4da9-ba7b-6701e1d942f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refer to Quiz #4 for help with n-gram language models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f13fe6-37d1-4d2b-afb5-0d9f4e03868c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <h4>WRITE CODE</h4>\n",
    "    In the cell below, write the code that implements a Tri-gram Language Model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9abf8031-12c5-46cc-9b4f-bcbda5ad0839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.833952766093598e-33\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "import math\n",
    "# YOUR CODE IN THIS CELL\n",
    "\n",
    "unigrams = list()\n",
    "bigrams = list()\n",
    "trigrams = list()\n",
    "totalProbability = 0\n",
    "\n",
    "#create unigram, digram and trigram for the train samples\n",
    "for i in range(20000):\n",
    "    text = trainData.iloc[i].comment_text\n",
    "    trigrams += list(nltk.ngrams(re.split('\\W+', text), 3))\n",
    "    bigrams += list(nltk.ngrams(re.split('\\W+', text), 2))\n",
    "    unigrams += list(nltk.ngrams(re.split('\\W+', text), 1))\n",
    "\n",
    "#create frequency distribution of each data model\n",
    "fdistTrigrams = FreqDist(trigrams)\n",
    "fdistBigrams = FreqDist(bigrams)\n",
    "fdistUnigrams = FreqDist(unigrams)\n",
    "\n",
    "for i in range(20000):\n",
    "    probability = 0\n",
    "    if trainData.iloc[i].toxic == 1:\n",
    "        text = re.split('\\W+', trainData.iloc[6].comment_text) #convert the text to a list\n",
    "        if len(text) >= 3:\n",
    "            for j in range (len(text) - 2):\n",
    "                if fdistTrigrams[(text[j], text[j+1], text[j+2])] in trigrams:              \n",
    "                    probability += math.log(fdistTrigrams[(text[j], text[j+1], text[j+2])] / len(trigrams))\n",
    "                elif fdistTrigrams[(text[j], text[j+1], text[j+2])] not in trigrams: \n",
    "                    probability += math.log(fdistBigrams[(text[j], text[j+1])] / len(bigrams))\n",
    "                else:\n",
    "                    probability += math.log(fdistUnigrams[(text[j])] / len(unigrams))\n",
    "            totalProbability += math.exp(probability)\n",
    "        elif len(text) == 2:\n",
    "            if fdistTrigrams[(text[0], text[1])] in bigrams:\n",
    "                probability += math.log(fdistBigrams[(text[0], text[1])] / len(bigrams))\n",
    "            else:\n",
    "                for j in range (2):\n",
    "                    probability += math.log(fdistUnigrams[(text[j])] / len(unigrams)) \n",
    "            totalProbability += math.exp(probability)\n",
    "        else:\n",
    "            probability += math.log(fdistTrigrams[(text[0])] / len(unigrams))\n",
    "            totalProbability += math.exp(probability)\n",
    "        \n",
    "\n",
    "print(totalProbability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1b8526-2c08-43f6-9574-9d0d20db7d80",
   "metadata": {},
   "source": [
    "----\n",
    "## Task: Random Forest Model   (5 Marks)\n",
    "\n",
    "Build a *random forest model* to identify toxic comments.\n",
    "\n",
    "### Random Forest Classifier\n",
    "\n",
    "The **scikit-learn** implementation of **Random Forest** \"*combines classifiers by averaging their probabilistic prediction, instead of letting each classifier vote for a single class*\".\\\n",
    "Code examples for implementing a **Random Forest Classifier** is [here](https://scikit-learn.org/stable/modules/ensemble.html#forest).\\\n",
    "API information on **Random Forest Classifiers** is [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier).\n",
    "\n",
    "\n",
    "### Random Forest Regressor\n",
    "\n",
    "A **Random Forest** model for a regression task is conceptually identical to the classifier version above.\n",
    "\n",
    "\n",
    "### Implementation\n",
    "\n",
    "Using **scikit-learn**, create a **Random Forest** model consisting of the following parameters (model parameters not specified can be left to their default values):\n",
    "* number of estimators = 100\n",
    "\n",
    "\n",
    "**Random Forest** for classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bd7821-c669-475c-937e-b60a97e287c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EXAMPLE CODE: COMMENT OUT THIS CODE CELL AFTER IMPLEMENTING YOUR CODE BELOW\n",
    "# # Code from: https://scikit-learn.org/stable/modules/ensemble.html#forest\n",
    "# # Classification\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# X = [[0, 0], [1, 1]]\n",
    "# Y = [0, 1]\n",
    "\n",
    "# clf = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "# clf = clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e7c905-fcc8-4b00-bc48-dc1ba644ddca",
   "metadata": {},
   "source": [
    "**Random Forest** for regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a00f27-fffd-4b01-839b-713769b6bca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EXAMPLE CODE: COMMENT OUT THIS CODE CELL AFTER IMPLEMENTING YOUR CODE BELOW\n",
    "# # Code from: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "# # Regression\n",
    "\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.datasets import make_regression\n",
    "# X, y = make_regression(n_features=4, n_informative=2, random_state=0, shuffle=False)\n",
    "\n",
    "# regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "\n",
    "# regr.fit(X, y)\n",
    "\n",
    "# print(regr.predict([[0, 0, 0, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f03d51-a57b-4ba2-96e8-3872b1dff3bf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <h4>WRITE CODE</h4>\n",
    "    In the cell below, write the code that implements a Random Forest Classifier.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e682a15b-88ef-42f1-bd3b-7846fdafaf4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=2, random_state=0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "regr.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89880302-4f63-4438-be94-53416166a37c",
   "metadata": {},
   "source": [
    "## Task: Voting Ensemble   (10 Marks)\n",
    "\n",
    "A **Voting Ensemble** classifier \"*combine conceptually different machine learning classifiers and use a majority vote (hard vote) or the average predicted probabilities (soft vote) to predict the class labels*\". \"*In his highly influential Society of Mind theory, Marvin Minsky proposes that human minds are constructed from an ensemble of agents*\" (from \"**AI: A Modern Approach**\" pg. 434, 3rd ed.).\n",
    "\n",
    "From https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier:\n",
    "> *The idea behind the VotingClassifier is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses.The idea behind the VotingClassifier is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses.*\n",
    "\n",
    "Using **scikit-learn**, create two **Voting Ensemble Classifiers** (*hard voting* and *soft voting*) consisting of the following models:\n",
    "* SVM classifier\n",
    "* Decision Tree classifier\n",
    "* Random Forest classifier\n",
    "\n",
    "Use the default parameters for both *hard voting* and *soft voting* classifiers.\n",
    "\n",
    "Code examples for implementing a **Voting Ensemble Classifier** is [here](https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier).\\\n",
    "API information on **Voting Ensemble Classifiers** is [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd90891-aa2d-42d6-9f2b-99f2aba7fd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EXAMPLE CODE: COMMENT OUT THIS CODE CELL AFTER IMPLEMENTING YOUR CODE BELOW\n",
    "# # Code from:  https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier\n",
    "\n",
    "# from sklearn import datasets\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from itertools import product\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# # Load some example data\n",
    "# iris = datasets.load_iris()\n",
    "# X = iris.data[:, [0, 2]]\n",
    "# y = iris.target\n",
    "\n",
    "# # Training classifiers\n",
    "# clf1 = DecisionTreeClassifier(max_depth=4)\n",
    "# clf2 = KNeighborsClassifier(n_neighbors=7)\n",
    "# clf3 = SVC(kernel='rbf', probability=True)\n",
    "\n",
    "# # The Ensemble classifier\n",
    "# # 'estimator' is another name for 'model' or 'learner'\n",
    "# eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)],\n",
    "#                         voting='soft',\n",
    "#                         weights=[2, 1, 2])\n",
    "\n",
    "# # train the individual classifiers first\n",
    "# clf1 = clf1.fit(X, y)\n",
    "# clf2 = clf2.fit(X, y)\n",
    "# clf3 = clf3.fit(X, y)\n",
    "# # then train the ensemble of the trained classifiers (clf1, clf2, & clf3)\n",
    "# eclf = eclf.fit(X, y)\n",
    "\n",
    "# # make predictions\n",
    "# print(\"Ensemble classifier's predictions:\\n\", eclf.predict(X))\n",
    "# print(\"\\nThe resulting dimensions of the Ensemble classifier:\", eclf.transform(X).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae8d8d4-b7c0-4b45-bb6b-2716f9304d24",
   "metadata": {},
   "source": [
    "A **Voting Ensemble** example for regression is [found here](https://scikit-learn.org/stable/modules/ensemble.html#voting-regressor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd82000-bbc5-4fa0-b41b-5d62c1907f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EXAMPLE CODE: COMMENT OUT THIS CODE CELL AFTER IMPLEMENTING YOUR CODE BELOW\n",
    "# # Code from:  https://scikit-learn.org/stable/modules/ensemble.html#voting-regressor\n",
    "\n",
    "# from sklearn.datasets import load_diabetes\n",
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# # Loading some example data\n",
    "# X, y = load_diabetes(return_X_y=True)\n",
    "\n",
    "# # Training individual models\n",
    "# reg1 = GradientBoostingRegressor(random_state=1)\n",
    "# reg2 = RandomForestRegressor(random_state=1)\n",
    "# reg3 = LinearRegression()\n",
    "\n",
    "# # create an ensemble from the individual models\n",
    "# ereg = VotingRegressor(estimators=[('clf', reg1), ('rf', reg2), ('lr', reg3)])\n",
    "\n",
    "# ereg = ereg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fead89e2-b8a8-4fc5-b1b3-812c890b2ab3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <h4>WRITE CODE</h4>\n",
    "    In the cell below, write the code that implements a Voting Ensemble classifier.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d20168e1-9721-47c3-8854-aeb5876abee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nguyen Do\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: FutureWarning: Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead.\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Training individual models\n",
    "reg1 = GradientBoostingRegressor(random_state=1)\n",
    "reg2 = RandomForestRegressor(random_state=1)\n",
    "reg3 = LinearRegression()\n",
    "# create an ensemble from the individual models\n",
    "ereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])\n",
    "ereg = ereg.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77267031-08c9-4a52-a44c-853c94e11f06",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "## Task: Neural Network   (5 Marks)\n",
    "\n",
    "Build a *neural network* to identify toxic comments.\n",
    "\n",
    "Note we will be building a *very superficial* neural network model.\\\n",
    "For those interested in learning more, please take **CMPT 410** (Machine Learning) and/or **CMPT 418** (Deep Learning).\n",
    "\n",
    "<!--\n",
    "A [reference tutorial from Google](https://developers.google.com/machine-learning/guides/text-classification/) provides an overview of a text classification workflow\\\n",
    "![](https://developers.google.com/machine-learning/guides/text-classification/images/TextClassificationFlowchart.png).\n",
    "-->\n",
    "\n",
    "The example below uses an existing (*already trained*) model downloaded from **Tensorflow**'s **Hub**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9db17d7-1593-4fc5-b3d2-40d06bc21f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EXAMPLE CODE: COMMENT OUT THIS CODE CELL AFTER IMPLEMENTING YOUR CODE BELOW\n",
    "# # from   https://www.tensorflow.org/tutorials/keras/text_classification_with_hub\n",
    "\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "# import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "# ########################\n",
    "# ### SET UP THE DATASET\n",
    "# ########################\n",
    "\n",
    "# # Uses the IMDB Movie Reviews dataset\n",
    "# # Split the training set into 60% and 40% to get\n",
    "# #     15,000 training examples\n",
    "# #     10,000 examples for validation\n",
    "# #     25,000 testing examples\n",
    "# train_data, validation_data, test_data = tfds.load( name=\"imdb_reviews\", \n",
    "#     split=('train[:60%]', 'train[60%:]', 'test'),\n",
    "#     as_supervised=True)\n",
    "\n",
    "# train_examples_batch, train_labels_batch = next(iter(train_data.batch(10)))\n",
    "\n",
    "# # print first 10 examples\n",
    "# print(train_examples_batch)\n",
    "# # print the first 10 labels\n",
    "# train_labels_batch\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# ### SET UP THE NEURAL NETWORK MODEL\n",
    "# ####################################\n",
    "\n",
    "# # create a Keras layer that uses a TensorFlow Hub model to embed the sentences\n",
    "# embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "# hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)\n",
    "# hub_layer(train_examples_batch[:3])\n",
    "\n",
    "# # build the full model\n",
    "# model = tf.keras.Sequential()\n",
    "# model.add(hub_layer)\n",
    "# model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# # configure the model to use an optimizer and a loss function\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# ### TRAIN THE NEURAL NETWORK MODEL\n",
    "# ####################################\n",
    "\n",
    "# # Train the model for 10 epochs in mini-batches of 512 samples\n",
    "# # This is 10 iterations (epochs) over all samples in the x_train and y_train tensors\n",
    "# # While training, monitor the model's loss and accuracy on the 10,000 samples from the validation set\n",
    "# history = model.fit(train_data.shuffle(10000).batch(512),\n",
    "#                     epochs=10,\n",
    "#                     validation_data=validation_data.batch(512),\n",
    "#                     verbose=1)\n",
    "\n",
    "\n",
    "# ######################################\n",
    "# ### EVALUATE THE NEURAL NETWORK MODEL\n",
    "# ######################################\n",
    "\n",
    "# # Evaluate the model\n",
    "# # Two values will be returned:\n",
    "# #    Loss (a number which represents our error, lower values are better)\n",
    "# #    Accuracy\n",
    "# results = model.evaluate(test_data.batch(512), verbose=2)\n",
    "# for name, value in zip(model.metrics_names, results):\n",
    "#     print(\"%s: %.3f\" % (name, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a32125-fb29-465f-bc1a-094f39b4f6f9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <h4>WRITE CODE</h4>\n",
    "    In the cell below, write the code that implements a Neural Network.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "89211ebe-e5d2-4692-8619-a4c8618357d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 962us/step - loss: 1717.4065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fb6fdf8b20>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "trainData = pd.read_csv(\"train.csv\")\n",
    "x = [] #abalone_features\n",
    "temp = []\n",
    "with open('features.txt') as f:\n",
    "    for i in range(1000):\n",
    "        for s in f.readline().split():\n",
    "            s = float(s)\n",
    "            temp.append(s)\n",
    "        x.append(temp)\n",
    "        temp = []\n",
    "\n",
    "y = [] #abalone_labels\n",
    "for i in range(len(x)):\n",
    "    toxic_point = trainData.iloc[i].toxic * 25  \n",
    "    obscene_point = trainData.iloc[i].obscene * (30 + toxic_point)\n",
    "    insult_point =  trainData.iloc[i].insult * (35 + toxic_point)\n",
    "    severe_toxic_point = trainData.iloc[i].severe_toxic * (45 + insult_point)\n",
    "    threat_point = trainData.iloc[i].threat * (50 + insult_point)\n",
    "    identity_hate_point = trainData.iloc[i].identity_hate * (50 + insult_point)\n",
    "    y.append(toxic_point + obscene_point + insult_point + severe_toxic_point + threat_point + identity_hate_point)\n",
    "model = tf.keras.Sequential([\n",
    "  layers.Dense(64),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss = tf.losses.MeanSquaredError(),\n",
    "                      optimizer = tf.optimizers.Adam())\n",
    "model.fit(np.array(x), np.array(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb38b6e-66d2-487f-8b28-fe4a5397a172",
   "metadata": {},
   "source": [
    "----\n",
    "## Task: Naive Bayes Classifier   (5 Marks)\n",
    "\n",
    "Build a **Naive Bayes classifier** to identify toxic comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d35b722-5c89-479f-b75b-c6835db33a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EXAMPLE CODE: COMMENT OUT THIS CODE CELL AFTER IMPLEMENTING YOUR CODE BELOW\n",
    "# # Code from:  https://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes\n",
    "\n",
    "# import numpy as np\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "# Y = np.array([1, 1, 1, 2, 2, 2])\n",
    "\n",
    "# clf = GaussianNB()\n",
    "# clf.fit(X, Y)\n",
    "\n",
    "# print(clf.predict([[-0.8, -1]]))\n",
    "\n",
    "# clf_pf = GaussianNB()\n",
    "# clf_pf.partial_fit(X, Y, np.unique(Y))\n",
    "\n",
    "# print(clf_pf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4753557-4871-4dca-ad24-9b751332bff7",
   "metadata": {},
   "source": [
    "### Regression (Linear Model)\n",
    "\n",
    "For regression, use a **Linear Regression** model instead of *Naive Bayes*.\\\n",
    "A succinct overview of using a [linear model to detect diabetes](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py) provides a good explanation of an end-to-end experimental workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1867806-8b86-4704-8216-79b44569a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EXAMPLE CODE: COMMENT OUT THIS CODE CELL AFTER IMPLEMENTING YOUR CODE BELOW\n",
    "# # Code from:  https://scikit-learn.org/stable/modules/linear_model.html\n",
    "\n",
    "# from sklearn import linear_model\n",
    "\n",
    "# reg = linear_model.LinearRegression()\n",
    "\n",
    "# # train the model with data\n",
    "# reg.fit([[0, 0], [1, 1], [2, 2]],\n",
    "#         [0, 1, 2])\n",
    "\n",
    "# # make predictions\n",
    "# prediction = reg.predict([[-0.8, -1]])\n",
    "# print(\"Prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf1896d-f3c0-48e1-a640-6f3ee19e471d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <h4>WRITE CODE</h4>\n",
    "    In the cell below, write the code that implements a Naive Bayes Classifier.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "48248840-0218-4df6-be5e-a6726206a3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nguyen Do\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: FutureWarning: Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead.\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg = reg.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfbb7f7-1b5f-43c0-97ae-47d7813851c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task: Evaluation   (10 Marks)\n",
    "\n",
    "The following classification models are to be evaluated (default parameters can be used in both models):\n",
    "* [Gaussian Naive Bayes classifier](https://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes) \n",
    "* [Decision Tree classifier](https://scikit-learn.org/stable/modules/tree.html#classification)\n",
    "* SVM classifier\n",
    "* Tri-gram Language Model\n",
    "* Neural Network classifier\n",
    "* Random Forest classifier\n",
    "* Ensemble Model classifier\n",
    "\n",
    "Models will be *trained* on the **training data**.\\\n",
    "Models will be *evaluated* on the **test data**.\n",
    "\n",
    "<font color=red>**TBD**.<font color=black> The below is *not* the final evaluation method but the one used in the **Kaggle** competition.\n",
    "\n",
    "<font color=lightgray>\n",
    "\n",
    "The evaluation description for the original **Idenifying Toxic Comments** task needs to be updated.\n",
    "    \n",
    "> Submissions are evaluated on the **mean column-wise ROC AUC**.\\\n",
    "> In other words, the score is the **average of the individual AUCs of each predicted column**.\n",
    "> \n",
    "> **Submission File**\n",
    "> \n",
    "> For each id in the test set, you must predict a probability for each of the six possible types of comment toxicity (*toxic, severetoxic, obscene, threat, insult, identityhate*). The columns must be in the same order as shown below. The file should contain a header and have the following format:\n",
    "> \n",
    "> \t\tid,toxic,severe_toxic,obscene,threat,insult,identity_hate\n",
    "> \t\t00001cee341fdb12,0.5,0.5,0.5,0.5,0.5,0.5\n",
    "> \t\t0000247867823ef7,0.5,0.5,0.5,0.5,0.5,0.5\n",
    "> \t\t...\n",
    "> \t\tetc.\n",
    "\n",
    "    \n",
    "<font color=black>\n",
    "\n",
    "### How To Evaluate A Machine Learning Model\n",
    "\n",
    "We will assign a toxicity score to two comments where one of the comments are determined by humans to be more toxic than the other comment.\n",
    "The performance of a model is measured by how many of the comment pairs agree with human rankings.\n",
    "\n",
    "For example, **Comment 1** (from `validation_data.csv`) is given a score of 1.65 &\n",
    "**Comment 2** is given a score of 76, resulting in **Comment 2** being more toxic than **Comment 1**. If this matches the human assessment then we score 1/1. If not, then the score is 0/1.\n",
    "    \n",
    "From the [task's description on **Kaggle**](https://www.kaggle.com/c/jigsaw-toxic-severity-rating/overview/evaluation):\n",
    "> *For each of the approximately 200,000 pair ratings in the ground truth test data, we use your predicted toxicity score to rank the comment pair. The pair receives a 1 if this ranking matches the annotator ranking, or 0 if it does not match.*\n",
    "    \n",
    "The data used in the evaluation comes from `validation_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54821a58-ea33-47f7-a3be-28ed15357a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EXAMPLE CODE: COMMENT OUT THIS CODE CELL AFTER IMPLEMENTING YOUR CODE BELOW\n",
    "\n",
    "# # keep track of how many comment pairs we correctly rank\n",
    "# total_correct_comment_pair_rankings = 0\n",
    "\n",
    "# # get next pair of comments from validation_data.csv\n",
    "# # NOTE: comment1 is from the column corresponding to \"LESS TOXIC\"\n",
    "# #       comment2 is from the column corresponding to \"MORE TOXIC\"\n",
    "# comment1 = \"Comment from Wikipedia! (less toxic comment)\"\n",
    "# comment2 = \"ANOTHER Comment from Wikipedia! Swear word: bA$$ (more toxic comment)\"\n",
    "\n",
    "# # convert the comment's text into a feature vector e.g., [0, 0, 2.51, 1, ...]\n",
    "# comment1_features = extract_features(comment1)\n",
    "# comment2_features = extract_features(comment2)\n",
    "\n",
    "# # compute the toxicity score of each comment\n",
    "# toxicicity_score_of_comment1 = some_model.predict(comment1_features)\n",
    "# toxicicity_score_of_comment2 = some_model.predict(comment2_features)\n",
    "\n",
    "# if toxicicity_score_of_comment2 > toxicicity_score_of_comment1:\n",
    "#     total_correct_comment_pair_rankings = total_correct_comment_pair_rankings + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53c5afb-aed8-4287-b637-227c85e004d4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <h4>WRITE CODE</h4>\n",
    "    In the cell below, write the code for evaluating a model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b746513-cfa0-4cd6-847b-b2d301e4d57c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14200/54717484.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtestSamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"validation_data.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdt_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mclf_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mregr_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mereg_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "testSamples = pd.read_csv(\"validation_data.csv\")\n",
    "dt_score = 0\n",
    "clf_score = 0\n",
    "regr_score = 0\n",
    "ereg_score = 0\n",
    "model_score = 0\n",
    "reg_score = 0\n",
    "\n",
    "for i in range(10000):\n",
    "    comment1 = testSamples.iloc[i].less_toxic\n",
    "    comment2 = testSamples.iloc[i].more_toxic\n",
    "    comment1_features = extract_features(comment1)\n",
    "    comment2_features = extract_features(comment2)\n",
    "    \n",
    "    \n",
    "    dt_comment1 = dt.predict([comment1_features])\n",
    "    dt_comment2 = dt.predict([comment2_features])\n",
    "    if dt_comment2 > dt_comment1:\n",
    "        dt_score = dt_score + 1\n",
    "    \n",
    "    clf_comment1 = clf.predict([comment1_features])\n",
    "    clf_comment2 = clf.predict([comment2_features])\n",
    "    if clf_comment2 > clf_comment1:\n",
    "        clf_score = clf_score + 1\n",
    "    \n",
    "    regr_comment1 = regr.predict([comment1_features])\n",
    "    regr_comment2 = regr.predict([comment2_features])\n",
    "    if regr_comment2 > regr_comment1:\n",
    "        regr_score = regr_score + 1\n",
    "    \n",
    "    ereg_comment1 = ereg.predict([comment1_features])\n",
    "    ereg_comment2 = ereg.predict([comment2_features])\n",
    "    if ereg_comment2 > ereg_comment1:\n",
    "        ereg_score = ereg_score + 1\n",
    "\n",
    "    model_comment1 = model.predict([comment1_features])\n",
    "    model_comment2 = model.predict([comment2_features])\n",
    "    if model_comment2 > model_comment1:\n",
    "        model_score = model_score + 1\n",
    "    \n",
    "    reg_comment1 = reg.predict([comment1_features])\n",
    "    reg_comment2 = reg.predict([comment2_features])\n",
    "    if reg_comment2 > reg_comment1:\n",
    "        reg_score = reg_score + 1\n",
    "print(dt_score)\n",
    "print(clf_score)\n",
    "print(regr_score)\n",
    "print(ereg_score)\n",
    "print(model_score)\n",
    "print(reg_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e17568c-6bc9-4f8b-af7c-a5e0745a569c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "----\n",
    "# Task: Project Report   \n",
    "\n",
    "The total marks for all of the sections below is **80 Marks**.\n",
    "\n",
    "This section corresponds to the write-up of the project. Your write-up is to be included within this **Jupyter Notebook** below (the code for this assignment is in the code cells above).\\\n",
    "The **Project Report** will consist of a few sections that each discuss a different stage of the end-to-end experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7cdacf-58cd-4226-b9d0-f5081a9d196f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overview    (5 Marks)\n",
    "\n",
    "Discuss:\n",
    "* the problem/task you are addressing\n",
    "* provide concrete examples of the problem\n",
    "* why the problem is worth the time and effort trying to solve\n",
    "* compare the task with other tasks that are similar\n",
    "\n",
    "The following are optional:\n",
    "* *Related Work* i.e., what have other people tried\n",
    "* historical background of the problem\n",
    "* discuss strategies used in other tasks that are similar to **Toxic Comment Identification**\n",
    "* discuss the differences with those tasks that are similar to **Toxic Comment Identification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d3cf30-88c1-4bac-b854-ee03add22f8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset    (5 Marks)\n",
    "\n",
    "Discuss:\n",
    "* the dataset's size\n",
    "* languages dataset contains\n",
    "* anything unusual about the data\n",
    "* how representative the dataset is of everyday communication\n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbcaf12-1c62-4a31-a18b-8d4036f17590",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Features    (5 Marks)\n",
    "\n",
    "Discuss:\n",
    "* the features that were extracted\n",
    "* the number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa3979f-0844-4cc9-9029-0bae2e1af800",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models    (5 Marks)\n",
    "\n",
    "Discuss:\n",
    "* the models used\n",
    "* any specific parameters, configuration, or settings of each model\n",
    "* any differences in how each model was trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6adfef-dd91-4803-b341-fae364ee2370",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation    (5 Marks)\n",
    "\n",
    "This section discusses:\n",
    "* how you evaluated the models in order to compare their relative performance\n",
    "* evaluating models based on *overall performance*\n",
    "* use visuals, tables, charts, graphs, etc. to communicate results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf8ed7f-317d-4778-8ac9-96ca60248877",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Discussion    (15 Marks)\n",
    " \n",
    "Compare the performance of the above models on **Identifying Toxic Comments**.\\\n",
    "Use visuals, charts, graphs, etc. to communicate your results.\n",
    "\n",
    "Discuss:\n",
    "* your findings in general\n",
    "* compare the performance of the various models (was the performance what you expected?)\n",
    "* which system performed best? why?\n",
    "* which system had the worst performance? why?\n",
    "* discuss the reasons which lead to the results from the evaluation\n",
    "* provide some ideas you would like to have tried (provided you had more time or resources) that could potentially improve the performance of the models or a question that you were interested in exploring (i.e., *Future Work*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dbea51-7f7a-4e75-9d1c-2ce85f3f9ff5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plotting & Visualizations\n",
    "\n",
    "Examples of various plots and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0b520c-2c0b-43cf-9487-ce93fe4a5c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EXAMPLE CODE: COMMENT OUT THIS CODE CELL AFTER IMPLEMENTING YOUR CODE BELOW\n",
    "\n",
    "# # TODO add example visuals\n",
    "\n",
    "# import seaborn\n",
    "\n",
    "# # Load example miles per gallon dataset\n",
    "# mpg = seaborn.load_dataset(\"mpg\")\n",
    "\n",
    "# # Plot miles per gallon against horsepower\n",
    "# seaborn.relplot(x=\"horsepower\", y=\"mpg\", hue=\"origin\", size=\"weight\", sizes=(40, 400), alpha=0.4, data=mpg)\n",
    "\n",
    "# # Display joint distribution of the features\n",
    "# seaborn.pairplot(mpg, diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dac2106-1c27-468d-99d7-299992efc546",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <h1>YOUR PROJECT REPORT BEGINS BELOW THIS CELL</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6649d70c-a699-4246-b5c1-751abf71a6ce",
   "metadata": {},
   "source": [
    "**INFORMATION**\\\n",
    "Student Name: Nguyen Do\\n\n",
    "Student ID: 301403112\n",
    "Student SFU Email: nguyend@sfu.ca\n",
    "\n",
    "# Overview\n",
    "\n",
    "For this experiment, we are aiming to identify the language patterns for toxic comments. A real life application for this experiment would be a system to identify and eliminate toxicity online which is highly demanded in the current age of the internet. We will compare the performance of different training models and features for detecting language patterns. Each model will formulate a toxicity score for each comment that it’s being given and the output will be a toxicity score for that comment. The comments with higher toxicity scores will be considered more toxic. In this experiment, specifically, we are trying to train the models to detect toxic languages, however, the problem of identifying language patterns can extend to other fields such as literature. For example, with a given extract from a book, another machine can analyze its language structure to identify the book’s author."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661b9913-05e0-4115-be2c-f7ed21fe049f",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "The training dataset is provided by Kaggle’s ‘Jigsaw Rate Severity of Toxic Comments’ challenge. Each entry in the dataset is extracted from the Wikipedia Talk page comments and are all in English. However, characters from other languages appear frequently which adds complexity to the problem. Each comment has 6 different values: toxic, severe_toxic, obscene, threat, insult and identity_hate. Each value could either be 1, which means the comment is in that category, or 0, which means that the comment is not in that category. The comments can be in multiple categories or in none of them, allowing a varying toxicity level in the dataset. The toxicity level of each comment is decided by several human judges who looked at them outside of context. Despite these comments being written by (presumably) humans, they are not very reflective of everyday communication. This is due to the anonymity provided by the internet which allows people to say things that they wouldn’t in person. Additionally, online writing gives people more time to think which results in the non-toxic comments to appear more formal than if it was spoken in person. On top of that, a lot of the comments use quotes and sources which are not very common in daily conversations. Something that I have noticed looking through the dataset  is that a lot of these comments use British phrases and slang. This makes me wonder how much the difference in regional dialects affects the data. Due to the constraint of time, the models that I used were only able to train on a dataset of 80000 comments out of around 150000 entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c309297-f6a7-4b7f-a15e-5dde113f3733",
   "metadata": {},
   "source": [
    "# Features\n",
    "\n",
    "Features\n",
    "There are 13 features that are extracted from the dataset:\n",
    "1. The number of swear words in the text\n",
    "2. The percentage of swear words in the text: \n",
    "3. The number of threats-related words in the text\n",
    "4. The number of of family-related words in the text\n",
    "5. The number of gender slurs used\n",
    "6. The number of n-word used\n",
    "7. The number of f-word used\n",
    "8. Any mention of nationalities\n",
    "9. Any mention of countries\n",
    "10. The percentage of the text that is all highercase \n",
    "11. The highest count of same consecutive characters\n",
    "12. The percentage of the word ‘you’ appeared in the text\n",
    "13. The length of the longest word in the text\n",
    "\n",
    "\n",
    "The idea behind the features that I have chosen is to use the first and second feature to identify the lowest and most common level of toxicity. The percentage of swear words is used in addition to the count of swear words because it can be obtained easily without needing any iteration through the text. Additionally, the percentage of swear words in the text allows better detection of shorter toxic comments since each swear word counts for more. On top of the first and second features, the others will put each comment into further toxicity categories. For example, the comments that are toxic and use any family-related words are very likely to be an insult or a threat. Most of these values are not to indicate toxicity by themselves, however, they should be working in conjunction with the first and second features. In a way, the first and second features are the prerequisite on whether a comment is toxic or not.\n",
    "\n",
    "There are two features which specifically count the number of f-words and n-words in the comment. Most people probably already know that the n-word will be flagged by any toxic detection system. Therefore, there is a very high chance that any use of the n-word will be decrypted in some way. On the other hand, the f-word has a lot of variations which might not be included in my list of swear words. I think that hardcoding these two words specifically will allow me to catch more toxic cases that I otherwise wouldn’t be able to. However, both of them could probably be condensed as an implementation in the first and second feature instead of being a feature themselves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3701e74d-ec62-4ef6-8781-fe01d6961d3d",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "The models that are being used are: Decision Tree classifier, SVM classifier, Random Forest classifier, Ensemble Model classifier, Neural Network classifier and Regression classifier. They are all being trained using the first 80000 texts from the training set and are tested on the first 10000 texts from the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76733eb8-7d58-44bd-aa78-a28f769365d2",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "To evaluate the performance of each model, they are provided a set of tests. Each test consists of two comments. With the help of human judges, one comment will be considered more toxic than the other. The models will then give each comment a toxic score. If the more toxic comment receives a higher score, it means the model is correct for that test. The total point of each model will be the number of tests that it got correct. Each model will be tested on the first 10000 entries from the testing sample provided by Kaggle.   \n",
    "\n",
    "Total number of correct tests for each model:\n",
    "Decision Tree classifier: 4528\n",
    "SVM classifier: 6352\n",
    "Random Forest classifier: 3618\n",
    "Ensemble Model classifier: 6519\n",
    "Neural Network classifier: 5638\n",
    "Regression classifier: 5737\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fad9e52",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\t\n",
    "For the most part, the models I used have a very high true positive rate but low true negative rate. This means they struggle a lot when having to compare comments that have a low level of toxicity. Comments with low toxicity level are usually given a score of 0. As a result, comparing between them is very difficult because we need one to be strictly more toxic than the other. This is due to the list of swear words that I used being very strict, meaning it contains a lot of words such as ‘burn’ that are only toxic in certain contexts. Because of this, words that are actually toxic are devalued and the comments that contain a small number of swear words are very hard to detect.  \n",
    "Based on the result gathered out of 10000 entries in the test sample, the Ensemble Model classifier is the most successful with a 65% correct rate. Because the Ensemble Model classifier takes into account multiple other classifiers, it has a higher chance of detecting toxicity in comments that have a low level of toxicity. On the other hand, the Random Forest classifier is the least successful with only a 36% correct rate. This is quite shocking because both the Ensemble Model and the Random Forest Model use the ensemble method but the result provided is vastly different. A potential reason why the Random Forest Model is so unsuccessful is because the number of toxic comments in the data set is much smaller than the number of non-toxic comments. Therefore, some of the classifiers in the Random Forest Model might not be trained on any toxic comment at all which can negatively affect its judgement. With that being said, I think that the Random Forest model is one that is still worth looking into. Given more time and resources, I would like to see how the model works with a different number of classifiers.\n",
    "Throughout this experiment, I have tested with datasets of different sizes ranging from 1000 to 100000. What I learn is that models which are more trained don't necessarily provide better results. This is potentially due to the idea of devaluation of toxic words that I talked about earlier. The models that are tested on a higher number of datas are probably exposed to a higher level of false positive prediction of negative words. If I could execute this experiment again, I would use a list of swear words that is more strict to prevent this from happening. I also noticed that for the most part, comments which are in the more toxic categories are also in the lower toxic categories. For example, if a comment is in the identity_hate category, it is also very likely to be in the toxic category. This results in the algorithm to calculate the toxic score that I have chosen. Instead of giving a higher score to the more toxic categories, each toxic category by itself will have a lower score but it will include the toxic score of the category below. For example, if a comment is in both the toxic category (2 points) and the insult category (5 points), the total toxic score of the comment will be 2 + 2 + 5 = 9, with the second 2 coming from the insult category due to it being in the more toxic category. This provides a better judgement for cases such as the comment “you will get banned” which is in the threat category but is not toxic.    \n",
    "One feature that I have attempted to add is the number of quotation marks used in the comment. The idea of this implementation is that people who use quotes (and quotation marks) are usually those that are trying to contribute, meaning they have less reasons to be toxic. However, in reality, this couldn’t be implemented because some entries in the dataset are in quotation for some reason. This is a feature that I would like to try given more time and resources. What I could attempt is to sanitize the comments in the data set before using them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bc40af-41a6-422a-9e1a-92950cc2a7ec",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n",
    "Through this assignment, my understanding of machine learning is much improved. This project allows me to implement an actual machine learning model rather than just learning about them in class. I am now also much more confident with file manipulations and code optimization through this project. Something that I didn’t expect to learn is how to use excel and the algorithm to different training models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
